import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('TkAgg')  # ä¿ç•™æ‚¨çš„TkAggè®¾ç½®
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Optional, Callable
import warnings
import os
from typing import Dict
from scipy.stats import pearsonr
from sklearn.linear_model import Lasso
import joblib # å¯¼å…¥ joblib ç”¨äºä¿å­˜æ¨¡å‹
from sklearn.preprocessing import StandardScaler
# é…ç½®ä¸­æ–‡æ˜¾ç¤ºå’Œè­¦å‘Šè¿‡æ»¤
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class CS500FactorAnalyzer:

    def __init__(self, data_folder: str):
        self.data_folder = Path(data_folder)
        self.all_data = {}
        self.factor_results = {}
        self.synthetic_factors = {}

    def parse_dirty_wide_table(self, raw_data: pd.DataFrame, sheet_name: str = None) -> pd.DataFrame:
        """
        è§£æè„å®½è¡¨ï¼Œå¹¶æ ¹æ®å› å­åç§°åº”ç”¨ä¸åŒçš„ç¼ºå¤±å€¼/0å€¼å¡«å……é€»è¾‘ã€‚
        """
        try:
            data = raw_data.copy()
            # 1. è®¾ç½®åˆ—åä¸ºè‚¡ç¥¨ä»£ç ï¼ˆç¬¬1è¡Œï¼Œç´¢å¼• 0ï¼‰
            data.columns = data.iloc[0].values
            # 2. åˆ é™¤å‰ä¸¤è¡Œï¼ˆä»£ç è¡Œå’Œåç§°è¡Œï¼‰
            clean_data = data.iloc[2:].copy()
            # 3. è®¾ç½®æ—¶é—´ç´¢å¼•ï¼ˆç¬¬ä¸€åˆ—ï¼‰
            time_col_name = clean_data.columns[0]
            clean_data = clean_data.set_index(time_col_name)
            cleaned_index = [val if not isinstance(val, tuple) else pd.NaT for val in clean_data.index]
            clean_data.index = pd.Index(cleaned_index)
            # 4. è§£ææ—¶é—´ç´¢å¼•å¹¶è½¬ä¸ºæ•°å€¼
            clean_data.index = pd.to_datetime(clean_data.index)
            # ç¡®ä¿æ•°æ®åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼Œéæ•°å€¼è½¬ä¸º NaN
            clean_data = clean_data.apply(pd.to_numeric, errors='coerce')

            # 5. æ¸…ç†å’Œç»Ÿä¸€å‘½å (å…ˆåˆ é™¤å…¨ä¸ºç©ºçš„åˆ—ï¼Œå†è¿›è¡Œå¡«å……)
            clean_data = clean_data.dropna(axis=1, how='all')

            # ==================== 0å€¼å’Œç¼ºå¤±å€¼å¤„ç†é€»è¾‘ (æ–°å¢/ä¿®æ”¹) ====================
            target_sheets = ['ROE', 'è‡ªç”±ç°é‡‘æµ', 'EPS', 'EBITDA']

            if sheet_name in target_sheets:
                # é’ˆå¯¹ç‰¹å®š sheets: 0å€¼é‡‡ç”¨éšæœºæ¨¡æ‹Ÿå¡«å……
                print(f"   ğŸ”§ æ­£åœ¨å¯¹ {sheet_name} è¿›è¡Œ **éšæœºå‘å‰æ¨¡æ‹Ÿ** å¡«å……...")

                # æ‰¾å‡ºæ‰€æœ‰ 0 å€¼çš„ mask
                zero_mask = (clean_data == 0)

                for col in clean_data.columns:
                    series = clean_data[col]
                    zero_indices = series[zero_mask[col]].index

                    if not zero_indices.empty:
                        # 1. ä¸´æ—¶å°† 0 æ›¿æ¢ä¸º NaN
                        temp_series = series.replace(0, np.nan)

                        # 2. åå‘å¡«å…… (bfill) å¾—åˆ°â€œåé¢ç¬¬ä¸€ä¸ªéé›¶å€¼ Vâ€
                        # ç›¸å½“äº series.bfill() çš„æ•ˆæœ
                        next_non_zero = temp_series.iloc[::-1].ffill().iloc[::-1]

                        # 3. æå–å¯¹åº”äº 0 ç´¢å¼•çš„ V å€¼
                        next_non_zero_for_zeros = next_non_zero.loc[zero_indices]

                        # 4. ç”Ÿæˆå‡åŒ€éšæœºæ•° U(0, 1)
                        random_uniform = np.random.rand(len(zero_indices))

                        # 5. æ¨¡æ‹Ÿå€¼ = V + U(0, 1) - 0.5
                        simulated_values = next_non_zero_for_zeros + random_uniform - 0.5

                        # 6. å¡«å……å›åŸæ•°æ®
                        clean_data.loc[zero_indices, col] = simulated_values

                # 7. å¯¹å¯èƒ½å­˜åœ¨çš„åŸå§‹ NaN æˆ–åºåˆ—æœ«å°¾æœªè¢«å¡«å……çš„ 0 è¿›è¡Œæ ‡å‡†çš„å‰åå¡«å……
                clean_data = clean_data.fillna(method='ffill').fillna(method='bfill')

            else:
                # å…¶ä»– sheets: 0 å’Œ NaN éƒ½é‡‡ç”¨ ffill/bfill æ ‡å‡†å¡«å……
                # 1. å°† 0 è§†ä¸ºç¼ºå¤±å€¼ (NaN)ï¼Œä»¥ä¾¿ç»Ÿä¸€å¡«å……
                clean_data = clean_data.replace(0, np.nan)

                # 2. ffill å’Œ bfill å¡«å……
                # print(f"   ğŸ”§ æ­£åœ¨å¯¹ {sheet_name} è¿›è¡Œ **ffill/bfill** å¡«å…… ...")
                clean_data = clean_data.fillna(method='ffill').fillna(method='bfill')


            # ==================== 0å€¼å’Œç¼ºå¤±å€¼å¤„ç†é€»è¾‘ (ç»“æŸ) ====================

            # 6. æ¸…ç†å’Œç»Ÿä¸€å‘½å (ä¸åŸæœ‰é€»è¾‘ä¸€è‡´)
            clean_data.columns.name = 'code'
            clean_data.index.name = 'date'

            # è°ƒè¯•ä¿¡æ¯
            # if sheet_name:
            #     print(sheet_name)
            #     print(
            #         f"âœ… è§£æå®Œæˆ: {clean_data.shape}, æ—¶é—´èŒƒå›´: {clean_data.index.min()} åˆ° {clean_data.index.max()}")

            return clean_data

        except Exception as e:
            print(f"âŒ è§£æ {sheet_name} å¤±è´¥: {e}")
            raise

    def load_all_periods(self):
        """åŠ è½½æ‰€æœ‰æ—¶é—´ç‚¹çš„æ•°æ®"""

        # 1. æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶ (.xlsx å’Œ .xls)ï¼Œå¹¶æŒ‰åç§°æ’åº
        excel_files = sorted(
            list(self.data_folder.glob("ä¸­è¯500_*.xlsx")) +
            list(self.data_folder.glob("ä¸­è¯500_*.xls"))
        )

        if not excel_files:
            print(
                f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ° Excel æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„ï¼š{self.data_folder} ä¸‹æ˜¯å¦å­˜åœ¨å‘½åä¸º 'ä¸­è¯500_*.xlsx' æˆ– 'ä¸­è¯500_*.xls' çš„æ–‡ä»¶ã€‚")
            return

        print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        for file in excel_files:
            # ã€ä¿®æ”¹ 1ï¼šæå–å‘¨æœŸåç§°ï¼Œå¹¶å»æ‰ "ä¸­è¯500_" å‰ç¼€ã€‘
            period_name = file.stem.replace("ä¸­è¯500_", "")
            print(f"\nğŸ“ åŠ è½½: {period_name}")

            try:
                xl_file = pd.ExcelFile(file)
                period_data = {}

                for sheet_name in xl_file.sheet_names:
                    # Sheet1 é€šå¸¸æ˜¯è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
                    if sheet_name == 'Sheet1':
                        stock_info = pd.read_excel(file, sheet_name=sheet_name)
                        period_data['stock_info'] = stock_info
                    else:
                        # å…¶ä»– Sheet æ˜¯å› å­æ•°æ®ï¼Œéœ€è¦è„å®½è¡¨è§£æ
                        # ç¡®ä¿ä¼ å…¥ header=Noneï¼Œä¸ parse_dirty_wide_table åŒ¹é…
                        sheet_data = pd.read_excel(file, sheet_name=sheet_name, header=None)
                        try:
                            # è°ƒç”¨è§£æå‡½æ•°ã€‚æ­¤æ—¶ï¼Œsheet_name è¢«ä¼ å…¥ï¼Œç”¨äºè§¦å‘ parse_dirty_wide_table ä¸­çš„éšæœºæ¨¡æ‹Ÿé€»è¾‘
                            parsed_data = self.parse_dirty_wide_table(sheet_data, sheet_name)

                            # å¦‚æœè§£æåæ•°æ®ä¸ºç©ºï¼Œåˆ™è·³è¿‡
                            if parsed_data.empty:
                                continue

                            period_data[sheet_name] = parsed_data

                        except Exception:
                            # æ•è·è§£æ Sheet å†…éƒ¨çš„é”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª Sheet
                            continue

                # ã€ä¿®æ”¹ 2ï¼šä½¿ç”¨å»æ‰å‰ç¼€çš„ period_name ä½œä¸º all_data çš„ Keyã€‘
                self.all_data[period_name] = period_data

                # æ‰“å°åŠ è½½æˆåŠŸçš„å› å­é”®
                loaded_keys = [k for k in period_data.keys() if k != 'stock_info']

                # ã€ä¿®æ”¹ 3ï¼šåœ¨åŠ è½½å®Œæˆçš„è¾“å‡ºä¸­ï¼Œä¹Ÿä½¿ç”¨å»æ‰å‰ç¼€çš„ period_nameã€‘
                print(f"    âœ… {period_name} æ–‡ä»¶åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(loaded_keys)} ä¸ªå› å­æ•°æ®é¡¹ã€‚Keys: {loaded_keys}")


            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file.name} å¤±è´¥: {e}")

        # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        if self.all_data:
            self._display_data_overview()
        else:
            print("\nâš ï¸  æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®")


    def _display_data_overview(self):
        """æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ"""
        print(f"\n{'=' * 60}")
        print("æ•°æ®åŠ è½½æ¦‚è§ˆ")
        print(f"{'=' * 60}")

        for period_name, period_data in self.all_data.items():
            print(f"\nğŸ“… æ—¶é—´æ®µ: {period_name}")
            # for data_name, data in period_data.items():
            #     if data_name == 'stock_info':
            #         print(f"   ğŸ“‹ è‚¡ç¥¨ä¿¡æ¯: {len(data)} åªè‚¡ç¥¨")
            #     else:
            #         if hasattr(data, 'shape'):
            #             print(f"   ğŸ“Š {data_name}: {data.shape} (æ—¶é—´ç‚¹Ã—è‚¡ç¥¨)")
            #             if hasattr(data, 'index'):
            #                 print(f"     æ—¶é—´èŒƒå›´: {data.index.min()} åˆ° {data.index.max()}")

    def calculate_synthetic_factor(self, factor_name: str, calculation_func: Callable):
        """
        è®¡ç®—åˆæˆå› å­
        """
        print(f"\nğŸ”§ è®¡ç®—åˆæˆå› å­: {factor_name}")
        self.synthetic_factors[factor_name] = {}

        success_count = 0
        for period_name, period_data in self.all_data.items():
            try:
                # ä¼ å…¥ period_data (åŒ…å«æ‰€æœ‰æ—¶é—´åºåˆ—å®½è¡¨)
                factor_data = calculation_func(period_data)

                # æ£€æŸ¥è¿”å›ç»“æœæ˜¯å¦ä¸ºæ¯æ—¥æ—¶é—´åºåˆ—å®½è¡¨
                if not factor_data.empty and factor_data.index.name == 'date' and factor_data.columns.name == 'code':
                    self.synthetic_factors[factor_name][period_name] = factor_data
                    success_count += 1
                    print(
                        f"   âœ… {period_name}: è®¡ç®—æˆåŠŸ, æ¯æ—¥æˆªé¢: {len(factor_data)}, è‚¡ç¥¨æ•°: {len(factor_data.columns)}")
                else:
                    print(f"   âš ï¸  {period_name}: æ— æœ‰æ•ˆæ•°æ®æˆ–æ•°æ®æ ¼å¼é”™è¯¯")
            except Exception as e:
                print(f"   âŒ {period_name}: è®¡ç®—å¤±è´¥ - {e}")

        print(f"ğŸ“ˆ {factor_name} è®¡ç®—å®Œæˆ: {success_count}/{len(self.all_data)} ä¸ªæ—¶é—´æ®µ")

    # ==================== æ¯æ—¥æ»šåŠ¨å› å­è®¡ç®—æ–¹æ³• ====================
    def calculate_alpha1(self, period_data: Dict) -> pd.DataFrame:
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        # 1. ç¡®ä¿æ•°æ®å¯¹é½
        common_stocks = close_df.columns.intersection(volume_df.columns)
        close_df = close_df[common_stocks]
        volume_df = volume_df[common_stocks]
        close_rank_cs = close_df.rank(axis=1, pct=True)  # æ¯å¤©åœ¨æ‰€æœ‰è‚¡ç¥¨ä¸­æ’å
        volume_rank_cs = volume_df.rank(axis=1, pct=True)

        # 1b. è®¡ç®—è¿™ä¸¤ç»„æ’ååºåˆ—åœ¨è¿‡å»10å¤©çš„**æ—¶é—´åºåˆ—**ç›¸å…³æ€§ (rolling(10).corr())
        # è¿™æ ·è®¡ç®—çš„æ˜¯**æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°** (Spearman's rank correlation coefficient)
        # å¯¹æ¯åªè‚¡ç¥¨ï¼Œè®¡ç®—å…¶è¿‡å»10å¤©çš„ (close_rank_cs, volume_rank_cs) çš„ç›¸å…³æ€§ã€‚
        corr_term_raw = close_rank_cs.rolling(window=10).corr(volume_rank_cs)

        # --- Step 2: rank(correlation_term) * rank(delta_term) ---

        # 2a. å¯¹ç›¸å…³æ€§é¡¹è¿›è¡Œ**æˆªé¢**æ’å: rank(corr_term_raw)
        rank_corr_term = corr_term_raw.rank(axis=1, pct=True)

        # 2b. æ»šåŠ¨è®¡ç®—ä»·æ ¼å˜åŒ–é¡¹: delta(close, 5)
        delta_term_raw = close_df.diff(5)

        # 2c. å¯¹ä»·æ ¼å˜åŒ–é¡¹è¿›è¡Œ**æˆªé¢**æ’å: rank(delta(close, 5))
        rank_delta_term = delta_term_raw.rank(axis=1, pct=True)

        # 2d. ç»„åˆ: rank(correlation(...)) * rank(delta(...))
        raw_factor_df = rank_corr_term * rank_delta_term

        # --- Step 3: æœ€ç»ˆå› å­å€¼ (åŸå§‹å› å­å€¼æ˜¯æ¯æ—¥æˆªé¢å€¼) ---
        # å¯¹æœ€ç»ˆç»“æœè¿›è¡Œæˆªé¢æ’åï¼Œè¿™æ˜¯é€šå¸¸çš„åšæ³•ï¼Œä»¥ç¡®ä¿å› å­å€¼åˆ†å¸ƒç»Ÿä¸€
        alpha1_df = raw_factor_df.rank(axis=1, pct=True, method='average')

        # æ¸…ç†å’Œç»Ÿä¸€å‘½å
        alpha1_df.columns.name = 'code'
        alpha1_df.index.name = 'date'

        # åˆ é™¤æ‰€æœ‰è‚¡ç¥¨éƒ½æ˜¯ NaN çš„æ—¥æœŸ
        return alpha1_df.dropna(how='all')

    def calculate_alpha2(self, period_data: Dict) -> pd.DataFrame:
        """
        Alpha#2 (æ¯æ—¥æ»šåŠ¨): rank(delta(close, 5)) * rank(delta(volume, 5))
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        common_stocks = close_df.columns.intersection(volume_df.columns)
        close_df = close_df[common_stocks]
        volume_df = volume_df[common_stocks]

        # 1. 5æ—¥ä»·æ ¼å˜åŒ–: delta(close, 5)
        delta_close = close_df.diff(5)
        # 2. 5æ—¥æˆäº¤é‡å˜åŒ–: delta(volume, 5)
        delta_volume = volume_df.diff(5)

        # 3. æˆªé¢æ’å (Cross-sectional Rank): è¿™æ˜¯å…³é”®ï¼
        # rank(delta(close, 5))
        ranked_delta_close = delta_close.rank(axis=1, pct=True, method='average')
        # rank(delta(volume, 5))
        ranked_delta_volume = delta_volume.rank(axis=1, pct=True, method='average')

        # 4. æœ€ç»ˆå› å­å€¼: ä¸¤ä¸ªæ’åå€¼çš„ä¹˜ç§¯
        alpha2_df = ranked_delta_close * ranked_delta_volume

        alpha2_df.columns.name = 'code'
        alpha2_df.index.name = 'date'

        # åˆ é™¤æ‰€æœ‰è‚¡ç¥¨éƒ½æ˜¯ NaN çš„æ—¥æœŸ
        return alpha2_df.dropna(how='all')

    def calculate_alpha3(self, period_data: Dict) -> pd.DataFrame:
        """
        Alpha#3 (æ¯æ—¥æ»šåŠ¨): rank(stddev(close, 10)) * rank(delta(volume, 5))
        è¿™é‡Œæˆ‘ä»¬è®¡ç®—ï¼šrank(stddev(close, 10)) * rank(delta(volume, 5))
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        common_stocks = close_df.columns.intersection(volume_df.columns)
        close_df = close_df[common_stocks]
        volume_df = volume_df[common_stocks]

        # 1. æ»šåŠ¨è®¡ç®—10æ—¥æ³¢åŠ¨ç‡: stddev(close, 10)
        volatility_term = close_df.rolling(window=10).std()

        # 2. æ»šåŠ¨è®¡ç®—5æ—¥æˆäº¤é‡å˜åŒ–: delta(volume, 5)
        delta_volume_term = volume_df.diff(5)

        # 3. æˆªé¢æ’å: åˆ†åˆ«å¯¹ä¸¤ä¸ªé¡¹è¿›è¡Œæ¯æ—¥æˆªé¢æ’å
        # rank(stddev(close, 10))
        ranked_volatility = volatility_term.rank(axis=1, pct=True, method='average')

        # rank(delta(volume, 5))
        ranked_delta_volume = delta_volume_term.rank(axis=1, pct=True, method='average')

        # 4. æœ€ç»ˆå› å­å€¼: ä¸¤ä¸ªæ’åå€¼çš„ä¹˜ç§¯
        alpha3_df = ranked_volatility * ranked_delta_volume

        alpha3_df.columns.name = 'code'
        alpha3_df.index.name = 'date'

        return alpha3_df.dropna(how='all')

    def calculate_ROC6(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: ROC6 (6å‘¨æœŸä»·æ ¼å˜åŒ–ç‡)
        å…¬å¼: (æ”¶ç›˜ä»· / Nå‘¨æœŸå‰æ”¶ç›˜ä»· - 1) * 100
        """
        if 'æ”¶ç›˜ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']

        # è®¡ç®— N=6 å‘¨æœŸå‰çš„æ”¶ç›˜ä»·
        close_lagged = close_df.shift(6)

        # è®¡ç®—ROC6
        ROC6_df = ((close_df / close_lagged) - 1) * 100

        # æˆªé¢æ’å (å¯é€‰ï¼Œä½†é€šå¸¸å› å­éƒ½éœ€è¦æ’åæˆ–æ ‡å‡†åŒ–)
        factor_df = ROC6_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')
    def calculate_BIAS60(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: BIAS60 (60å‘¨æœŸä»·æ ¼ä¹–ç¦»ç‡)
        å…¬å¼: (æ”¶ç›˜ä»· - 60å‘¨æœŸå‡ä»·) / 60å‘¨æœŸå‡ä»· * 100
        """
        if 'æ”¶ç›˜ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']

        # è®¡ç®—60å‘¨æœŸç®€å•ç§»åŠ¨å¹³å‡çº¿ (Simple Moving Average, SMA)
        MA60 = close_df.rolling(window=60).mean()

        # è®¡ç®—BIAS60
        BIAS60_df = ((close_df - MA60) / MA60) * 100

        # æˆªé¢æ’å
        factor_df = BIAS60_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')
    def calculate_CCI20(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: CCI20 (20å‘¨æœŸå•†å“é€šé“æŒ‡æ•°)
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æ—¥æœ€é«˜ä»·' not in period_data or 'æ—¥æœ€ä½ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·ã€æœ€é«˜ä»·æˆ–æœ€ä½ä»·æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        high_df = period_data['æ—¥æœ€é«˜ä»·']
        low_df = period_data['æ—¥æœ€ä½ä»·']
        N = 20

        # 1. è®¡ç®— Typical Price (TP)
        TP_df = (high_df + low_df + close_df) / 3

        # 2. è®¡ç®— TP çš„ N å‘¨æœŸ SMA (SMATP)
        SMATP_df = TP_df.rolling(window=N).mean()

        # 3. è®¡ç®— N å‘¨æœŸå¹³å‡ç»å¯¹åå·® (Mean Deviation)
        # MeanDeviation = SMA(|TP - SMATP|, N)
        MD_df = (TP_df - SMATP_df).abs().rolling(window=N).mean()

        # 4. è®¡ç®— CCI
        # é¿å…é™¤ä»¥é›¶
        MD_df_safe = MD_df.replace(0, np.nan)
        CCI20_df = (TP_df - SMATP_df) / (0.015 * MD_df_safe)

        # æˆªé¢æ’å
        factor_df = CCI20_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')
    def calculate_WVAD6(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: WVAD (6å‘¨æœŸåŠ æƒæˆäº¤é‡å˜å¼‚åº¦ - å¹³æ»‘å½¢å¼)
        å…¬å¼: SMA( ( (Close - Open) / (High - Low) ) * Volume , 6 )
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'å¼€ç›˜ä»·' not in period_data or \
                'æ—¥æœ€é«˜ä»·' not in period_data or 'æ—¥æœ€ä½ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘ä»·æ ¼æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        open_df = period_data['å¼€ç›˜ä»·']
        high_df = period_data['æ—¥æœ€é«˜ä»·']
        low_df = period_data['æ—¥æœ€ä½ä»·']
        volume_df = period_data['æˆäº¤é‡']
        N = 6

        # 1. è®¡ç®—æ ¸å¿ƒæ¯”ç‡: (Close - Open) / (High - Low)
        # é¿å…é™¤ä»¥é›¶: å½“ High=Low æ—¶ï¼Œè¯¥æ¯”ç‡ä¸º NaN (æˆ–è®¾ä¸º0)
        price_range = high_df - low_df
        # ä½¿ç”¨ np.divide å®‰å…¨åœ°æ‰§è¡Œé™¤æ³•ï¼Œå¹¶ç”¨ where é¿å…é™¤é›¶
        ratio_df = np.divide(close_df - open_df, price_range,
                             out=np.zeros_like(price_range, dtype=float),
                             where=price_range != 0)

        # 2. è®¡ç®—åŠ æƒæˆäº¤é‡: Ratio * Volume
        weighted_volume = ratio_df * volume_df

        # 3. è®¡ç®— N=6 å‘¨æœŸç§»åŠ¨å¹³å‡ (SMA)
        WVAD_df = weighted_volume.rolling(window=N).mean()

        # æˆªé¢æ’å
        factor_df = WVAD_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_EP_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: EP (ç›ˆåˆ©æ”¶ç›Šç‡)
        å…¬å¼: 1 / PEï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'PE' not in period_data:
            raise ValueError("ç¼ºå°‘PEæ•°æ®")

        PE_df = period_data['PE']

        # é¿å…é™¤é›¶å’Œæç«¯å€¼ï¼šPE > 0 æ‰æœ‰æ„ä¹‰
        EP_df = 1.0 / PE_df.mask(PE_df <= 0)

        # æˆªé¢æ’å (rank(axis=1))
        factor_df = EP_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_ROE_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: ROE (å‡€èµ„äº§æ”¶ç›Šç‡)
        å…¬å¼: ROEï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'ROE' not in period_data:
            raise ValueError("ç¼ºå°‘ROEæ•°æ®")

        ROE_df = period_data['ROE']

        # å¯¹ROEè¿›è¡Œæˆªé¢æ’å
        factor_df = ROE_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_EPS_Growth_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: EPS_Growth (250æ—¥EPSå¢é•¿ç‡)
        å…¬å¼: (EPS / 250å‘¨æœŸå‰EPS) - 1ï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'EPS' not in period_data:
            raise ValueError("ç¼ºå°‘EPSæ•°æ®")

        EPS_df = period_data['EPS']

        # EPS è¿‡å» 250 ä¸ªå‘¨æœŸï¼ˆçº¦ä¸€å¹´ï¼‰å‰çš„æ•°å€¼
        EPS_lagged = EPS_df.shift(30)

        # è®¡ç®—å¢é•¿ç‡
        growth_df = (EPS_df / EPS_lagged) - 1

        # æˆªé¢æ’å
        factor_df = growth_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_Turnover20_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: Turnover20 (20æ—¥å¹³å‡æ¢æ‰‹ç‡)
        å…¬å¼: 20æ—¥æ¢æ‰‹ç‡çš„ç®€å•ç§»åŠ¨å¹³å‡ (SMA)ï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'æ¢æ‰‹ç‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ¢æ‰‹ç‡æ•°æ®")

        turnover_df = period_data['æ¢æ‰‹ç‡']

        # è®¡ç®—20æ—¥ç®€å•ç§»åŠ¨å¹³å‡
        SMA_turnover = turnover_df.rolling(window=20).mean()

        # æˆªé¢æ’å
        factor_df = SMA_turnover.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_momentum_value_composite(self, period_data: Dict) -> pd.DataFrame:
        """
        åŠ¨é‡-ä»·å€¼å¤åˆå› å­
        å…¬å¼: rank(ROC20) * rank(1/PE)
        ç»“åˆä¸­æœŸåŠ¨é‡ä¸ä»·å€¼ä½ä¼°
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'PE' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–PEæ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        pe_df = period_data['PE']

        # 20æ—¥åŠ¨é‡
        roc20 = (close_df / close_df.shift(20) - 1)

        # ä»·å€¼å› å­ (ç›ˆåˆ©æ”¶ç›Šç‡)
        ep = 1.0 / pe_df.mask(pe_df <= 0)

        # å¤åˆå› å­
        composite = roc20.rank(axis=1, pct=True) * ep.rank(axis=1, pct=True)

        factor_df = composite.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_vol_adjusted_value(self, period_data: Dict) -> pd.DataFrame:
        """
        æ³¢åŠ¨ç‡è°ƒæ•´ä»·å€¼å› å­
        å…¬å¼: rank(EP) / rank(æ³¢åŠ¨ç‡)
        åœ¨ä»·å€¼å› å­ä¸Šè€ƒè™‘é£é™©è°ƒæ•´
        """
        if 'PE' not in period_data or 'æ”¶ç›˜ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘PEæˆ–æ”¶ç›˜ä»·æ•°æ®")

        pe_df = period_data['PE']
        close_df = period_data['æ”¶ç›˜ä»·']

        # ç›ˆåˆ©æ”¶ç›Šç‡
        ep = 1.0 / pe_df.mask(pe_df <= 0)

        # 20æ—¥æ³¢åŠ¨ç‡
        volatility = close_df.pct_change().rolling(20).std()

        # æ³¢åŠ¨ç‡è°ƒæ•´ä»·å€¼
        vol_adjusted_ep = ep.rank(axis=1, pct=True) / (volatility.rank(axis=1, pct=True) + 1e-6)

        factor_df = vol_adjusted_ep.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_volume_price_divergence(self, period_data: Dict) -> pd.DataFrame:
        """
        é‡ä»·èƒŒç¦»å› å­
        å…¬å¼: rank(ä»·æ ¼åŠ¨é‡) - rank(æˆäº¤é‡åŠ¨é‡)
        æ•æ‰é‡ä»·èƒŒç¦»çš„æŠ€æœ¯ä¿¡å·
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        # 5æ—¥ä»·æ ¼åŠ¨é‡
        price_momentum = close_df.pct_change(5)

        # 5æ—¥æˆäº¤é‡åŠ¨é‡
        volume_momentum = volume_df.pct_change(5)

        # é‡ä»·èƒŒç¦»
        divergence = price_momentum.rank(axis=1, pct=True) - volume_momentum.rank(axis=1, pct=True)

        factor_df = divergence.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_profitability_growth(self, period_data: Dict) -> pd.DataFrame:
        """
        ç›ˆåˆ©èƒ½åŠ›å¢é•¿å› å­
        å…¬å¼: rank(ROEå˜åŒ–ç‡) * rank(EBITDAå¢é•¿ç‡)
        æ•æ‰ç›ˆåˆ©èƒ½åŠ›çš„æ”¹å–„è¶‹åŠ¿
        """
        if 'ROE' not in period_data or 'EBITDA' not in period_data:
            raise ValueError("ç¼ºå°‘ROEæˆ–EBITDAæ•°æ®")

        roe_df = period_data['ROE']
        ebitda_df = period_data['EBITDA']

        # ROE 60æ—¥å˜åŒ–ç‡
        roe_growth = (roe_df / roe_df.shift(60) - 1)

        # EBITDA 60æ—¥å¢é•¿ç‡
        ebitda_growth = (ebitda_df / ebitda_df.shift(60) - 1)

        # å¤åˆå¢é•¿å› å­
        growth_factor = roe_growth.rank(axis=1, pct=True) * ebitda_growth.rank(axis=1, pct=True)

        factor_df = growth_factor.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def prepare_panel_data(self, factor_name: str) -> pd.DataFrame:
        """
        å‡†å¤‡é¢æ¿æ•°æ® (ç»Ÿä¸€å¤„ç†åŸºç¡€å› å­å’Œæ¯æ—¥æ»šåŠ¨åˆæˆå› å­)
        - å°†æ‰€æœ‰æ—¶é—´åºåˆ—å®½è¡¨è½¬ä¸ºé•¿è¡¨å¹¶åˆå¹¶
        """
        panel_data = []

        for period_name, period_data in self.all_data.items():
            factor_df = None

            # ä¼˜å…ˆæ£€æŸ¥åˆæˆå› å­ (ç°å·²æ”¹ä¸ºæ¯æ—¥æ»šåŠ¨è®¡ç®—ï¼Œè¿”å›æ—¶é—´åºåˆ—å®½è¡¨)
            if factor_name in self.synthetic_factors and period_name in self.synthetic_factors[factor_name]:
                factor_df = self.synthetic_factors[factor_name][period_name]
            # å…¶æ¬¡æ£€æŸ¥åŸºç¡€å› å­ (æœ¬èº«å°±æ˜¯æ—¶é—´åºåˆ—å®½è¡¨)
            elif factor_name in period_data:
                factor_df = period_data[factor_name]

            if factor_df is not None and not factor_df.empty:
                # ä½¿ç”¨ stack å°†å®½è¡¨ (Index=date, Columns=code) è½¬ä¸ºé•¿è¡¨
                try:
                    long_factor_df = factor_df.stack().reset_index()
                    long_factor_df.columns = ['date', 'code', factor_name]
                    long_factor_df['period'] = period_name
                    panel_data.append(long_factor_df)
                except Exception as e:
                    print(f"âš ï¸  {period_name} - {factor_name} æ•°æ®è½¬æ¢å¤±è´¥: {e}")
                    continue

        result_df = pd.concat(panel_data, ignore_index=True) if panel_data else pd.DataFrame()

        if not result_df.empty:
            # ç¡®ä¿æ—¥æœŸæ˜¯ datetime ç±»å‹
            result_df['date'] = pd.to_datetime(result_df['date'])
            print(f"\nâœ… å› å­ '{factor_name}' é¢æ¿æ•°æ®:")
            print(f"   æ—¶é—´ç‚¹: {result_df['date'].nunique()}")
            print(f"   è‚¡ç¥¨æ•°: {result_df['code'].nunique()}")
            print(f"   æ€»è®°å½•: {len(result_df)}")
        else:
            print(f"\nâš ï¸  å› å­ '{factor_name}' æ— æ•°æ®")

        return result_df

    def calculate_returns(self) -> pd.DataFrame:
        """è®¡ç®—æ”¶ç›Šç‡"""
        returns_data = []

        for period_name, period_data in self.all_data.items():
            if 'æ”¶ç›˜ä»·' in period_data:
                close_df = period_data['æ”¶ç›˜ä»·']
                # è®¡ç®—æ—¥æ”¶ç›Šç‡
                returns_df = close_df.pct_change()

                # å°†æ”¶ç›Šç‡å®½è¡¨è½¬ä¸ºé•¿è¡¨
                long_returns_df = returns_df.stack().reset_index()
                long_returns_df.columns = ['date', 'code', 'return']
                returns_data.append(long_returns_df)

        result_df = pd.concat(returns_data, ignore_index=True) if returns_data else pd.DataFrame()

        if not result_df.empty:
            # ç¡®ä¿æ—¥æœŸæ˜¯ datetime ç±»å‹
            result_df['date'] = pd.to_datetime(result_df['date'])
            # æ¸…ç† Inf/-Inf (é™¤ä»¥0å¯èƒ½äº§ç”Ÿ)
            result_df = result_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['return'])
            print(f"\nğŸ“ˆ æ”¶ç›Šç‡æ•°æ®: {len(result_df)} æ¡è®°å½•")
        return result_df

    def analyze_factor(self, factor_name: str, plot: bool = False):
        """åˆ†æå•ä¸ªå› å­"""
        print(f"\n{'=' * 60}")
        print(f"ğŸ” åˆ†æå› å­: {factor_name}")
        print(f"{'=' * 60}")

        # å‡†å¤‡å› å­æ•°æ® (å·²æ˜¯æ¯æ—¥æ—¶é—´åºåˆ—)
        factor_df = self.prepare_panel_data(factor_name)
        if factor_df.empty:
            print(f"âŒ å› å­ {factor_name} æ— æ•°æ®")
            return

        # å‡†å¤‡æ”¶ç›Šæ•°æ®
        returns_df = self.calculate_returns()

        # åˆå¹¶æ•°æ®
        # ç»Ÿä¸€åˆ—åä»¥ä¾›ä¸‹ä¸€æ­¥åˆ†æ
        merged_data = factor_df.merge(returns_df, on=['date', 'code'], how='inner')

        if merged_data.empty:
            print("âŒ æ— æœ‰æ•ˆæ•°æ®ç”¨äºåˆ†æ")
            dates = factor_df['date'].unique()
            if len(dates) <= 1:
                print("æç¤ºï¼šå½“å‰æ•°æ®é›†ä¸­äº¤æ˜“æ—¥è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œ T->T+1 çš„æ—¶é—´åºåˆ—åˆ†æã€‚")
            return

        print(f"âœ… åˆå¹¶æ•°æ®: {len(merged_data)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {merged_data['date'].min()} åˆ° {merged_data['date'].max()}")
        print(f"   è‚¡ç¥¨æ•°é‡: {merged_data['code'].nunique()}")

        # æ‰§è¡Œåˆ†æ
        results, daily_returns = self._perform_analysis(merged_data, factor_name)

        # å¯è§†åŒ–
        if plot:
            self._create_plots(results, factor_name, merged_data)
            if 'long_short_returns' in daily_returns and not daily_returns['long_short_returns'].empty:
                self._plot_cumulative_return(daily_returns['long_short_returns'], factor_name)

        self.factor_results[factor_name] = results
        return results

    def _perform_analysis(self, data: pd.DataFrame, factor_name: str) -> tuple[Dict, Dict]:
        """æ‰§è¡Œå› å­åˆ†æ (å·²ä¿®å¤ T->T+1 åŒ¹é…é€»è¾‘, å¢åŠ ICè‡ªç›¸å…³æ€§å’Œå¤šç©ºç»„åˆæ”¶ç›Šåºåˆ—)"""
        results = {}
        daily_returns = {}  # ç”¨äºå­˜å‚¨å¤šç©ºç»„åˆæ¯æ—¥æ”¶ç›Šåºåˆ—

        # ICåˆ†æ
        ic_series = []
        Rankic_series = []
        # è·å–æ‰€æœ‰å”¯ä¸€çš„ã€æŒ‰å‡åºæ’åˆ—çš„æ—¥æœŸ
        dates = sorted(data['date'].unique())

        # ç”¨äºåˆ†æ¡£åˆ†æ (æ¯æ—¥å¤šç©ºæ”¶ç›Š)
        long_short_daily_returns = []

        # æ³¨æ„ï¼šè¿™é‡Œ date[i] æ˜¯å› å­æ—¥æœŸ Tï¼Œdate[i+1] æ˜¯æ”¶ç›Šç‡æ—¥æœŸ T+1
        for i in range(len(dates) - 1):
            current_date = dates[i]
            next_date = dates[i + 1]

            # T æ—¥çš„å› å­å€¼
            current_factors = data[data['date'] == current_date][['code', factor_name]].copy()
            # T+1 æ—¥çš„æ”¶ç›Šç‡
            next_returns = data[data['date'] == next_date][['code', 'return']].copy()

            # åˆå¹¶ T æ—¥å› å­å’Œ T+1 æ—¥æ”¶ç›Š
            merged = current_factors.merge(next_returns, on='code', how='inner')

            if len(merged) > 20:  # æœ€å°è‚¡ç¥¨æ•°é‡è¦æ±‚ (ä¸ºåˆ†æ¡£å¤šç©ºç•™å‡ºä½™é‡)
                # --- RankIC è®¡ç®— ---
                ic = merged[factor_name].corr(merged['return'], method='pearson')
                Rankic = merged[factor_name].corr(merged['return'], method='spearman')
                if not np.isnan(ic):
                    ic_series.append(ic)
                if not np.isnan(Rankic):
                    Rankic_series.append(Rankic)

                # --- åˆ†æ¡£å¤šç©ºæ”¶ç›Šè®¡ç®— ---
                try:
                    # Tæ—¥åˆ†æ¡£
                    merged['decile'] = pd.qcut(
                        merged[factor_name], 5, labels=False, duplicates='drop'
                    )

                    # ç¡®ä¿åˆ†æ¡£åœ¨ 0-9 ä¹‹é—´
                    if merged['decile'].min() == 0 and merged['decile'].max() == 4:
                        # æœ€é«˜æ¡£ (9) å¹³å‡æ”¶ç›Š - æœ€ä½æ¡£ (0) å¹³å‡æ”¶ç›Š
                        return_4 = merged[merged['decile'] == 4]['return'].mean()
                        return_0 = merged[merged['decile'] == 0]['return'].mean()
                        long_short_return = return_4 - return_0
                        long_short_daily_returns.append(pd.Series(long_short_return, index=[next_date]))
                except Exception:
                    # å¯èƒ½æ•°æ®ä¸è¶³å¯¼è‡´åˆ†æ¡£å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸€å¤©
                    pass

        if ic_series:
            # ICåºåˆ—çš„ç´¢å¼•åº”è¯¥å¯¹åº”æ”¶ç›Šç‡æ—¥æœŸ (T+1 æ—¥)
            ic_series = pd.Series(ic_series, index=dates[1:len(ic_series) + 1])
            ic_series = ic_series[(ic_series != 0) & (ic_series.notna()) & (ic_series != '')]
            monthly_ic_mean = ic_series.resample('M').mean()
            annual_ic_mean = ic_series.resample('Y').mean()
            Rankic_series = pd.Series(Rankic_series, index=dates[1:len(Rankic_series) + 1])
            Rankic_series = Rankic_series[(Rankic_series != 0) & (ic_series.notna()) & (Rankic_series != '')]
            results['ic_series'] = ic_series
            results['annual_ic_mean'] = annual_ic_mean.mean()
            results['monthly_ic_mean'] = monthly_ic_mean.mean()
            results['Rankic_series'] = Rankic_series
            results['ic_mean'] = ic_series.mean()
            results['Rankic_mean'] = Rankic_series.mean()
            results['ic_std'] = ic_series.std()
            results['ir'] = results['ic_mean'] / results['ic_std'] if results['ic_std'] != 0 else 0
            results['win_rate'] = (ic_series > 0).mean() if results['ic_mean'] >= 0 else (ic_series < 0).mean()
            results['factor_direction'] = 'æ­£å› å­' if results['ic_mean'] > 0 else 'è´Ÿå› å­'

            # è¡¥å……ï¼šIC è‡ªç›¸å…³æ€§
            autocorr_1 = self._calculate_ic_autocorrelation(ic_series, lag=1)
            results['ic_autocorr_1'] = autocorr_1

            print(f"ğŸ“Š ICåˆ†æç»“æœ:")
            print(f"   ICå‡å€¼: {results['ic_mean']:.4f}")
            print(f"   æœˆåº¦ICå‡å€¼: {results['monthly_ic_mean']:.4f}")
            print(f"   å¹´åº¦ICå‡å€¼: {results['annual_ic_mean']:.4f}")
            print(f"   RankICå‡å€¼: {results['Rankic_mean']:.4f}")
            # print(f"   ICæ ‡å‡†å·®: {results['ic_std']:.4f}")
            print(f"   IR: {results['ir']:.3f}")
            print(f"   èƒœç‡: {results['win_rate']:.2%}")
            print(f"   ICè‡ªç›¸å…³æ€§ (Lag 1): {autocorr_1:.4f}")
            print(f"   å› å­æ–¹å‘: {results['factor_direction']}")

        # æ¯æ—¥å¤šç©ºç»„åˆæ”¶ç›Šåºåˆ—
        if long_short_daily_returns:
            daily_returns['long_short_returns'] = pd.concat(long_short_daily_returns)

        # åˆ†æ¡£åˆ†æ (ä¸ IC åˆ†æä½¿ç”¨ç›¸åŒçš„ T->T+1 é”™ä½é€»è¾‘)
        decile_returns_mean = self._decile_analysis_mean(data, factor_name)
        if decile_returns_mean is not None:
            results['decile_returns_mean'] = decile_returns_mean
            results['monotonicity'] = self._calculate_monotonicity(decile_returns_mean)
            print(f"   å•è°ƒæ€§: {results['monotonicity']:.3f}")

        return results, daily_returns

    def _calculate_ic_autocorrelation(self, ic_series: pd.Series, lag: int = 1) -> float:
        """
        è®¡ç®— IC åºåˆ—çš„è‡ªç›¸å…³æ€§
        """
        if len(ic_series) <= lag:
            return 0.0
        # å°†åºåˆ—é”™ä½ lag æœŸ
        lagged_ic = ic_series.shift(lag)
        # è®¡ç®— Pearson ç›¸å…³ç³»æ•° (IC å€¼æœ¬èº«å°±æ˜¯æ•°å€¼ï¼Œç”¨ Pearson å³å¯)
        # æ’é™¤ NaN å€¼
        valid_data = pd.DataFrame({'ic': ic_series, 'lagged_ic': lagged_ic}).dropna()

        if len(valid_data) < 2:
            return 0.0

        corr, _ = pearsonr(valid_data['ic'], valid_data['lagged_ic'])
        return corr

    def _decile_analysis_mean(self, data: pd.DataFrame, factor_name: str) -> Optional[pd.Series]:
        """åˆ†æ¡£ç»„åˆåˆ†æ (è¿”å›æ‰€æœ‰æœŸå¹³å‡æ”¶ç›Š)"""
        try:
            all_decile_returns = []
            dates = sorted(data['date'].unique())

            for i in range(len(dates) - 1):
                # T æ—¥çš„å› å­æ•°æ®
                current_data = data[data['date'] == dates[i]].copy()
                # T+1 æ—¥çš„æ”¶ç›Šæ•°æ®
                next_data = data[data['date'] == dates[i + 1]]

                if len(current_data) < 20:  # æœ€å°‘20åªè‚¡ç¥¨
                    continue

                # å½“å‰æœŸåˆ†æ¡£ (Tæ—¥)
                # ä½¿ç”¨ labels=False è¿”å›åˆ†æ¡£æ•°å­— 0-9
                current_data['decile'] = pd.qcut(
                    current_data[factor_name], 5, labels=False, duplicates='drop'
                )

                # åˆå¹¶ä¸‹ä¸€æœŸæ”¶ç›Š (T+1æ—¥)
                merged = current_data.merge(
                    next_data[['code', 'return']], on='code', suffixes=('', '_next'), how='inner'
                )

                if not merged.empty:
                    # è®¡ç®—æ¯ä¸ªåˆ†æ¡£åœ¨ T+1 æ—¥çš„å¹³å‡æ”¶ç›Š
                    decile_return = merged.groupby('decile')['return_next'].mean()
                    all_decile_returns.append(decile_return)

            if all_decile_returns:
                # å¯¹æ‰€æœ‰æ—¶é—´æ®µçš„å¹³å‡æ”¶ç›Šå–å¹³å‡
                # é‡æ–°è®¾ç½®ç´¢å¼•å
                mean_returns = pd.DataFrame(all_decile_returns).mean()
                mean_returns.index = [f'Decile {i + 1}' for i in mean_returns.index]
                return mean_returns

        except Exception as e:
            print(f"åˆ†æ¡£åˆ†æé”™è¯¯: {e}")

        return None

    def _calculate_monotonicity(self, decile_returns: pd.Series) -> float:
        """è®¡ç®—å•è°ƒæ€§"""
        if len(decile_returns) < 2:
            return 0
        # å¯¹åˆ†æ¡£æ•°å­— (0, 1, ..., 9) å’Œå¹³å‡æ”¶ç›Šè¿›è¡Œç›¸å…³æ€§åˆ†æ
        ranks = np.arange(len(decile_returns))
        correlation = np.corrcoef(ranks, decile_returns.values)[0, 1]
        return correlation if not np.isnan(correlation) else 0

    def _create_plots(self, results: Dict, factor_name: str, data: pd.DataFrame):
        # ç®€æ´ä¸‰è‰²é…è‰²
        colors = ['#1f77b4', '#d62728', '#7f7f7f']  # è“è‰²ã€çº¢è‰²ã€ç°è‰²
        # å›¾ï¼šåˆ†æ¡£ç»„åˆæ”¶ç›ŠæŸ±çŠ¶å›¾
        if 'decile_returns_mean' in results and len(results['decile_returns_mean']) > 0:
            plt.figure(figsize=(10, 7))  # å¢åŠ å›¾è¡¨å°ºå¯¸ï¼Œä¸ºæ ‡ç­¾ç•™å‡ºæ›´å¤šç©ºé—´
            decile_returns = results['decile_returns_mean']
            x_labels = [i for i in range(1, len(decile_returns) + 1)]
            # å¹³å‡æ—¥æ”¶ç›Šè½¬å¹´åŒ–æ”¶ç›Š
            annualized_returns = (1 + decile_returns.values) ** 252 - 1
            y_data_percent = annualized_returns * 100  # å¹´åŒ–ç™¾åˆ†æ¯”æ”¶ç›Š

            bar_width = 1.0
            bars = plt.bar(x_labels, y_data_percent,
                           width=bar_width,
                           color=colors[0], alpha=1, edgecolor='white', linewidth=0.5)

            # è´Ÿæ”¶ç›ŠæŸ±å­ç”¨çº¢è‰²
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if height < 0:
                    bar.set_color(colors[1])

            # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ ‡ç­¾ä½ç½®ä¼˜åŒ– (é˜²æ­¢é‡å ) ---
            # ç¡®å®šå›¾è¡¨çš„Yè½´å®é™…æ•°æ®æ˜¾ç¤ºèŒƒå›´ï¼Œä¸ºæ ‡ç­¾åç§»é‡æä¾›å‚è€ƒ
            y_min_data, y_max_data = np.min(y_data_percent), np.max(y_data_percent)
            y_data_span = y_max_data - y_min_data

            # è®¾ç½®ä¸€ä¸ªåŸºäºæ•°æ®èŒƒå›´çš„åŠ¨æ€æœ€å°åç§»é‡
            # ç¡®ä¿å³ä½¿æ•°æ®é‡å¾ˆå°ï¼Œæ ‡ç­¾ä¹Ÿèƒ½æœ‰æ¸…æ™°çš„é—´è·
            min_label_offset = max(y_data_span * 0.03, 0.005)  # æœ€å°åç§»é‡ä¸ºæ•°æ®èŒƒå›´çš„3%æˆ–0.005%

            for i, bar in enumerate(bars):
                height = bar.get_height()

                # æ ‡ç­¾æ–‡æœ¬
                label_text = f'{height:.2f}%'

                # æ ¹æ®æŸ±å­é«˜åº¦æ­£è´Ÿï¼Œå†³å®šæ ‡ç­¾çš„å‚ç›´å¯¹é½å’Œåˆå§‹åç§»æ–¹å‘
                if height >= 0:
                    va = 'bottom'
                    # æ ‡ç­¾æ”¾åœ¨æŸ±å­ä¸Šæ–¹çš„å›ºå®šåç§»é‡
                    offset = min_label_offset
                else:
                    va = 'top'
                    # æ ‡ç­¾æ”¾åœ¨æŸ±å­ä¸‹æ–¹çš„å›ºå®šåç§»é‡
                    offset = -min_label_offset

                # å¦‚æœæŸ±å­é«˜åº¦ä¸º0ï¼Œç‰¹æ®Šå¤„ç†æ ‡ç­¾ä½ç½®ï¼Œç›´æ¥åœ¨0çº¿ç¨ä¸Šæ–¹æ˜¾ç¤º
                if np.isclose(height, 0, atol=0.001):  # ä½¿ç”¨ np.isclose å¤„ç†æµ®ç‚¹æ•°æ¥è¿‘0çš„æƒ…å†µ
                    label_text = '0.00%'
                    va = 'bottom'
                    offset = min_label_offset  # ç¡®ä¿åœ¨0çº¿ä¹‹ä¸Š

                # ç»˜åˆ¶æ ‡ç­¾
                plt.text(bar.get_x() + bar.get_width() / 2, height + offset,
                         label_text, ha='center', va=va,
                         fontsize=10,
                         color='black'  # ç»Ÿä¸€æ ‡ç­¾é¢œè‰²
                         )

            plt.xlim(0.5, len(decile_returns) + 0.5)

            # --- å…³é”®ä¿®æ”¹ 2: ä¼˜åŒ–Yè½´èŒƒå›´å’Œåˆ»åº¦ ---
            # ç¡®ä¿Yè½´åŒ…å«æ‰€æœ‰æ•°æ®ç‚¹å’Œå…¶æ ‡ç­¾
            # è·å–æ‰€æœ‰æ ‡ç­¾çš„yä½ç½®ï¼Œä»¥ä¾¿ç¡®å®šä¸€ä¸ªåˆé€‚çš„Yè½´ä¸Šé™
            all_y_labels_pos = []
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if height >= 0:
                    all_y_labels_pos.append(height + min_label_offset)
                else:
                    all_y_labels_pos.append(height - min_label_offset)

            # ç»“åˆæ•°æ®æœ¬èº«çš„èŒƒå›´å’Œæ ‡ç­¾çš„æœ€é«˜/æœ€ä½ä½ç½®æ¥è®¾ç½®Yè½´
            current_y_min = y_min_data
            current_y_max = y_max_data

            # å¦‚æœæœ‰æ ‡ç­¾ï¼Œç¡®ä¿Yè½´èƒ½è¦†ç›–æ‰€æœ‰æ ‡ç­¾
            if all_y_labels_pos:
                current_y_max = max(current_y_max, np.max(all_y_labels_pos))
                current_y_min = min(current_y_min, np.min(all_y_labels_pos))

            # å¢åŠ é¢å¤–çš„ä¸Šä¸‹è¾¹è·ï¼Œä½¿å›¾è¡¨ä¸æ‹¥æŒ¤
            # æ ¹æ®å®é™…æ•°æ®èŒƒå›´åŠ¨æ€è°ƒæ•´ Y è½´çš„ä¸Šä¸‹è¾¹ç•Œ
            # é¿å…å½“æ‰€æœ‰æ”¶ç›Šéƒ½é›†ä¸­åœ¨ç‹­çª„åŒºé—´æ—¶ï¼Œå›¾è¡¨ä»ç„¶æ˜¾å¾—å¤ªç©º
            y_range_padding = (current_y_max - current_y_min) * 0.15  # å¢åŠ 15%çš„ä¸Šä¸‹è¾¹è·

            # ç¡®ä¿ Y è½´ä¸‹é™ä¸ä¼šè¿‡é«˜ï¼Œå¦‚æœæ‰€æœ‰å€¼éƒ½æ˜¯æ­£æ•°ï¼Œç¡®ä¿èƒ½çœ‹åˆ°0çº¿
            y_lower_bound = min(current_y_min - y_range_padding,
                                0 if y_min_data > 0 else current_y_min - y_range_padding)
            y_upper_bound = current_y_max + y_range_padding

            if y_lower_bound == y_upper_bound:  # é¿å…é™¤é›¶é”™è¯¯æˆ–èŒƒå›´ä¸º0
                y_lower_bound -= 0.1
                y_upper_bound += 0.1

            plt.ylim(y_lower_bound, y_upper_bound)

            # ç§»é™¤Yè½´åˆ»åº¦æ ‡ç­¾ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰æŸ±é¡¶æ ‡ç­¾
            plt.yticks([])

            plt.title(f'åˆ†æ¡£ç»„åˆå¹³å‡æ”¶ç›Š - {factor_name}', fontsize=16, fontweight='bold', pad=20)  # å¢åŠ æ ‡é¢˜ä¸å›¾è¡¨çš„é—´è·
            plt.xlabel('åˆ†æ¡£ (1:æœ€ä½, 5:æœ€é«˜)', fontsize=12)
            plt.ylabel('å¹´åŒ–å¹³å‡æ”¶ç›Š (%)', fontsize=12)
            plt.xticks(x_labels, fontsize=5)  # ç¡®ä¿Xè½´åˆ»åº¦æ˜¯ 1 åˆ° 10
            plt.axhline(y=0, color='black', linewidth=1)  # 0çº¿
            plt.grid(False)  # ç§»é™¤èƒŒæ™¯ç½‘æ ¼

            # è°ƒæ•´å¸ƒå±€ï¼Œé˜²æ­¢å…ƒç´ é‡å 
            plt.tight_layout()

            # ä¿å­˜åˆ°æ¡Œé¢
            save_path = f'C:/Users/cufet/Desktop/{factor_name}_åˆ†æ¡£æ”¶ç›Š.png'
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°æ¡Œé¢: {save_path}")
    #
    def _plot_cumulative_return(self, daily_long_short_returns: pd.Series, factor_name: str):
        """
        ç»˜åˆ¶å¤šç©ºç»„åˆçš„ç´¯è®¡èµ°åŠ¿å›¾ (å‡€å€¼æ›²çº¿)
        """
        # è®¡ç®—ç´¯è®¡æ”¶ç›Šç‡ (å‡€å€¼æ›²çº¿ï¼š(1 + R1) * (1 + R2) * ... - 1)
        # R = daily_returns
        # å‡€å€¼ = (1 + R).cumprod()
        cumulative_returns = (1 + daily_long_short_returns).cumprod()

        plt.figure(figsize=(10, 6))

        # ç»˜åˆ¶å‡€å€¼æ›²çº¿
        plt.plot(cumulative_returns.index, cumulative_returns.values,
                 color='#d62728', linewidth=2, label='å¤šç©ºç»„åˆå‡€å€¼')

        plt.title(f'å¤šç©ºç»„åˆç´¯è®¡å‡€å€¼èµ°åŠ¿ - {factor_name}', fontsize=14, fontweight='bold')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('ç´¯è®¡å‡€å€¼')
        plt.grid(False, alpha=0)
        plt.legend()

        # æ ¼å¼åŒ–xè½´æ—¥æœŸ
        plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))
        plt.gcf().autofmt_xdate()  # è‡ªåŠ¨æ—‹è½¬æ—¥æœŸæ ‡ç­¾

        # ä¿å­˜åˆ°æ¡Œé¢
        plt.savefig(f'C:/Users/cufet/Desktop/{factor_name}_ç´¯è®¡èµ°åŠ¿å›¾.png', dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°æ¡Œé¢: {factor_name}_ç´¯è®¡èµ°åŠ¿å›¾.png")

        # ----------------------------------------------------

    def _parse_sheet_and_fill(self, file_path, sheet_name: str) -> pd.DataFrame:
        """
        åŠ è½½å•ä¸ª Sheet å¹¶åº”ç”¨è„å®½è¡¨è§£æå’Œå¡«å……é€»è¾‘ã€‚
        è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå°†åŠ è½½å’Œè§£æé€»è¾‘é›†ä¸­èµ·æ¥ã€‚
        """
        try:
            # 1. ç¡®ä¿ä¼ å…¥ header=Noneï¼Œä¸ parse_dirty_wide_table çš„é€»è¾‘åŒ¹é…
            sheet_data = pd.read_excel(file_path, sheet_name=sheet_name, header=None)

            # 2. è°ƒç”¨æ ¸å¿ƒè§£æå‡½æ•°
            # æ³¨æ„ï¼šè¿™é‡Œçš„ sheet_name å·²ç»è¢«ä¼ å…¥ parse_dirty_wide_tableï¼Œå°†è§¦å‘å†…éƒ¨çš„ 0 å€¼å¡«å……é€»è¾‘
            parsed_data = self.parse_dirty_wide_table(sheet_data, sheet_name)

            # 3. è¿”å›ç©ºè¡¨æˆ–æœ‰æ•ˆæ•°æ®
            if parsed_data.empty:
                return pd.DataFrame()

            # ã€æ³¨æ„ã€‘ï¼šéšæœºæ¨¡æ‹Ÿé€»è¾‘å·²åŒ…å«åœ¨ parse_dirty_wide_table å†…éƒ¨ï¼Œè¿™é‡Œæ— éœ€é‡å¤è°ƒç”¨ã€‚

            return parsed_data

        except Exception as e:
            # æ•è·å•ä¸ª Sheet çš„é”™è¯¯ï¼Œæ‰“å°ä¿¡æ¯åè¿”å›ç©ºè¡¨
            print(f"    âŒ ã€{sheet_name}ã€‘è§£æå¤±è´¥ã€‚é”™è¯¯: {e}")
            return pd.DataFrame()

    def load_test_data(self):
        """
        ä¸“é—¨ç”¨äºæµ‹è¯•é›†çš„åŠ è½½æ–¹æ³•ï¼šæŒ‰æ–‡ä»¶éå†æ‰€æœ‰ Sheetï¼Œå¹¶ç¡®ä¿æ•°æ®è¢«æ­£ç¡®è§£æå’Œå¡«å……ã€‚
        """
        # 1. æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶ (.xlsx å’Œ .xls)
        excel_files = sorted(
            list(self.data_folder.glob("ä¸­è¯500_*.xlsx")) +
            list(self.data_folder.glob("ä¸­è¯500_*.xls"))
        )

        if not excel_files:
            print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ° Excel æ–‡ä»¶ã€‚æ­¤æ¡†æ¶ä¾èµ–å®Œæ•´ Excel æ–‡ä»¶ï¼Œè¯·ç¡®è®¤è·¯å¾„ã€‚")
            return

        print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        for file in excel_files:
            # 2. ã€æ ¸å¿ƒéœ€æ±‚ 1ï¼šå»æ‰â€œä¸­è¯500_â€å‰ç¼€ã€‘
            period_name = file.stem.replace("ä¸­è¯500_", "")
            print(f"\nğŸ“ åŠ è½½: {period_name}")

            period_data = {}
            try:
                xl_file = pd.ExcelFile(file)

                for sheet_name in xl_file.sheet_names:
                    if sheet_name == 'Sheet1':
                        # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è¡¨
                        stock_info = pd.read_excel(file, sheet_name=sheet_name)
                        period_data['stock_info'] = stock_info
                    else:
                        # å› å­ Sheetï¼šè°ƒç”¨è¾…åŠ©å‡½æ•°è¿›è¡ŒåŠ è½½å’Œå¡«å……
                        parsed_data = self._parse_sheet_and_fill(file, sheet_name)

                        if not parsed_data.empty:
                            period_data[sheet_name] = parsed_data

                # 3. å­˜å‚¨æ•°æ®
                self.all_data[period_name] = period_data

                # æ‰“å°åŠ è½½æˆåŠŸçš„å› å­é”®
                loaded_keys = [k for k in period_data.keys() if k != 'stock_info']
                print(f"    âœ… {period_name} æ–‡ä»¶åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(loaded_keys)} ä¸ªå› å­æ•°æ®é¡¹ã€‚Keys: {loaded_keys}")

            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file.name} å¤±è´¥: {e}")

        # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        if self.all_data:
            self._display_data_overview()

    def load_data(self):
        """
        åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ Excel æ–‡ä»¶ï¼Œè§£æï¼Œå¹¶è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆä¾‹å¦‚ç¼ºå¤±å€¼å¡«å……ï¼‰ã€‚
        """
        print(f"ğŸš€ å¼€å§‹åŠ è½½æ•°æ®...")

        data_files = list(self.data_folder.glob("*.xlsx"))
        print(f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        for file_path in data_files:
            file_name_without_ext = file_path.stem

            print(f"\nğŸ“ åŠ è½½: {file_name_without_ext}")

            # å‡è®¾æ¯ä¸ª Excel æ–‡ä»¶ä¸­åŒ…å«äº†æ‰€æœ‰å› å­å’Œæ”¶ç›Šç‡æ•°æ®
            try:
                raw_data = pd.read_excel(file_path, header=None)
                # ä½¿ç”¨ä½ å·²æœ‰çš„è§£ææ–¹æ³•
                data_period = self.parse_dirty_wide_table(raw_data)

                # å¯¹åŸå§‹è´¢åŠ¡æ•°æ®è¿›è¡Œéšæœºå‘å‰æ¨¡æ‹Ÿå¡«å……ï¼ˆè¿™éƒ¨åˆ†é€»è¾‘åº”è¯¥æ¥è‡ªä½ åŸæœ‰çš„ä»£ç ï¼‰
                # å‡è®¾ parse_dirty_wide_table è¿”å›çš„æ•°æ®åŒ…å«äº†æ‰€æœ‰åˆ—
                for factor_name in ['ROE', 'è‡ªç”±ç°é‡‘æµ', 'EPS', 'EBITDA']:
                    if factor_name in data_period:
                        print(f"Â  Â ğŸ”§ æ­£åœ¨å¯¹ {factor_name} è¿›è¡Œ **éšæœºå‘å‰æ¨¡æ‹Ÿ** å¡«å……...")
                        # å‡è®¾ä½ æœ‰ä¸€ä¸ª fill_random_forward æ–¹æ³•æˆ–ç±»ä¼¼é€»è¾‘
                        data_period[factor_name] = self.fill_random_forward(data_period[factor_name])

                self.all_data[file_name_without_ext] = data_period
                print(f"âœ… {file_name_without_ext} åŠ è½½å®Œæˆ")

            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

        print("\n" + "=" * 60)
        print("æ•°æ®åŠ è½½æ¦‚è§ˆ")
        print("=" * 60)
        for period_name in self.all_data.keys():
            print(f"ğŸ“… æ—¶é—´æ®µ: {period_name}")


    def aggregate_factor_panel(self, factor_names: List[str], target_return_name: str = 'return'):
        """
        èšåˆæ‰€æœ‰æ—¶é—´æ®µçš„å› å­æ•°æ®å’Œç›®æ ‡æ”¶ç›Šç‡ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚
        å°†å®½è¡¨å› å­æ•°æ®è½¬æ¢ä¸ºé•¿é¢æ¿ (MultiIndex=(æ—¥æœŸ, è‚¡ç¥¨ä»£ç ))ã€‚
        """
        all_factor_data_list = []

        # 1. å‡†å¤‡æ‰€æœ‰å› å­çš„é¢æ¿æ•°æ® (å‡è®¾ prepare_panel_data è¿”å›é•¿è¡¨ï¼ŒåŒ…å« 'date', 'code' åˆ—)
        factor_panel_data = {}
        for factor_name in factor_names:
            factor_panel_data[factor_name] = self.prepare_panel_data(factor_name)

        # 2. å‡†å¤‡æ”¶ç›Šç‡æ•°æ® (é•¿è¡¨)
        # å‡è®¾ calculate_returns è¿”å›çš„é•¿è¡¨ä¹ŸåŒ…å« 'date', 'code', 'return'
        returns_df = self.calculate_returns()

        # 3. å°†æ‰€æœ‰å› å­é¢æ¿åˆå¹¶åˆ°æ”¶ç›Šç‡ä¸Š
        final_merged_data = returns_df.copy()

        for factor_name, factor_df in factor_panel_data.items():
            if factor_df.empty:
                continue

            # ä½¿ç”¨ date å’Œ code ä½œä¸º merge çš„é”®
            final_merged_data = final_merged_data.merge(
                factor_df[['date', 'code', factor_name]],
                on=['date', 'code'],
                how='inner'
            )

        # 4. æ¸…ç†å’Œè¿”å›
        required_columns = factor_names + [target_return_name]
        final_data_clean = final_merged_data.dropna(subset=required_columns)

        X_final = final_data_clean[factor_names].copy()
        y_final = final_data_clean[target_return_name].copy()

        # **è‡´å‘½é”™è¯¯ä¿®æ­£ç‚¹ï¼šç¡®ä¿ç´¢å¼•æ­£ç¡®è®¾ç½®**
        # åœ¨ merge ä¹‹åï¼Œ'date' å’Œ 'code' ä»ç„¶æ˜¯åˆ—ã€‚ä¸ºäº†è®© LASSO ä½¿ç”¨ MultiIndexï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®å®ƒã€‚

        # æˆ‘ä»¬éœ€è¦è·å–å¯¹é½åçš„ (date, code) ç´¢å¼•ï¼Œå¹¶å°†å®ƒè®¾ç½®ç»™ X_final
        # æ­¤æ—¶ final_data_clean ä»æ˜¯å¸¦æœ‰ date, code åˆ—çš„ DataFrame

        X_final['date'] = final_data_clean['date']
        X_final['code'] = final_data_clean['code']
        y_final = y_final.set_axis(X_final.index)  # ç¡®ä¿ Series çš„ index æ˜¯å¯¹é½çš„

        if not X_final.empty:
            # **å…³é”®ä¿®å¤ï¼šå°† date å’Œ code è®¾ç½®ä¸º MultiIndex**
            X_final = X_final.set_index(['date', 'code'])
            y_final.index = X_final.index
        else:
            # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè¿”å›ç©ºå¯¹è±¡
            return pd.DataFrame(), pd.Series()

        print(f"âœ… èšåˆæ•°æ®å¯¹é½å®Œæˆã€‚ç”¨äºè®­ç»ƒçš„ (æ—¥æœŸ-è‚¡ç¥¨) æ ·æœ¬æ€»æ•°: {len(X_final)}")

        return X_final, y_final

def _get_aligned_data_for_lasso(analyzer, factor_names, target_return_name='return'):
    """
    ã€å¤–éƒ¨å®ç°æ•°æ®å¯¹é½ã€‘
    åˆ©ç”¨ analyzer çš„ç°æœ‰æ–¹æ³•ï¼Œå°†æ‰€æœ‰å› å­å’Œç›®æ ‡æ”¶ç›Šç‡åˆå¹¶å¹¶å¯¹é½æˆé¢æ¿æ•°æ®ã€‚

    Args:
        analyzer: CS500FactorAnalyzer å®ä¾‹ã€‚
        factor_names: ç”¨äº LASSO çš„å› å­åç§°åˆ—è¡¨ï¼ˆXï¼‰ã€‚
        target_return_name: ç›®æ ‡æ”¶ç›Šç‡çš„åˆ—åï¼ˆYï¼‰ã€‚

    Returns:
        X (pd.DataFrame): å¯¹é½åçš„ç‰¹å¾çŸ©é˜µã€‚
        Y (pd.Series): å¯¹é½åçš„ç›®æ ‡å˜é‡ã€‚
    """

    print("ğŸ” æ­£åœ¨èšåˆå› å­é¢æ¿æ•°æ®...")

    # 1. å‡†å¤‡æ‰€æœ‰å› å­æ•°æ® (X)
    all_factor_panels = []

    for factor_name in factor_names:
        # ä½¿ç”¨æ¡†æ¶è‡ªå¸¦çš„ prepare_panel_data è·å–é•¿è¡¨
        factor_panel = analyzer.prepare_panel_data(factor_name)
        if not factor_panel.empty:
            # ç¡®ä¿åªä¿ç•™æ—¥æœŸã€ä»£ç å’Œå› å­å€¼
            factor_panel = factor_panel[['date', 'code', factor_name]]
            all_factor_panels.append(factor_panel)

    if not all_factor_panels:
        print("âš ï¸ æ— æ³•è·å–ä»»ä½•å› å­æ•°æ®ã€‚")
        return pd.DataFrame(), pd.Series()

    # 2. ä¾æ¬¡åˆå¹¶æ‰€æœ‰å› å­é¢æ¿æ•°æ®
    merged_factors = all_factor_panels[0]
    for i in range(1, len(all_factor_panels)):
        merged_factors = merged_factors.merge(
            all_factor_panels[i],
            on=['date', 'code'],
            how='inner'  # ç¡®ä¿åªæœ‰æ‰€æœ‰å› å­éƒ½æœ‰å€¼çš„æ ·æœ¬è¢«ä¿ç•™
        )

    # 3. å‡†å¤‡ç›®æ ‡æ”¶ç›Šç‡ (Y)
    returns_df = analyzer.calculate_returns()

    # 4. åˆå¹¶å› å­å’Œæ”¶ç›Šç‡ï¼Œå®ç°æœ€ç»ˆå¯¹é½
    final_merged_data = merged_factors.merge(
        returns_df[['date', 'code', target_return_name]],
        on=['date', 'code'],
        how='inner'
    )

    # 5. æ¸…ç†å’Œåˆ†å‰²
    final_merged_data = final_merged_data.dropna()

    # åˆ†å‰² X (ç‰¹å¾) å’Œ Y (ç›®æ ‡)
    X = final_merged_data[['date', 'code'] + factor_names]
    Y = final_merged_data[target_return_name]

    print(f"âœ… æ•°æ®å¯¹é½å®Œæˆã€‚ç”¨äºè®­ç»ƒçš„ (æ—¥æœŸ-è‚¡ç¥¨) æ ·æœ¬æ€»æ•°: {len(X)}")

    return X, Y


# ----------------------------------------------------
# **ã€ä¿®å¤ 1ï¼šç¼ºå¤±çš„è®­ç»ƒæ•°æ®èšåˆæ–¹æ³•ã€‘**
# ----------------------------------------------------


def calculate_lasso_composite(
        analyzer,
        factor_names,
        alpha=0
):
    """
    è®­ç»ƒ LASSO æ¨¡å‹ï¼Œç”Ÿæˆå¤åˆå› å­ï¼Œå¹¶å°†æ¨¡å‹å¯¹è±¡ä¿å­˜åˆ°ç£ç›˜ã€‚

    æ³¨æ„ï¼šæ­¤å‡½æ•°è¿”å›çš„ç»“æœæ˜¯ä¸€ä¸ª Series/DataFrameï¼Œç”¨äº analyzer.calculate_synthetic_factor å­˜å‚¨ã€‚
    """

    # 1. æ•°æ®å¯¹é½ (è°ƒç”¨å¤–éƒ¨å‡½æ•°)
    # X_panel åŒ…å« ['date', 'code'] åˆ—ï¼ŒX_train åªåŒ…å«å› å­åˆ—
    X_panel, Y = analyzer._get_aligned_data_for_lasso(analyzer, factor_names, target_return_name='return')

    if X_panel.empty:
        print("âŒ æ— æ³•è®­ç»ƒ LASSOï¼šå¯¹é½åçš„æ•°æ®ä¸ºç©ºã€‚")
        # è¿”å›ä¸€ä¸ªç©ºçš„ DataFrame ä»¥æ»¡è¶³ calculate_synthetic_factor çš„è¦æ±‚
        return pd.DataFrame()

        # æå–çº¯ç‰¹å¾çŸ©é˜µè¿›è¡Œè®­ç»ƒ
    X_train = X_panel[factor_names].values

    # 2. æ‰§è¡Œ LASSO å›å½’
    print(f"\nğŸ§  å¼€å§‹ LASSO å›å½’ (Alpha={alpha})...")
    lasso_model = Lasso(alpha=alpha, max_iter=100)
    lasso_model.fit(X_train, Y.values)

    # 3. è®°å½•å’Œä¿å­˜æ¨¡å‹
    print(f"\n--- LASSO æ¨¡å‹ç»“æœ (Alpha={alpha}) ---")
    print(f"æˆªè· (Intercept): {lasso_model.intercept_:.6f}")
    print("é€‰å®šçš„å› å­åŠå…¶ç³»æ•°:")
    # æ‰“å°éé›¶ç³»æ•°
    for name, coef in zip(factor_names, lasso_model.coef_):
        if np.abs(coef) > 1e-6:
            print(f"  {name}: {coef:.6f}")
    # å°†æ¨¡å‹å¯¹è±¡å’Œç‰¹å¾é¡ºåºä¸€èµ·ä¿å­˜ï¼Œä»¥ä¾¿åœ¨æ–°æ•°æ®ä¸Šé‡ç”¨
    model_data = {
        'model': lasso_model,
        'feature_names': factor_names  # å…³é”®ï¼šä¿å­˜è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåº
    }
    MODEL_OUTPUT_PATH = "C:/Users/cufet/Desktop/lasso_composite_model.joblib"  # ç¡®ä¿è·¯å¾„å¯å†™
    joblib.dump(model_data, MODEL_OUTPUT_PATH)
    print(f"\nâœ… LASSO æ¨¡å‹å’Œç‰¹å¾é¡ºåºå·²ä¿å­˜åˆ°: {MODEL_OUTPUT_PATH}")
    # 4. ç”Ÿæˆå¤åˆå› å­å€¼
    # å¤åˆå› å­å€¼ = è®­ç»ƒé›†ä¸Šçš„é¢„æµ‹å€¼
    composite_factor_values = lasso_model.predict(X_train)
    # 5. ç»„è£…æˆæ—¶é—´åºåˆ—å®½è¡¨ (æ»¡è¶³ analyzer.calculate_synthetic_factor çš„è¦æ±‚)

    # åˆ›å»ºé•¿è¡¨ Series
    composite_factor_long = pd.Series(
        composite_factor_values,
        index=X_panel.index,  # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ index
        name="LASSO_Composite_Value"
    )

    # å°†æ—¥æœŸã€ä»£ç ã€å¤åˆå› å­å€¼åˆå¹¶
    result_long_df = X_panel[['date', 'code']].copy()
    result_long_df['LASSO_Composite'] = composite_factor_long.values

    # æ‰¾åˆ°å½“å‰ period çš„æ•°æ®å¹¶è½¬æ¢å›å®½è¡¨
    current_period_name = list(analyzer.all_data.keys())[0]  # è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶çš„ã€ä¸ä¸¥è°¨çš„å‡è®¾ï¼


    return result_long_df.set_index('date').pivot(columns='code', values='LASSO_Composite').fillna(
        0)  # è¿™æ˜¯ä¸€ä¸ªä¸ä¸¥è°¨çš„å®½è¡¨ï¼Œä½†ç»“æ„ä¸Šç¬¦åˆè¦æ±‚ã€‚

    # --- ä¿®æ­£åçš„æ¨¡å‹è®­ç»ƒå’Œåº”ç”¨å‡½æ•° ---

    # 1. è®­ç»ƒå’Œä¿å­˜æ¨¡å‹ (åªæ‰§è¡Œä¸€æ¬¡)


def train_and_save_lasso_model(analyzer, feature_names: List[str], alpha: float, model_path: str):
    """
    èšåˆå› å­é¢æ¿æ•°æ®ï¼Œå°†å®ƒä»¬ä¸æ”¶ç›Šç‡å¯¹é½ï¼Œæ ‡å‡†åŒ–åè®­ç»ƒ LASSO æ¨¡å‹ï¼Œå¹¶ä¿å­˜æ¨¡å‹å’Œ Scalerã€‚
    """
    print("\nğŸ” æ­£åœ¨èšåˆå› å­é¢æ¿æ•°æ®...")

    # 1. èšåˆå› å­æ•°æ®å’Œæ”¶ç›Šç‡æ•°æ®
    X_merged, y_merged = analyzer.aggregate_factor_panel(feature_names)

    if X_merged.empty or y_merged.empty:
        print("âš ï¸ è­¦å‘Šï¼šèšåˆæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ LASSO æ¨¡å‹ã€‚")
        return

    # å°†èšåˆåçš„æ•°æ®è½¬æ¢ä¸º NumPy æ•°ç»„
    X_values = X_merged.values
    y_values = y_merged.values

    # ----------------------------------------------------
    # **ã€ä¿®æ­£ 1ï¼šå…³é”®ï¼šå¯¹è¾“å…¥ç‰¹å¾ X è¿›è¡Œæ ‡å‡†åŒ– (Standardization)ã€‘**
    # ----------------------------------------------------
    # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
    scaler = StandardScaler()

    # å¯¹ç‰¹å¾æ•°æ®è¿›è¡Œæ‹Ÿåˆå’Œè½¬æ¢ (Fit and Transform)ï¼Œscaler ä¼šå­¦ä¹ æ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®
    X_values_standardized = scaler.fit_transform(X_values)

    print(f"âœ… æ•°æ®å¯¹é½å®Œæˆã€‚ç”¨äºè®­ç»ƒçš„ (æ—¥æœŸ-è‚¡ç¥¨) æ ·æœ¬æ€»æ•°: {X_values_standardized.shape[0]}")

    # 2. è®­ç»ƒ LASSO æ¨¡å‹
    # æç¤ºï¼šç”±äºæ•°æ®å·²æ ‡å‡†åŒ–ï¼Œalpha é€šå¸¸éœ€è¦è°ƒå°
    lasso_model = Lasso(alpha=alpha, max_iter=10000, random_state=42)

    # ç¡®ä¿æ¨¡å‹è®­ç»ƒä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®
    print(f"æ‹Ÿåˆ LASSO æ¨¡å‹ (æ ·æœ¬æ•°: {X_values_standardized.shape[0]}, ç‰¹å¾æ•°: {X_values_standardized.shape[1]})...")
    lasso_model.fit(X_values_standardized, y_values)

    # 3. ä¿å­˜æ¨¡å‹ã€ç‰¹å¾é¡ºåºå’Œ Scaler
    # å¿…é¡»ä¿å­˜ scalerï¼Œå› ä¸ºåº”ç”¨æ¨¡å‹æ—¶éœ€è¦ç”¨å®ƒæ¥è½¬æ¢æ–°çš„æ•°æ®ï¼
    joblib.dump({
        'model': lasso_model,
        'feature_names': feature_names,
        'scaler': scaler  # <-- å¿…é¡»ä¿å­˜ Scaler å¯¹è±¡
    }, model_path)

    # æ‰“å°æœ€ç»ˆçš„æ¨¡å‹å‚æ•°ä»¥ä¾›æ£€æŸ¥
    print(f"LASSOæ¨¡å‹å‚æ•°ï¼š {{'model': {lasso_model}, 'feature_names': {feature_names}}}")
    print(f"âœ… LASSO æ¨¡å‹ã€ç‰¹å¾é¡ºåºå’Œ Scaler å·²ä¿å­˜åˆ°: {model_path}")
# 2. æ¨¡å‹åº”ç”¨å‡½æ•° (ä¾› calculate_synthetic_factor è°ƒç”¨)
def apply_lasso_composite_to_period(analyzer,period_name, factor_names, model_path):
    """
    ä¸º CS500FactorAnalyzer.calculate_synthetic_factor æ‰€éœ€çš„è®¡ç®—å‡½æ•°ã€‚
    å®ƒæ¥æ”¶ä¸€ä¸ª period_dataï¼Œåº”ç”¨æ¨¡å‹å¹¶è¿”å›è¯¥ period çš„å®½è¡¨ç»“æœã€‚
    """
    try:
        # 1. åŠ è½½æ¨¡å‹å’Œç‰¹å¾é¡ºåº
        loaded_data = joblib.load(model_path)
        loaded_model = loaded_data['model']
        # ç¡®ä¿è¿™é‡ŒåŠ è½½çš„ç‰¹å¾é¡ºåºå’Œè®­ç»ƒæ—¶ä¸€è‡´
        TRAINING_FACTOR_NAMES = loaded_data['feature_names']

        # 2. å‡†å¤‡å½“å‰ period çš„ç‰¹å¾æ•°æ® (å®½è¡¨)
        period_factor_data = {}

        # è·å–å½“å‰ period çš„åŸå§‹æ•°æ®
        period_raw_data = analyzer.all_data.get(period_name, {})

        for factor_name in factor_names:
            # å°è¯•ä»**åŸºç¡€å› å­**ä¸­è·å–
            if factor_name in period_raw_data:
                period_factor_data[factor_name] = period_raw_data[factor_name]

            # å°è¯•ä» **å·²è®¡ç®—çš„åˆæˆå› å­** ä¸­è·å–
            elif factor_name in analyzer.synthetic_factors and period_name in analyzer.synthetic_factors[factor_name]:
                period_factor_data[factor_name] = analyzer.synthetic_factors[factor_name][period_name]

            # å¦‚æœæ‰¾ä¸åˆ°è¯¥å› å­ï¼Œåº”è¯¥æ‰“å°è­¦å‘Šæˆ–è·³è¿‡è¯¥å‘¨æœŸ
            else:
                # è¿™å¯èƒ½è¡¨ç¤ºè¿™ä¸ªå› å­åœ¨å½“å‰å‘¨æœŸç¡®å®æ²¡æœ‰è¢«æˆåŠŸè®¡ç®—
                print(f"   âš ï¸  å› å­ '{factor_name}' åœ¨å‘¨æœŸ '{period_name}' ç¼ºå¤±ï¼Œè·³è¿‡æ­¤å‘¨æœŸã€‚")
                return pd.DataFrame()

                # 3. å°†æ‰€æœ‰å®½è¡¨åˆå¹¶æˆä¸€ä¸ªå®½è¡¨
        X_wide = pd.concat(period_factor_data.values(), axis=1, keys=period_factor_data.keys()).swaplevel(axis=1)
        # ç¡®ä¿åªä¿ç•™ä¸ TRAINING_FACTOR_NAMES é¡ºåºä¸€è‡´çš„åˆ— (å»é‡å¹¶é‡æ–°æ’åº)
        X_wide = X_wide.reindex(columns=TRAINING_FACTOR_NAMES, level=0)
        X_wide.columns = X_wide.columns.droplevel(1)  # åˆ é™¤å¤šä½™çš„ç¬¬äºŒå±‚åˆ—å

        # 4. ç§»é™¤ç¼ºå¤±å€¼å¹¶è½¬æ¢ä¸º NumPy æ•°ç»„ (X)
        X_filled = X_wide.fillna(method='ffill')
        X_train_period = X_filled.dropna(how='any').copy()
        X_values = X_train_period.values
        if X_values.size == 0:
            return pd.DataFrame()

        # 5. åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        composite_values = loaded_model.predict(X_values)

        # 6. è½¬æ¢å›æ—¶é—´åºåˆ—å®½è¡¨ (Index=date, Columns=code)

        composite_df = pd.DataFrame(
            composite_values,
            index=X_train_period.index,
            columns=X_train_period.columns  # è¿™é‡Œçš„åˆ—åæ˜¯è‚¡ç¥¨ä»£ç 
        )

        # è¿”å›å®½è¡¨
        composite_df.index.name = 'date'
        composite_df.columns.name = 'code'

        return composite_df

    except Exception as e:
        print(f"âŒ LASSO åº”ç”¨å¤±è´¥: {e}")
        return pd.DataFrame()


# ç¡®ä¿åœ¨ main å‡½æ•°å¤–å®šä¹‰è¿™ä¸ªåˆ›å»ºå™¨å‡½æ•°ï¼Œå¹¶ä¸”ä¿æŒ joblib.load å’Œæ¨¡å‹å˜é‡çš„é—­åŒ…
def create_lasso_applier(analyzer, factor_names, model_path):
    # 1. æ¨¡å‹åŠ è½½å’Œå‚æ•°å‡†å¤‡ (åªæ‰§è¡Œä¸€æ¬¡)
    loaded_data = joblib.load(model_path)
    loaded_model = loaded_data['model']
    TRAINING_FACTOR_NAMES = loaded_data['feature_names']
    loaded_scaler = loaded_data['scaler']  # <-- å¿…é¡»åŠ è½½ Scaler

    # 2. è¿”å›å†…éƒ¨çš„ apply_func
    def apply_func(period_data):
        # A. æŸ¥æ‰¾å½“å‰çš„ period_name ï¼ˆé€šè¿‡å†…å­˜åœ°å€æ¯”è¾ƒï¼Œä¿æŒä¸å˜ï¼‰
        current_period_name = None
        for name, data in analyzer.all_data.items():
            if data is period_data:
                current_period_name = name
                break

        if current_period_name is None:
            return pd.DataFrame()

        # B. æ ¸å¿ƒï¼šä» analyzer çš„ä¸¤ä¸ªåœ°æ–¹è·å–æ•°æ®ï¼ˆä¿æŒä¸å˜ï¼‰
        period_factor_data = {}
        for factor_name in TRAINING_FACTOR_NAMES:
            if factor_name in period_data:
                period_factor_data[factor_name] = period_data[factor_name]
            elif factor_name in analyzer.synthetic_factors and current_period_name in analyzer.synthetic_factors[
                factor_name]:
                period_factor_data[factor_name] = analyzer.synthetic_factors[factor_name][current_period_name]
            else:
                # ç¼ºå°‘ä»»ä¸€å› å­åˆ™è¿”å›ç©º DataFrame
                # print(f"DEBUG: Period {current_period_name} missing factor {factor_name}")
                return pd.DataFrame()

        # ----------------------------------------------------
        # **ã€ä¿®æ­£åçš„æ ¸å¿ƒï¼šLong-Panel åˆå¹¶ã€é¢„æµ‹ã€Unstackã€‘**
        # ----------------------------------------------------

        # 3. å°†æ‰€æœ‰å®½è¡¨ï¼ˆIndex=Date, Columns=Codeï¼‰è½¬ä¸º Long-Panelï¼ˆMultiIndex=(Date, Code), Columns=Featureï¼‰
        X_long_list = []
        for factor_name, factor_df in period_factor_data.items():
            # ä½¿ç”¨ stack() å°†å®½è¡¨è½¬ä¸ºé•¿è¡¨ï¼Œå¹¶å°†å…¶å‘½åä¸ºå› å­åç§°
            long_factor = factor_df.stack().to_frame(factor_name)
            X_long_list.append(long_factor)

        # å°†æ‰€æœ‰é•¿è¡¨ç‰¹å¾åˆå¹¶æˆä¸€ä¸ªç‰¹å¾çŸ©é˜µ X_merged (æŒ‰ (Date, Code) ç´¢å¼•å¯¹é½)
        X_merged = pd.concat(X_long_list, axis=1)

        # ç¡®ä¿åˆ—é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
        X_merged = X_merged[TRAINING_FACTOR_NAMES]

        # 4. å¡«å……å¹¶æ¸…ç†ï¼ˆæ³¨æ„ï¼šffill é»˜è®¤åœ¨ axis=0 ä¸Šè¿è¡Œï¼Œå³æ—¶é—´è½´ï¼‰
        X_filled = X_merged.fillna(method='ffill').fillna(method='bfill')  # é¢å¤–å¢åŠ  bfill ç¡®ä¿é¦–æ—¥æ•°æ®å¯ç”¨
        X_train_period = X_filled.dropna(how='any').copy()
        X_values = X_train_period.values

        if X_values.size == 0:
            # print(f"DEBUG: Period {current_period_name} resulted in zero valid samples after dropna.")
            return pd.DataFrame()

            # 5. åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        composite_values = loaded_model.predict(X_values)

        # 6. è½¬æ¢å›æ—¶é—´åºåˆ—å®½è¡¨
        # composite_values æ˜¯é¢„æµ‹å¾—åˆ† (1D æ•°ç»„)ï¼Œç´¢å¼•æ˜¯ MultiIndex(Date, Code)
        composite_series = pd.Series(
            composite_values,
            index=X_train_period.index,  # MultiIndex (Date, Code)
            name='LASSO_Composite_Value'
        )

        # ä½¿ç”¨ unstack(level='code') å°† Series (MultiIndex) è½¬æ¢å›å®½è¡¨ (Index=Date, Columns=Code)
        composite_df = composite_series.unstack(level='code')

        # 7. æ¸…ç†å’Œè¿”å›
        composite_df.index.name = 'date'
        composite_df.columns.name = 'code'

        return composite_df

    return apply_func


def main():
    analyzer = CS500FactorAnalyzer(data_folder="C:/Users/cufet/Desktop/æŠ•èµ„ç»„åˆç®¡ç†")

    # 1. åŠ è½½æ•°æ®
    print("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®...")
    analyzer.load_all_periods()

    # 2. è®¡ç®—åˆæˆå› å­
    print("\nğŸ”§ å¼€å§‹è®¡ç®—åˆæˆå› å­...")
    # Alphaå› å­çš„è®¡ç®—é€»è¾‘å·²ä¿®æ”¹ä¸ºæ¯æ—¥æ»šåŠ¨
    analyzer.calculate_synthetic_factor("Alpha#1", analyzer.calculate_alpha1)
    analyzer.calculate_synthetic_factor("Alpha#2", analyzer.calculate_alpha2)
    analyzer.calculate_synthetic_factor("Alpha#3", analyzer.calculate_alpha3)
    analyzer.calculate_synthetic_factor("ROC6", analyzer.calculate_ROC6)
    analyzer.calculate_synthetic_factor("BIAS60", analyzer.calculate_BIAS60)
    analyzer.calculate_synthetic_factor("CCI20", analyzer.calculate_CCI20)
    analyzer.calculate_synthetic_factor("WVAD6", analyzer.calculate_WVAD6)
    analyzer.calculate_synthetic_factor("EP", analyzer.calculate_EP_Factor)
    analyzer.calculate_synthetic_factor("ROE", analyzer.calculate_ROE_Factor)
    analyzer.calculate_synthetic_factor("EPSå¢é•¿", analyzer.calculate_EPS_Growth_Factor)
    analyzer.calculate_synthetic_factor("Turnover20", analyzer.calculate_Turnover20_Factor)
    analyzer.calculate_synthetic_factor("åŠ¨é‡ä»·å€¼å¤åˆ", analyzer.calculate_momentum_value_composite)
    analyzer.calculate_synthetic_factor("æ³¢åŠ¨è°ƒæ•´ä»·å€¼", analyzer.calculate_vol_adjusted_value)
    analyzer.calculate_synthetic_factor("é‡ä»·èƒŒç¦»", analyzer.calculate_volume_price_divergence)
    analyzer.calculate_synthetic_factor("ç›ˆåˆ©å¢é•¿", analyzer.calculate_profitability_growth)

    global all_factors
    all_factors = [
        "Alpha#1", "Alpha#2", "Alpha#3", "ROC6", "BIAS60", "CCI20",
        "WVAD6", "EP", "ROE", "EPSå¢é•¿", "Turnover20",
        "åŠ¨é‡ä»·å€¼å¤åˆ", "æ³¢åŠ¨è°ƒæ•´ä»·å€¼", "é‡ä»·èƒŒç¦»", "ç›ˆåˆ©å¢é•¿"
    ]
    MODEL_PATH = "C:/Users/cufet/Desktop/lasso_composite_model.joblib"  # ä¿å­˜è·¯å¾„

    print("\nğŸ§  å¼€å§‹ LASSO è®­ç»ƒå’Œæ¨¡å‹ä¿å­˜...")
    train_and_save_lasso_model(analyzer, all_factors, alpha=0.0005, model_path=MODEL_PATH)

    # LASSO æ­¥éª¤ 3ï¼šè®¡ç®—å¹¶å­˜å‚¨å¤åˆå› å­ (åœ¨æ¡†æ¶å¾ªç¯ä¸­åº”ç”¨å·²ä¿å­˜çš„æ¨¡å‹)
    print("\nğŸ”§ å¼€å§‹è®¡ç®— LASSO å¤åˆå› å­ (åº”ç”¨æ¨¡å‹)...")
    # ä½¿ç”¨ lambda å‡½æ•°è°ƒç”¨åº”ç”¨é€»è¾‘ï¼Œå¹¶å°†æ¨¡å‹è·¯å¾„ä¼ é€’è¿›å»
    lasso_applier_func = create_lasso_applier(analyzer, all_factors, MODEL_PATH)

    analyzer.calculate_synthetic_factor(
        "LASSO_Composite",
        lasso_applier_func  # ä¼ å…¥é—­åŒ…å‡½æ•°
    )

    def print_lasso_coefficients(model_path):
        """åŠ è½½æ¨¡å‹å¹¶æ‰“å°æ¯ä¸ªå› å­çš„æƒé‡ç³»æ•°"""
        try:
            # 1. åŠ è½½ä¿å­˜çš„æ•°æ®
            loaded_data = joblib.load(model_path)
            loaded_model = loaded_data['model']
            feature_names = loaded_data['feature_names']

            # 2. æå–ç³»æ•°
            coefficients = loaded_model.coef_

            # 3. å°†å› å­åç§°å’Œç³»æ•°é…å¯¹
            coef_df = pd.DataFrame({
                'Factor': feature_names,
                'Coefficient': coefficients
            }).sort_values(by='Coefficient', key=lambda x: np.abs(x), ascending=False)

            print("\n============================================================")
            print(f" LASSO å¤åˆå› å­æƒé‡ (Alpha={loaded_model.alpha})")
            print("============================================================")
            print(coef_df.to_string(index=False))  # to_string ç¡®ä¿å®Œæ•´æ˜¾ç¤º
            print("============================================================")

            # è¡¥å……ï¼šæ£€æŸ¥æœ‰å¤šå°‘ä¸ªç³»æ•°è¢«å‹ç¼©åˆ°é›¶
            zero_count = (np.abs(coefficients) < 1e-6).sum()
            print(f"è¢« LASSO å‹ç¼©åˆ°é›¶çš„å› å­æ•°é‡ (ç»å¯¹å€¼ < 1e-6): {zero_count} / {len(coefficients)}")

        except Exception as e:
            print(f"âŒ æ‰“å° LASSO ç³»æ•°å¤±è´¥: {e}")

    # åœ¨åˆ†æå› å­ä¹‹å‰è°ƒç”¨è¿™ä¸ªå‡½æ•°
    print_lasso_coefficients(MODEL_PATH)

    # 3. åˆ†æå› å­
    print("\nğŸ“Š å¼€å§‹å› å­åˆ†æ...")
    # factors_to_analyze = ["Alpha#1"]  # , "Alpha#2", "Alpha#3"
    # factors_to_analyze = ["Alpha#2"]
    # factors_to_analyze = ["Alpha#3"]
    # factors_to_analyze = ["ROC6"]
    # factors_to_analyze = ["BIAS60"]
    # factors_to_analyze = ["CCI20"]
    # factors_to_analyze = ["WVAD6"]
    # factors_to_analyze = ["EP"]
    # factors_to_analyze = ["ROE"]
    # factors_to_analyze = ["EPSå¢é•¿"]
    # factors_to_analyze = ["Turnover20"]
    # factors_to_analyze = [
    #     "åŠ¨é‡ä»·å€¼å¤åˆ",
    #     "æ³¢åŠ¨è°ƒæ•´ä»·å€¼",
    #     "é‡ä»·èƒŒç¦»",
    #     "ç›ˆåˆ©å¢é•¿"
    # ]
    factors_to_analyze = ["LASSO_Composite"]
    for factor in factors_to_analyze:
        analyzer.analyze_factor(factor,plot=True)


if __name__ == "__main__":
    main()

#####ä»£ç ç»“æ„çš„é—®é¢˜ï¼Œç”¨ä¸‹é¢è¿™äº›ä»£ç åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('TkAgg')  # ä¿ç•™æ‚¨çš„TkAggè®¾ç½®
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Optional, Callable
import warnings
import os
from typing import Dict
from scipy.stats import pearsonr
from sklearn.linear_model import Lasso
import joblib # å¯¼å…¥ joblib ç”¨äºä¿å­˜æ¨¡å‹
from sklearn.preprocessing import StandardScaler
# é…ç½®ä¸­æ–‡æ˜¾ç¤ºå’Œè­¦å‘Šè¿‡æ»¤
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial']
matplotlib.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')

class CS500FactorAnalyzer:

    def __init__(self, data_folder: str):
        self.data_folder = Path(data_folder)
        self.all_data = {}
        self.factor_results = {}
        self.synthetic_factors = {}

    def parse_dirty_wide_table(self, raw_data: pd.DataFrame, sheet_name: str = None) -> pd.DataFrame:
        try:
            data = raw_data.copy()
            # 1. è®¾ç½®åˆ—åä¸ºè‚¡ç¥¨ä»£ç ï¼ˆç¬¬1è¡Œï¼Œç´¢å¼• 0ï¼‰
            data.columns = data.iloc[0].values
            # 2. åˆ é™¤å‰ä¸¤è¡Œï¼ˆä»£ç è¡Œå’Œåç§°è¡Œï¼‰
            clean_data = data.iloc[2:].copy()
            # 3. è®¾ç½®æ—¶é—´ç´¢å¼•ï¼ˆç¬¬ä¸€åˆ—ï¼‰
            time_col_name = clean_data.columns[0]
            clean_data = clean_data.set_index(time_col_name)
            cleaned_index = [val if not isinstance(val, tuple) else pd.NaT for val in clean_data.index]
            clean_data.index = pd.Index(cleaned_index)
            # 4. è§£ææ—¶é—´ç´¢å¼•å¹¶è½¬ä¸ºæ•°å€¼
            clean_data.index = pd.to_datetime(clean_data.index)
            # ç¡®ä¿æ•°æ®åˆ—æ˜¯æ•°å€¼ç±»å‹ï¼Œéæ•°å€¼è½¬ä¸º NaN
            clean_data = clean_data.apply(pd.to_numeric, errors='coerce')

            # 5. æ¸…ç†å’Œç»Ÿä¸€å‘½å (å…ˆåˆ é™¤å…¨ä¸ºç©ºçš„åˆ—ï¼Œå†è¿›è¡Œå¡«å……)
            clean_data = clean_data.dropna(axis=1, how='all')

            # ==================== 0å€¼å’Œç¼ºå¤±å€¼å¤„ç†é€»è¾‘ (æ–°å¢/ä¿®æ”¹) ====================
            target_sheets = ['ROE', 'è‡ªç”±ç°é‡‘æµ', 'EPS', 'EBITDA']

            if sheet_name in target_sheets:
                # é’ˆå¯¹ç‰¹å®š sheets: 0å€¼é‡‡ç”¨éšæœºæ¨¡æ‹Ÿå¡«å……
                print(f"   ğŸ”§ æ­£åœ¨å¯¹ {sheet_name} è¿›è¡Œ **éšæœºå‘å‰æ¨¡æ‹Ÿ** å¡«å……...")

                # æ‰¾å‡ºæ‰€æœ‰ 0 å€¼çš„ mask
                zero_mask = (clean_data == 0)

                for col in clean_data.columns:
                    series = clean_data[col]
                    zero_indices = series[zero_mask[col]].index

                    if not zero_indices.empty:
                        # 1. ä¸´æ—¶å°† 0 æ›¿æ¢ä¸º NaN
                        temp_series = series.replace(0, np.nan)

                        # 2. åå‘å¡«å…… (bfill) å¾—åˆ°â€œåé¢ç¬¬ä¸€ä¸ªéé›¶å€¼ Vâ€
                        # ç›¸å½“äº series.bfill() çš„æ•ˆæœ
                        next_non_zero = temp_series.iloc[::-1].ffill().iloc[::-1]

                        # 3. æå–å¯¹åº”äº 0 ç´¢å¼•çš„ V å€¼
                        next_non_zero_for_zeros = next_non_zero.loc[zero_indices]

                        # 4. ç”Ÿæˆå‡åŒ€éšæœºæ•° U(0, 1)
                        random_uniform = np.random.rand(len(zero_indices))

                        # 5. æ¨¡æ‹Ÿå€¼ = V + U(0, 1) - 0.5
                        simulated_values = next_non_zero_for_zeros + random_uniform - 0.5

                        # 6. å¡«å……å›åŸæ•°æ®
                        clean_data.loc[zero_indices, col] = simulated_values

                # 7. å¯¹å¯èƒ½å­˜åœ¨çš„åŸå§‹ NaN æˆ–åºåˆ—æœ«å°¾æœªè¢«å¡«å……çš„ 0 è¿›è¡Œæ ‡å‡†çš„å‰åå¡«å……
                clean_data = clean_data.fillna(method='ffill').fillna(method='bfill')

            else:
                # å…¶ä»– sheets: 0 å’Œ NaN éƒ½é‡‡ç”¨ ffill/bfill æ ‡å‡†å¡«å……
                # 1. å°† 0 è§†ä¸ºç¼ºå¤±å€¼ (NaN)ï¼Œä»¥ä¾¿ç»Ÿä¸€å¡«å……
                clean_data = clean_data.replace(0, np.nan)

                # 2. ffill å’Œ bfill å¡«å……
                # print(f"   ğŸ”§ æ­£åœ¨å¯¹ {sheet_name} è¿›è¡Œ **ffill/bfill** å¡«å…… ...")
                clean_data = clean_data.fillna(method='ffill').fillna(method='bfill')


            # ==================== 0å€¼å’Œç¼ºå¤±å€¼å¤„ç†é€»è¾‘ (ç»“æŸ) ====================

            # 6. æ¸…ç†å’Œç»Ÿä¸€å‘½å (ä¸åŸæœ‰é€»è¾‘ä¸€è‡´)
            clean_data.columns.name = 'code'
            clean_data.index.name = 'date'

            return clean_data

        except Exception as e:
            print(f"âŒ è§£æ {sheet_name} å¤±è´¥: {e}")
            raise

    def load_all_periods(self):
        """åŠ è½½æ‰€æœ‰æ—¶é—´ç‚¹çš„æ•°æ®"""

        # 1. æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶ (.xlsx å’Œ .xls)ï¼Œå¹¶æŒ‰åç§°æ’åº
        excel_files = sorted(
            list(self.data_folder.glob("ä¸­è¯500_*.xlsx")) +
            list(self.data_folder.glob("ä¸­è¯500_*.xls"))
        )

        if not excel_files:
            print(
                f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ° Excel æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è·¯å¾„ï¼š{self.data_folder} ä¸‹æ˜¯å¦å­˜åœ¨å‘½åä¸º 'ä¸­è¯500_*.xlsx' æˆ– 'ä¸­è¯500_*.xls' çš„æ–‡ä»¶ã€‚")
            return

        print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        for file in excel_files:
            # ã€ä¿®æ”¹ 1ï¼šæå–å‘¨æœŸåç§°ï¼Œå¹¶å»æ‰ "ä¸­è¯500_" å‰ç¼€ã€‘
            period_name = file.stem.replace("ä¸­è¯500_", "")
            print(f"\nğŸ“ åŠ è½½: {period_name}")

            try:
                xl_file = pd.ExcelFile(file)
                period_data = {}

                for sheet_name in xl_file.sheet_names:
                    # Sheet1 é€šå¸¸æ˜¯è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
                    if sheet_name == 'Sheet1':
                        stock_info = pd.read_excel(file, sheet_name=sheet_name)
                        period_data['stock_info'] = stock_info
                    else:
                        # å…¶ä»– Sheet æ˜¯å› å­æ•°æ®ï¼Œéœ€è¦è„å®½è¡¨è§£æ
                        # ç¡®ä¿ä¼ å…¥ header=Noneï¼Œä¸ parse_dirty_wide_table åŒ¹é…
                        sheet_data = pd.read_excel(file, sheet_name=sheet_name, header=None)
                        try:
                            # è°ƒç”¨è§£æå‡½æ•°ã€‚æ­¤æ—¶ï¼Œsheet_name è¢«ä¼ å…¥ï¼Œç”¨äºè§¦å‘ parse_dirty_wide_table ä¸­çš„éšæœºæ¨¡æ‹Ÿé€»è¾‘
                            parsed_data = self.parse_dirty_wide_table(sheet_data, sheet_name)

                            # å¦‚æœè§£æåæ•°æ®ä¸ºç©ºï¼Œåˆ™è·³è¿‡
                            if parsed_data.empty:
                                continue

                            period_data[sheet_name] = parsed_data

                        except Exception:
                            # æ•è·è§£æ Sheet å†…éƒ¨çš„é”™è¯¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª Sheet
                            continue

                # ã€ä¿®æ”¹ 2ï¼šä½¿ç”¨å»æ‰å‰ç¼€çš„ period_name ä½œä¸º all_data çš„ Keyã€‘
                self.all_data[period_name] = period_data

                # æ‰“å°åŠ è½½æˆåŠŸçš„å› å­é”®
                loaded_keys = [k for k in period_data.keys() if k != 'stock_info']

                # ã€ä¿®æ”¹ 3ï¼šåœ¨åŠ è½½å®Œæˆçš„è¾“å‡ºä¸­ï¼Œä¹Ÿä½¿ç”¨å»æ‰å‰ç¼€çš„ period_nameã€‘
                print(f"    âœ… {period_name} æ–‡ä»¶åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(loaded_keys)} ä¸ªå› å­æ•°æ®é¡¹ã€‚Keys: {loaded_keys}")


            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file.name} å¤±è´¥: {e}")

        # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        if self.all_data:
            self._display_data_overview()
        else:
            print("\nâš ï¸  æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®")


    def _display_data_overview(self):
        """æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ"""
        print(f"\n{'=' * 60}")
        print("æ•°æ®åŠ è½½æ¦‚è§ˆ")
        print(f"{'=' * 60}")

        for period_name, period_data in self.all_data.items():
            print(f"\nğŸ“… æ—¶é—´æ®µ: {period_name}")

    def calculate_synthetic_factor(self, factor_name: str, calculation_func: Callable):
        """
        è®¡ç®—åˆæˆå› å­
        """
        print(f"\nğŸ”§ è®¡ç®—åˆæˆå› å­: {factor_name}")
        self.synthetic_factors[factor_name] = {}

        success_count = 0
        for period_name, period_data in self.all_data.items():
            try:
                # ä¼ å…¥ period_data (åŒ…å«æ‰€æœ‰æ—¶é—´åºåˆ—å®½è¡¨)
                # factor_data = calculation_func(period_data)
                factor_data = calculation_func(self, period_data)

                # æ£€æŸ¥è¿”å›ç»“æœæ˜¯å¦ä¸ºæ¯æ—¥æ—¶é—´åºåˆ—å®½è¡¨
                if not factor_data.empty and factor_data.index.name == 'date' and factor_data.columns.name == 'code':
                    self.synthetic_factors[factor_name][period_name] = factor_data
                    success_count += 1
                    print(
                        f"   âœ… {period_name}: è®¡ç®—æˆåŠŸ, æ¯æ—¥æˆªé¢: {len(factor_data)}, è‚¡ç¥¨æ•°: {len(factor_data.columns)}")
                else:
                    print(f"   âš ï¸  {period_name}: æ— æœ‰æ•ˆæ•°æ®æˆ–æ•°æ®æ ¼å¼é”™è¯¯")
            except Exception as e:
                print(f"   âŒ {period_name}: è®¡ç®—å¤±è´¥ - {e}")

        print(f"ğŸ“ˆ {factor_name} è®¡ç®—å®Œæˆ: {success_count}/{len(self.all_data)} ä¸ªæ—¶é—´æ®µ")

    # ==================== æ¯æ—¥æ»šåŠ¨å› å­è®¡ç®—æ–¹æ³• ====================
    def calculate_alpha1(self, period_data: Dict) -> pd.DataFrame:
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        # 1. ç¡®ä¿æ•°æ®å¯¹é½
        common_stocks = close_df.columns.intersection(volume_df.columns)
        close_df = close_df[common_stocks]
        volume_df = volume_df[common_stocks]
        close_rank_cs = close_df.rank(axis=1, pct=True)  # æ¯å¤©åœ¨æ‰€æœ‰è‚¡ç¥¨ä¸­æ’å
        volume_rank_cs = volume_df.rank(axis=1, pct=True)

        # 1b. è®¡ç®—è¿™ä¸¤ç»„æ’ååºåˆ—åœ¨è¿‡å»10å¤©çš„**æ—¶é—´åºåˆ—**ç›¸å…³æ€§ (rolling(10).corr())
        # è¿™æ ·è®¡ç®—çš„æ˜¯**æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°** (Spearman's rank correlation coefficient)
        # å¯¹æ¯åªè‚¡ç¥¨ï¼Œè®¡ç®—å…¶è¿‡å»10å¤©çš„ (close_rank_cs, volume_rank_cs) çš„ç›¸å…³æ€§ã€‚
        corr_term_raw = close_rank_cs.rolling(window=10).corr(volume_rank_cs)

        # --- Step 2: rank(correlation_term) * rank(delta_term) ---

        # 2a. å¯¹ç›¸å…³æ€§é¡¹è¿›è¡Œ**æˆªé¢**æ’å: rank(corr_term_raw)
        rank_corr_term = corr_term_raw.rank(axis=1, pct=True)

        # 2b. æ»šåŠ¨è®¡ç®—ä»·æ ¼å˜åŒ–é¡¹: delta(close, 5)
        delta_term_raw = close_df.diff(5)

        # 2c. å¯¹ä»·æ ¼å˜åŒ–é¡¹è¿›è¡Œ**æˆªé¢**æ’å: rank(delta(close, 5))
        rank_delta_term = delta_term_raw.rank(axis=1, pct=True)

        # 2d. ç»„åˆ: rank(correlation(...)) * rank(delta(...))
        raw_factor_df = rank_corr_term * rank_delta_term

        # --- Step 3: æœ€ç»ˆå› å­å€¼ (åŸå§‹å› å­å€¼æ˜¯æ¯æ—¥æˆªé¢å€¼) ---
        # å¯¹æœ€ç»ˆç»“æœè¿›è¡Œæˆªé¢æ’åï¼Œè¿™æ˜¯é€šå¸¸çš„åšæ³•ï¼Œä»¥ç¡®ä¿å› å­å€¼åˆ†å¸ƒç»Ÿä¸€
        alpha1_df = raw_factor_df.rank(axis=1, pct=True, method='average')

        # æ¸…ç†å’Œç»Ÿä¸€å‘½å
        alpha1_df.columns.name = 'code'
        alpha1_df.index.name = 'date'

        # åˆ é™¤æ‰€æœ‰è‚¡ç¥¨éƒ½æ˜¯ NaN çš„æ—¥æœŸ
        return alpha1_df.dropna(how='all')

    def calculate_alpha2(self, period_data: Dict) -> pd.DataFrame:
        """
        Alpha#2 (æ¯æ—¥æ»šåŠ¨): rank(delta(close, 5)) * rank(delta(volume, 5))
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        common_stocks = close_df.columns.intersection(volume_df.columns)
        close_df = close_df[common_stocks]
        volume_df = volume_df[common_stocks]

        # 1. 5æ—¥ä»·æ ¼å˜åŒ–: delta(close, 5)
        delta_close = close_df.diff(5)
        # 2. 5æ—¥æˆäº¤é‡å˜åŒ–: delta(volume, 5)
        delta_volume = volume_df.diff(5)

        # 3. æˆªé¢æ’å (Cross-sectional Rank): è¿™æ˜¯å…³é”®ï¼
        # rank(delta(close, 5))
        ranked_delta_close = delta_close.rank(axis=1, pct=True, method='average')
        # rank(delta(volume, 5))
        ranked_delta_volume = delta_volume.rank(axis=1, pct=True, method='average')

        # 4. æœ€ç»ˆå› å­å€¼: ä¸¤ä¸ªæ’åå€¼çš„ä¹˜ç§¯
        alpha2_df = ranked_delta_close * ranked_delta_volume

        alpha2_df.columns.name = 'code'
        alpha2_df.index.name = 'date'

        # åˆ é™¤æ‰€æœ‰è‚¡ç¥¨éƒ½æ˜¯ NaN çš„æ—¥æœŸ
        return alpha2_df.dropna(how='all')

    def calculate_alpha3(self, period_data: Dict) -> pd.DataFrame:
        """
        Alpha#3 (æ¯æ—¥æ»šåŠ¨): rank(stddev(close, 10)) * rank(delta(volume, 5))
        è¿™é‡Œæˆ‘ä»¬è®¡ç®—ï¼šrank(stddev(close, 10)) * rank(delta(volume, 5))
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        common_stocks = close_df.columns.intersection(volume_df.columns)
        close_df = close_df[common_stocks]
        volume_df = volume_df[common_stocks]

        # 1. æ»šåŠ¨è®¡ç®—10æ—¥æ³¢åŠ¨ç‡: stddev(close, 10)
        volatility_term = close_df.rolling(window=10).std()

        # 2. æ»šåŠ¨è®¡ç®—5æ—¥æˆäº¤é‡å˜åŒ–: delta(volume, 5)
        delta_volume_term = volume_df.diff(5)

        # 3. æˆªé¢æ’å: åˆ†åˆ«å¯¹ä¸¤ä¸ªé¡¹è¿›è¡Œæ¯æ—¥æˆªé¢æ’å
        # rank(stddev(close, 10))
        ranked_volatility = volatility_term.rank(axis=1, pct=True, method='average')

        # rank(delta(volume, 5))
        ranked_delta_volume = delta_volume_term.rank(axis=1, pct=True, method='average')

        # 4. æœ€ç»ˆå› å­å€¼: ä¸¤ä¸ªæ’åå€¼çš„ä¹˜ç§¯
        alpha3_df = ranked_volatility * ranked_delta_volume

        alpha3_df.columns.name = 'code'
        alpha3_df.index.name = 'date'

        return alpha3_df.dropna(how='all')

    def calculate_ROC6(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: ROC6 (6å‘¨æœŸä»·æ ¼å˜åŒ–ç‡)
        å…¬å¼: (æ”¶ç›˜ä»· / Nå‘¨æœŸå‰æ”¶ç›˜ä»· - 1) * 100
        """
        if 'æ”¶ç›˜ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']

        # è®¡ç®— N=6 å‘¨æœŸå‰çš„æ”¶ç›˜ä»·
        close_lagged = close_df.shift(6)

        # è®¡ç®—ROC6
        ROC6_df = ((close_df / close_lagged) - 1) * 100

        # æˆªé¢æ’å (å¯é€‰ï¼Œä½†é€šå¸¸å› å­éƒ½éœ€è¦æ’åæˆ–æ ‡å‡†åŒ–)
        factor_df = ROC6_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')
    def calculate_BIAS60(self, period_data: Dict) -> pd.DataFrame:
        """calculate_synthetic_factor
        å› å­: BIAS60 (60å‘¨æœŸä»·æ ¼ä¹–ç¦»ç‡)
        å…¬å¼: (æ”¶ç›˜ä»· - 60å‘¨æœŸå‡ä»·) / 60å‘¨æœŸå‡ä»· * 100
        """
        if 'æ”¶ç›˜ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']

        # è®¡ç®—60å‘¨æœŸç®€å•ç§»åŠ¨å¹³å‡çº¿ (Simple Moving Average, SMA)
        MA60 = close_df.rolling(window=60).mean()

        # è®¡ç®—BIAS60
        BIAS60_df = ((close_df - MA60) / MA60) * 100

        # æˆªé¢æ’å
        factor_df = BIAS60_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')
    def calculate_CCI20(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: CCI20 (20å‘¨æœŸå•†å“é€šé“æŒ‡æ•°)
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æ—¥æœ€é«˜ä»·' not in period_data or 'æ—¥æœ€ä½ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·ã€æœ€é«˜ä»·æˆ–æœ€ä½ä»·æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        high_df = period_data['æ—¥æœ€é«˜ä»·']
        low_df = period_data['æ—¥æœ€ä½ä»·']
        N = 20

        # 1. è®¡ç®— Typical Price (TP)
        TP_df = (high_df + low_df + close_df) / 3

        # 2. è®¡ç®— TP çš„ N å‘¨æœŸ SMA (SMATP)
        SMATP_df = TP_df.rolling(window=N).mean()

        # 3. è®¡ç®— N å‘¨æœŸå¹³å‡ç»å¯¹åå·® (Mean Deviation)
        # MeanDeviation = SMA(|TP - SMATP|, N)
        MD_df = (TP_df - SMATP_df).abs().rolling(window=N).mean()

        # 4. è®¡ç®— CCI
        # é¿å…é™¤ä»¥é›¶
        MD_df_safe = MD_df.replace(0, np.nan)
        CCI20_df = (TP_df - SMATP_df) / (0.015 * MD_df_safe)

        # æˆªé¢æ’å
        factor_df = CCI20_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')
    def calculate_WVAD6(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: WVAD (6å‘¨æœŸåŠ æƒæˆäº¤é‡å˜å¼‚åº¦ - å¹³æ»‘å½¢å¼)
        å…¬å¼: SMA( ( (Close - Open) / (High - Low) ) * Volume , 6 )
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'å¼€ç›˜ä»·' not in period_data or \
                'æ—¥æœ€é«˜ä»·' not in period_data or 'æ—¥æœ€ä½ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘ä»·æ ¼æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        open_df = period_data['å¼€ç›˜ä»·']
        high_df = period_data['æ—¥æœ€é«˜ä»·']
        low_df = period_data['æ—¥æœ€ä½ä»·']
        volume_df = period_data['æˆäº¤é‡']
        N = 6

        # 1. è®¡ç®—æ ¸å¿ƒæ¯”ç‡: (Close - Open) / (High - Low)
        # é¿å…é™¤ä»¥é›¶: å½“ High=Low æ—¶ï¼Œè¯¥æ¯”ç‡ä¸º NaN (æˆ–è®¾ä¸º0)
        price_range = high_df - low_df
        # ä½¿ç”¨ np.divide å®‰å…¨åœ°æ‰§è¡Œé™¤æ³•ï¼Œå¹¶ç”¨ where é¿å…é™¤é›¶
        ratio_df = np.divide(close_df - open_df, price_range,
                             out=np.zeros_like(price_range, dtype=float),
                             where=price_range != 0)

        # 2. è®¡ç®—åŠ æƒæˆäº¤é‡: Ratio * Volume
        weighted_volume = ratio_df * volume_df

        # 3. è®¡ç®— N=6 å‘¨æœŸç§»åŠ¨å¹³å‡ (SMA)
        WVAD_df = weighted_volume.rolling(window=N).mean()

        # æˆªé¢æ’å
        factor_df = WVAD_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_EP_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: EP (ç›ˆåˆ©æ”¶ç›Šç‡)
        å…¬å¼: 1 / PEï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'PE' not in period_data:
            raise ValueError("ç¼ºå°‘PEæ•°æ®")

        PE_df = period_data['PE']

        # é¿å…é™¤é›¶å’Œæç«¯å€¼ï¼šPE > 0 æ‰æœ‰æ„ä¹‰
        EP_df = 1.0 / PE_df.mask(PE_df <= 0)

        # æˆªé¢æ’å (rank(axis=1))
        factor_df = EP_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_ROE_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: ROE (å‡€èµ„äº§æ”¶ç›Šç‡)
        å…¬å¼: ROEï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'ROE' not in period_data:
            raise ValueError("ç¼ºå°‘ROEæ•°æ®")

        ROE_df = period_data['ROE']

        # å¯¹ROEè¿›è¡Œæˆªé¢æ’å
        factor_df = ROE_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_EPS_Growth_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: EPS_Growth (250æ—¥EPSå¢é•¿ç‡)
        å…¬å¼: (EPS / 250å‘¨æœŸå‰EPS) - 1ï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'EPS' not in period_data:
            raise ValueError("ç¼ºå°‘EPSæ•°æ®")

        EPS_df = period_data['EPS']

        # EPS è¿‡å» 250 ä¸ªå‘¨æœŸï¼ˆçº¦ä¸€å¹´ï¼‰å‰çš„æ•°å€¼
        EPS_lagged = EPS_df.shift(30)

        # è®¡ç®—å¢é•¿ç‡
        growth_df = (EPS_df / EPS_lagged) - 1

        # æˆªé¢æ’å
        factor_df = growth_df.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_Turnover20_Factor(self, period_data: Dict) -> pd.DataFrame:
        """
        å› å­: Turnover20 (20æ—¥å¹³å‡æ¢æ‰‹ç‡)
        å…¬å¼: 20æ—¥æ¢æ‰‹ç‡çš„ç®€å•ç§»åŠ¨å¹³å‡ (SMA)ï¼Œç„¶åè¿›è¡Œæˆªé¢æ’åã€‚
        """
        if 'æ¢æ‰‹ç‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ¢æ‰‹ç‡æ•°æ®")

        turnover_df = period_data['æ¢æ‰‹ç‡']

        # è®¡ç®—20æ—¥ç®€å•ç§»åŠ¨å¹³å‡
        SMA_turnover = turnover_df.rolling(window=20).mean()

        # æˆªé¢æ’å
        factor_df = SMA_turnover.rank(axis=1, pct=True)

        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_momentum_value_composite(self, period_data: Dict) -> pd.DataFrame:
        """
        åŠ¨é‡-ä»·å€¼å¤åˆå› å­
        å…¬å¼: rank(ROC20) * rank(1/PE)
        ç»“åˆä¸­æœŸåŠ¨é‡ä¸ä»·å€¼ä½ä¼°
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'PE' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–PEæ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        pe_df = period_data['PE']

        # 20æ—¥åŠ¨é‡
        roc20 = (close_df / close_df.shift(20) - 1)

        # ä»·å€¼å› å­ (ç›ˆåˆ©æ”¶ç›Šç‡)
        ep = 1.0 / pe_df.mask(pe_df <= 0)

        # å¤åˆå› å­
        composite = roc20.rank(axis=1, pct=True) * ep.rank(axis=1, pct=True)

        factor_df = composite.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_vol_adjusted_value(self, period_data: Dict) -> pd.DataFrame:
        """
        æ³¢åŠ¨ç‡è°ƒæ•´ä»·å€¼å› å­
        å…¬å¼: rank(EP) / rank(æ³¢åŠ¨ç‡)
        åœ¨ä»·å€¼å› å­ä¸Šè€ƒè™‘é£é™©è°ƒæ•´
        """
        if 'PE' not in period_data or 'æ”¶ç›˜ä»·' not in period_data:
            raise ValueError("ç¼ºå°‘PEæˆ–æ”¶ç›˜ä»·æ•°æ®")

        pe_df = period_data['PE']
        close_df = period_data['æ”¶ç›˜ä»·']

        # ç›ˆåˆ©æ”¶ç›Šç‡
        ep = 1.0 / pe_df.mask(pe_df <= 0)

        # 20æ—¥æ³¢åŠ¨ç‡
        volatility = close_df.pct_change().rolling(20).std()

        # æ³¢åŠ¨ç‡è°ƒæ•´ä»·å€¼
        vol_adjusted_ep = ep.rank(axis=1, pct=True) / (volatility.rank(axis=1, pct=True) + 1e-6)

        factor_df = vol_adjusted_ep.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_volume_price_divergence(self, period_data: Dict) -> pd.DataFrame:
        """
        é‡ä»·èƒŒç¦»å› å­
        å…¬å¼: rank(ä»·æ ¼åŠ¨é‡) - rank(æˆäº¤é‡åŠ¨é‡)
        æ•æ‰é‡ä»·èƒŒç¦»çš„æŠ€æœ¯ä¿¡å·
        """
        if 'æ”¶ç›˜ä»·' not in period_data or 'æˆäº¤é‡' not in period_data:
            raise ValueError("ç¼ºå°‘æ”¶ç›˜ä»·æˆ–æˆäº¤é‡æ•°æ®")

        close_df = period_data['æ”¶ç›˜ä»·']
        volume_df = period_data['æˆäº¤é‡']

        # 5æ—¥ä»·æ ¼åŠ¨é‡
        price_momentum = close_df.pct_change(5)

        # 5æ—¥æˆäº¤é‡åŠ¨é‡
        volume_momentum = volume_df.pct_change(5)

        # é‡ä»·èƒŒç¦»
        divergence = price_momentum.rank(axis=1, pct=True) - volume_momentum.rank(axis=1, pct=True)

        factor_df = divergence.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def calculate_profitability_growth(self, period_data: Dict) -> pd.DataFrame:
        """
        ç›ˆåˆ©èƒ½åŠ›å¢é•¿å› å­
        å…¬å¼: rank(ROEå˜åŒ–ç‡) * rank(EBITDAå¢é•¿ç‡)
        æ•æ‰ç›ˆåˆ©èƒ½åŠ›çš„æ”¹å–„è¶‹åŠ¿
        """
        if 'ROE' not in period_data or 'EBITDA' not in period_data:
            raise ValueError("ç¼ºå°‘ROEæˆ–EBITDAæ•°æ®")

        roe_df = period_data['ROE']
        ebitda_df = period_data['EBITDA']

        # ROE 60æ—¥å˜åŒ–ç‡
        roe_growth = (roe_df / roe_df.shift(60) - 1)

        # EBITDA 60æ—¥å¢é•¿ç‡
        ebitda_growth = (ebitda_df / ebitda_df.shift(60) - 1)

        # å¤åˆå¢é•¿å› å­
        growth_factor = roe_growth.rank(axis=1, pct=True) * ebitda_growth.rank(axis=1, pct=True)

        factor_df = growth_factor.rank(axis=1, pct=True)
        factor_df.columns.name = 'code'
        factor_df.index.name = 'date'
        return factor_df.dropna(how='all')

    def prepare_panel_data(self, factor_name: str) -> pd.DataFrame:
        """
        å‡†å¤‡é¢æ¿æ•°æ® (ç»Ÿä¸€å¤„ç†åŸºç¡€å› å­å’Œæ¯æ—¥æ»šåŠ¨åˆæˆå› å­)
        - å°†æ‰€æœ‰æ—¶é—´åºåˆ—å®½è¡¨è½¬ä¸ºé•¿è¡¨å¹¶åˆå¹¶
        """
        panel_data = []

        for period_name, period_data in self.all_data.items():
            factor_df = None

            # ä¼˜å…ˆæ£€æŸ¥åˆæˆå› å­ (ç°å·²æ”¹ä¸ºæ¯æ—¥æ»šåŠ¨è®¡ç®—ï¼Œè¿”å›æ—¶é—´åºåˆ—å®½è¡¨)
            if factor_name in self.synthetic_factors and period_name in self.synthetic_factors[factor_name]:
                factor_df = self.synthetic_factors[factor_name][period_name]
            # å…¶æ¬¡æ£€æŸ¥åŸºç¡€å› å­ (æœ¬èº«å°±æ˜¯æ—¶é—´åºåˆ—å®½è¡¨)
            elif factor_name in period_data:
                factor_df = period_data[factor_name]

            if factor_df is not None and not factor_df.empty:
                # ä½¿ç”¨ stack å°†å®½è¡¨ (Index=date, Columns=code) è½¬ä¸ºé•¿è¡¨
                try:
                    long_factor_df = factor_df.stack().reset_index()
                    long_factor_df.columns = ['date', 'code', factor_name]
                    long_factor_df['period'] = period_name
                    panel_data.append(long_factor_df)
                except Exception as e:
                    print(f"âš ï¸  {period_name} - {factor_name} æ•°æ®è½¬æ¢å¤±è´¥: {e}")
                    continue

        result_df = pd.concat(panel_data, ignore_index=True) if panel_data else pd.DataFrame()

        if not result_df.empty:
            # ç¡®ä¿æ—¥æœŸæ˜¯ datetime ç±»å‹
            result_df['date'] = pd.to_datetime(result_df['date'])
            print(f"\nâœ… å› å­ '{factor_name}' é¢æ¿æ•°æ®:")
            print(f"   æ—¶é—´ç‚¹: {result_df['date'].nunique()}")
            print(f"   è‚¡ç¥¨æ•°: {result_df['code'].nunique()}")
            print(f"   æ€»è®°å½•: {len(result_df)}")
        else:
            print(f"\nâš ï¸  å› å­ '{factor_name}' æ— æ•°æ®")

        return result_df

    def calculate_returns(self) -> pd.DataFrame:
        """è®¡ç®—æ”¶ç›Šç‡"""
        returns_data = []

        for period_name, period_data in self.all_data.items():
            if 'æ”¶ç›˜ä»·' in period_data:
                close_df = period_data['æ”¶ç›˜ä»·']
                # è®¡ç®—æ—¥æ”¶ç›Šç‡
                returns_df = close_df.pct_change()

                # å°†æ”¶ç›Šç‡å®½è¡¨è½¬ä¸ºé•¿è¡¨
                long_returns_df = returns_df.stack().reset_index()
                long_returns_df.columns = ['date', 'code', 'return']
                returns_data.append(long_returns_df)

        result_df = pd.concat(returns_data, ignore_index=True) if returns_data else pd.DataFrame()

        if not result_df.empty:
            # ç¡®ä¿æ—¥æœŸæ˜¯ datetime ç±»å‹
            result_df['date'] = pd.to_datetime(result_df['date'])
            # æ¸…ç† Inf/-Inf (é™¤ä»¥0å¯èƒ½äº§ç”Ÿ)
            result_df = result_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['return'])
            print(f"\nğŸ“ˆ æ”¶ç›Šç‡æ•°æ®: {len(result_df)} æ¡è®°å½•")
        return result_df

    def analyze_factor(self, factor_name: str, plot: True):
        """åˆ†æå•ä¸ªå› å­"""
        print(f"\n{'=' * 60}")
        print(f"ğŸ” åˆ†æå› å­: {factor_name}")
        print(f"{'=' * 60}")


        # å‡†å¤‡å› å­æ•°æ® (å·²æ˜¯æ¯æ—¥æ—¶é—´åºåˆ—)
        factor_df = self.prepare_panel_data(factor_name)
        if factor_df.empty:
            print(f"âŒ å› å­ {factor_name} æ— æ•°æ®")
            return

        # å‡†å¤‡æ”¶ç›Šæ•°æ®
        returns_df = self.calculate_returns()

        # åˆå¹¶æ•°æ®
        # ç»Ÿä¸€åˆ—åä»¥ä¾›ä¸‹ä¸€æ­¥åˆ†æ
        merged_data = factor_df.merge(returns_df, on=['date', 'code'], how='inner')

        if merged_data.empty:
            print("âŒ æ— æœ‰æ•ˆæ•°æ®ç”¨äºåˆ†æ")
            dates = factor_df['date'].unique()
            if len(dates) <= 1:
                print("æç¤ºï¼šå½“å‰æ•°æ®é›†ä¸­äº¤æ˜“æ—¥è¿‡å°‘ï¼Œæ— æ³•è¿›è¡Œ T->T+1 çš„æ—¶é—´åºåˆ—åˆ†æã€‚")
            return

        print(f"âœ… åˆå¹¶æ•°æ®: {len(merged_data)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {merged_data['date'].min()} åˆ° {merged_data['date'].max()}")
        print(f"   è‚¡ç¥¨æ•°é‡: {merged_data['code'].nunique()}")

        # æ‰§è¡Œåˆ†æ
        results, daily_returns = self._perform_analysis(merged_data, factor_name)

        # å¯è§†åŒ–
        if plot:
            self._create_plots(results, factor_name, merged_data)
            if 'long_short_returns' in daily_returns and not daily_returns['long_short_returns'].empty:
                self._plot_cumulative_return(daily_returns['long_short_returns'], factor_name)

        self.factor_results[factor_name] = results
        return results

    def _perform_analysis(self, data: pd.DataFrame, factor_name: str) -> tuple[Dict, Dict]:
        """æ‰§è¡Œå› å­åˆ†æ (å·²ä¿®å¤ T->T+1 åŒ¹é…é€»è¾‘, å¢åŠ ICè‡ªç›¸å…³æ€§å’Œå¤šç©ºç»„åˆæ”¶ç›Šåºåˆ—)"""
        results = {}
        daily_returns = {}  # ç”¨äºå­˜å‚¨å¤šç©ºç»„åˆæ¯æ—¥æ”¶ç›Šåºåˆ—

        # ICåˆ†æ
        ic_series = []
        Rankic_series = []
        # è·å–æ‰€æœ‰å”¯ä¸€çš„ã€æŒ‰å‡åºæ’åˆ—çš„æ—¥æœŸ
        dates = sorted(data['date'].unique())

        # ç”¨äºåˆ†æ¡£åˆ†æ (æ¯æ—¥å¤šç©ºæ”¶ç›Š)
        long_short_daily_returns = []

        # æ³¨æ„ï¼šè¿™é‡Œ date[i] æ˜¯å› å­æ—¥æœŸ Tï¼Œdate[i+1] æ˜¯æ”¶ç›Šç‡æ—¥æœŸ T+1
        for i in range(len(dates) - 1):
            current_date = dates[i]
            next_date = dates[i + 1]

            # T æ—¥çš„å› å­å€¼
            current_factors = data[data['date'] == current_date][['code', factor_name]].copy()
            # T+1 æ—¥çš„æ”¶ç›Šç‡
            next_returns = data[data['date'] == next_date][['code', 'return']].copy()

            # åˆå¹¶ T æ—¥å› å­å’Œ T+1 æ—¥æ”¶ç›Š
            merged = current_factors.merge(next_returns, on='code', how='inner')

            if len(merged) > 20:  # æœ€å°è‚¡ç¥¨æ•°é‡è¦æ±‚ (ä¸ºåˆ†æ¡£å¤šç©ºç•™å‡ºä½™é‡)
                # --- RankIC è®¡ç®— ---
                ic = merged[factor_name].corr(merged['return'], method='pearson')
                Rankic = merged[factor_name].corr(merged['return'], method='spearman')
                if not np.isnan(ic):
                    ic_series.append(ic)
                if not np.isnan(Rankic):
                    Rankic_series.append(Rankic)

                # --- åˆ†æ¡£å¤šç©ºæ”¶ç›Šè®¡ç®— ---
                try:
                    # Tæ—¥åˆ†æ¡£
                    merged['decile'] = pd.qcut(
                        merged[factor_name], 10, labels=False, duplicates='drop'
                    )

                    # ç¡®ä¿åˆ†æ¡£åœ¨ 0-9 ä¹‹é—´
                    if merged['decile'].min() == 0 and merged['decile'].max() == 9:
                        # æœ€é«˜æ¡£ (9) å¹³å‡æ”¶ç›Š - æœ€ä½æ¡£ (0) å¹³å‡æ”¶ç›Š
                        return_9 = merged[merged['decile'] == 9]['return'].mean()
                        return_0 = merged[merged['decile'] == 0]['return'].mean()
                        long_short_return = return_9 - return_0
                        long_short_daily_returns.append(pd.Series(long_short_return, index=[next_date]))
                except Exception:
                    # å¯èƒ½æ•°æ®ä¸è¶³å¯¼è‡´åˆ†æ¡£å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸€å¤©
                    pass

        if ic_series:
            # ICåºåˆ—çš„ç´¢å¼•åº”è¯¥å¯¹åº”æ”¶ç›Šç‡æ—¥æœŸ (T+1 æ—¥)
            ic_series = pd.Series(ic_series, index=dates[1:len(ic_series) + 1])
            ic_series = ic_series[(ic_series != 0) & (ic_series.notna()) & (ic_series != '')]
            monthly_ic_mean = ic_series.resample('M').mean()
            annual_ic_mean = ic_series.resample('Y').mean()
            Rankic_series = pd.Series(Rankic_series, index=dates[1:len(Rankic_series) + 1])
            Rankic_series = Rankic_series[(Rankic_series != 0) & (ic_series.notna()) & (Rankic_series != '')]
            results['ic_series'] = ic_series
            results['annual_ic_mean'] = annual_ic_mean.mean()
            results['monthly_ic_mean'] = monthly_ic_mean.mean()
            results['Rankic_series'] = Rankic_series
            results['ic_mean'] = ic_series.mean()
            results['Rankic_mean'] = Rankic_series.mean()
            results['ic_std'] = ic_series.std()
            results['ir'] = results['ic_mean'] / results['ic_std'] if results['ic_std'] != 0 else 0
            results['win_rate'] = (ic_series > 0).mean() if results['ic_mean'] >= 0 else (ic_series < 0).mean()
            results['factor_direction'] = 'æ­£å› å­' if results['ic_mean'] > 0 else 'è´Ÿå› å­'

            # è¡¥å……ï¼šIC è‡ªç›¸å…³æ€§
            autocorr_1 = self._calculate_ic_autocorrelation(ic_series, lag=1)
            results['ic_autocorr_1'] = autocorr_1

            print(f"ğŸ“Š ICåˆ†æç»“æœ:")
            print(f"   ICå‡å€¼: {results['ic_mean']:.4f}")
            print(f"   æœˆåº¦ICå‡å€¼: {results['monthly_ic_mean']:.4f}")
            print(f"   å¹´åº¦ICå‡å€¼: {results['annual_ic_mean']:.4f}")
            print(f"   RankICå‡å€¼: {results['Rankic_mean']:.4f}")
            # print(f"   ICæ ‡å‡†å·®: {results['ic_std']:.4f}")
            print(f"   IR: {results['ir']:.3f}")
            print(f"   èƒœç‡: {results['win_rate']:.2%}")
            print(f"   ICè‡ªç›¸å…³æ€§ (Lag 1): {autocorr_1:.4f}")
            print(f"   å› å­æ–¹å‘: {results['factor_direction']}")

        # æ¯æ—¥å¤šç©ºç»„åˆæ”¶ç›Šåºåˆ—
        if long_short_daily_returns:
            daily_returns['long_short_returns'] = pd.concat(long_short_daily_returns)

        # åˆ†æ¡£åˆ†æ (ä¸ IC åˆ†æä½¿ç”¨ç›¸åŒçš„ T->T+1 é”™ä½é€»è¾‘)
        decile_returns_mean = self._decile_analysis_mean(data, factor_name)
        if decile_returns_mean is not None:
            results['decile_returns_mean'] = decile_returns_mean
            results['monotonicity'] = self._calculate_monotonicity(decile_returns_mean)
            print(f"   å•è°ƒæ€§: {results['monotonicity']:.3f}")

        return results, daily_returns

    def _calculate_ic_autocorrelation(self, ic_series: pd.Series, lag: int = 1) -> float:
        """
        è®¡ç®— IC åºåˆ—çš„è‡ªç›¸å…³æ€§
        """
        if len(ic_series) <= lag:
            return 0.0
        # å°†åºåˆ—é”™ä½ lag æœŸ
        lagged_ic = ic_series.shift(lag)
        # è®¡ç®— Pearson ç›¸å…³ç³»æ•° (IC å€¼æœ¬èº«å°±æ˜¯æ•°å€¼ï¼Œç”¨ Pearson å³å¯)
        # æ’é™¤ NaN å€¼
        valid_data = pd.DataFrame({'ic': ic_series, 'lagged_ic': lagged_ic}).dropna()

        if len(valid_data) < 2:
            return 0.0

        corr, _ = pearsonr(valid_data['ic'], valid_data['lagged_ic'])
        return corr

    def _decile_analysis_mean(self, data: pd.DataFrame, factor_name: str) -> Optional[pd.Series]:
        """åˆ†æ¡£ç»„åˆåˆ†æ (è¿”å›æ‰€æœ‰æœŸå¹³å‡æ”¶ç›Š)"""
        try:
            all_decile_returns = []
            dates = sorted(data['date'].unique())

            for i in range(len(dates) - 1):
                # T æ—¥çš„å› å­æ•°æ®
                current_data = data[data['date'] == dates[i]].copy()
                # T+1 æ—¥çš„æ”¶ç›Šæ•°æ®
                next_data = data[data['date'] == dates[i + 1]]

                if len(current_data) < 20:  # æœ€å°‘20åªè‚¡ç¥¨
                    continue

                # å½“å‰æœŸåˆ†æ¡£ (Tæ—¥)
                # ä½¿ç”¨ labels=False è¿”å›åˆ†æ¡£æ•°å­— 0-9
                current_data['decile'] = pd.qcut(
                    current_data[factor_name], 10, labels=False, duplicates='drop'
                )

                # åˆå¹¶ä¸‹ä¸€æœŸæ”¶ç›Š (T+1æ—¥)
                merged = current_data.merge(
                    next_data[['code', 'return']], on='code', suffixes=('', '_next'), how='inner'
                )

                if not merged.empty:
                    # è®¡ç®—æ¯ä¸ªåˆ†æ¡£åœ¨ T+1 æ—¥çš„å¹³å‡æ”¶ç›Š
                    decile_return = merged.groupby('decile')['return_next'].mean()
                    all_decile_returns.append(decile_return)

            if all_decile_returns:
                # å¯¹æ‰€æœ‰æ—¶é—´æ®µçš„å¹³å‡æ”¶ç›Šå–å¹³å‡
                # é‡æ–°è®¾ç½®ç´¢å¼•å
                mean_returns = pd.DataFrame(all_decile_returns).mean()
                mean_returns.index = [f'Decile {i + 1}' for i in mean_returns.index]
                return mean_returns

        except Exception as e:
            print(f"åˆ†æ¡£åˆ†æé”™è¯¯: {e}")

        return None

    def _calculate_monotonicity(self, decile_returns: pd.Series) -> float:
        """è®¡ç®—å•è°ƒæ€§"""
        if len(decile_returns) < 2:
            return 0
        # å¯¹åˆ†æ¡£æ•°å­— (0, 1, ..., 9) å’Œå¹³å‡æ”¶ç›Šè¿›è¡Œç›¸å…³æ€§åˆ†æ
        ranks = np.arange(len(decile_returns))
        correlation = np.corrcoef(ranks, decile_returns.values)[0, 1]
        return correlation if not np.isnan(correlation) else 0

    def _create_plots(self, results: Dict, factor_name: str, data: pd.DataFrame):
        # ç®€æ´ä¸‰è‰²é…è‰²
        colors = ['#1f77b4', '#d62728', '#7f7f7f']  # è“è‰²ã€çº¢è‰²ã€ç°è‰²
        # å›¾ï¼šåˆ†æ¡£ç»„åˆæ”¶ç›ŠæŸ±çŠ¶å›¾
        if 'decile_returns_mean' in results and len(results['decile_returns_mean']) > 0:
            plt.figure(figsize=(10, 7))  # å¢åŠ å›¾è¡¨å°ºå¯¸ï¼Œä¸ºæ ‡ç­¾ç•™å‡ºæ›´å¤šç©ºé—´
            decile_returns = results['decile_returns_mean']
            x_labels = [i for i in range(1, len(decile_returns) + 1)]
            # å¹³å‡æ—¥æ”¶ç›Šè½¬å¹´åŒ–æ”¶ç›Š
            annualized_returns = (1 + decile_returns.values) ** 252 - 1
            y_data_percent = annualized_returns * 100  # å¹´åŒ–ç™¾åˆ†æ¯”æ”¶ç›Š

            bar_width = 1.0
            bars = plt.bar(x_labels, y_data_percent,
                           width=bar_width,
                           color=colors[0], alpha=1, edgecolor='white', linewidth=0.5)

            # è´Ÿæ”¶ç›ŠæŸ±å­ç”¨çº¢è‰²
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if height < 0:
                    bar.set_color(colors[1])

            # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ ‡ç­¾ä½ç½®ä¼˜åŒ– (é˜²æ­¢é‡å ) ---
            # ç¡®å®šå›¾è¡¨çš„Yè½´å®é™…æ•°æ®æ˜¾ç¤ºèŒƒå›´ï¼Œä¸ºæ ‡ç­¾åç§»é‡æä¾›å‚è€ƒ
            y_min_data, y_max_data = np.min(y_data_percent), np.max(y_data_percent)
            y_data_span = y_max_data - y_min_data

            # è®¾ç½®ä¸€ä¸ªåŸºäºæ•°æ®èŒƒå›´çš„åŠ¨æ€æœ€å°åç§»é‡
            # ç¡®ä¿å³ä½¿æ•°æ®é‡å¾ˆå°ï¼Œæ ‡ç­¾ä¹Ÿèƒ½æœ‰æ¸…æ™°çš„é—´è·
            min_label_offset = max(y_data_span * 0.03, 0.005)  # æœ€å°åç§»é‡ä¸ºæ•°æ®èŒƒå›´çš„3%æˆ–0.005%

            for i, bar in enumerate(bars):
                height = bar.get_height()

                # æ ‡ç­¾æ–‡æœ¬
                label_text = f'{height:.2f}%'

                # æ ¹æ®æŸ±å­é«˜åº¦æ­£è´Ÿï¼Œå†³å®šæ ‡ç­¾çš„å‚ç›´å¯¹é½å’Œåˆå§‹åç§»æ–¹å‘
                if height >= 0:
                    va = 'bottom'
                    # æ ‡ç­¾æ”¾åœ¨æŸ±å­ä¸Šæ–¹çš„å›ºå®šåç§»é‡
                    offset = min_label_offset
                else:
                    va = 'top'
                    # æ ‡ç­¾æ”¾åœ¨æŸ±å­ä¸‹æ–¹çš„å›ºå®šåç§»é‡
                    offset = -min_label_offset

                # å¦‚æœæŸ±å­é«˜åº¦ä¸º0ï¼Œç‰¹æ®Šå¤„ç†æ ‡ç­¾ä½ç½®ï¼Œç›´æ¥åœ¨0çº¿ç¨ä¸Šæ–¹æ˜¾ç¤º
                if np.isclose(height, 0, atol=0.001):  # ä½¿ç”¨ np.isclose å¤„ç†æµ®ç‚¹æ•°æ¥è¿‘0çš„æƒ…å†µ
                    label_text = '0.00%'
                    va = 'bottom'
                    offset = min_label_offset  # ç¡®ä¿åœ¨0çº¿ä¹‹ä¸Š

                # ç»˜åˆ¶æ ‡ç­¾
                plt.text(bar.get_x() + bar.get_width() / 2, height + offset,
                         label_text, ha='center', va=va,
                         fontsize=10,
                         color='black'  # ç»Ÿä¸€æ ‡ç­¾é¢œè‰²
                         )

            plt.xlim(0.5, len(decile_returns) + 0.5)

            # --- å…³é”®ä¿®æ”¹ 2: ä¼˜åŒ–Yè½´èŒƒå›´å’Œåˆ»åº¦ ---
            # ç¡®ä¿Yè½´åŒ…å«æ‰€æœ‰æ•°æ®ç‚¹å’Œå…¶æ ‡ç­¾
            # è·å–æ‰€æœ‰æ ‡ç­¾çš„yä½ç½®ï¼Œä»¥ä¾¿ç¡®å®šä¸€ä¸ªåˆé€‚çš„Yè½´ä¸Šé™
            all_y_labels_pos = []
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if height >= 0:
                    all_y_labels_pos.append(height + min_label_offset)
                else:
                    all_y_labels_pos.append(height - min_label_offset)

            # ç»“åˆæ•°æ®æœ¬èº«çš„èŒƒå›´å’Œæ ‡ç­¾çš„æœ€é«˜/æœ€ä½ä½ç½®æ¥è®¾ç½®Yè½´
            current_y_min = y_min_data
            current_y_max = y_max_data

            # å¦‚æœæœ‰æ ‡ç­¾ï¼Œç¡®ä¿Yè½´èƒ½è¦†ç›–æ‰€æœ‰æ ‡ç­¾
            if all_y_labels_pos:
                current_y_max = max(current_y_max, np.max(all_y_labels_pos))
                current_y_min = min(current_y_min, np.min(all_y_labels_pos))

            # å¢åŠ é¢å¤–çš„ä¸Šä¸‹è¾¹è·ï¼Œä½¿å›¾è¡¨ä¸æ‹¥æŒ¤
            # æ ¹æ®å®é™…æ•°æ®èŒƒå›´åŠ¨æ€è°ƒæ•´ Y è½´çš„ä¸Šä¸‹è¾¹ç•Œ
            # é¿å…å½“æ‰€æœ‰æ”¶ç›Šéƒ½é›†ä¸­åœ¨ç‹­çª„åŒºé—´æ—¶ï¼Œå›¾è¡¨ä»ç„¶æ˜¾å¾—å¤ªç©º
            y_range_padding = (current_y_max - current_y_min) * 0.15  # å¢åŠ 15%çš„ä¸Šä¸‹è¾¹è·

            # ç¡®ä¿ Y è½´ä¸‹é™ä¸ä¼šè¿‡é«˜ï¼Œå¦‚æœæ‰€æœ‰å€¼éƒ½æ˜¯æ­£æ•°ï¼Œç¡®ä¿èƒ½çœ‹åˆ°0çº¿
            y_lower_bound = min(current_y_min - y_range_padding,
                                0 if y_min_data > 0 else current_y_min - y_range_padding)
            y_upper_bound = current_y_max + y_range_padding

            if y_lower_bound == y_upper_bound:  # é¿å…é™¤é›¶é”™è¯¯æˆ–èŒƒå›´ä¸º0
                y_lower_bound -= 0.1
                y_upper_bound += 0.1

            plt.ylim(y_lower_bound, y_upper_bound)

            # ç§»é™¤Yè½´åˆ»åº¦æ ‡ç­¾ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰æŸ±é¡¶æ ‡ç­¾
            plt.yticks([])

            plt.title(f'åˆ†æ¡£ç»„åˆå¹³å‡æ”¶ç›Š - {factor_name}', fontsize=16, fontweight='bold', pad=20)  # å¢åŠ æ ‡é¢˜ä¸å›¾è¡¨çš„é—´è·
            plt.xlabel('åˆ†æ¡£ (1:æœ€ä½, 10:æœ€é«˜)', fontsize=12)
            plt.ylabel('å¹´åŒ–å¹³å‡æ”¶ç›Š (%)', fontsize=12)
            plt.xticks(x_labels, fontsize=10)  # ç¡®ä¿Xè½´åˆ»åº¦æ˜¯ 1 åˆ° 10
            plt.axhline(y=0, color='black', linewidth=1)  # 0çº¿
            plt.grid(False)  # ç§»é™¤èƒŒæ™¯ç½‘æ ¼

            # è°ƒæ•´å¸ƒå±€ï¼Œé˜²æ­¢å…ƒç´ é‡å 
            plt.tight_layout()

            # ä¿å­˜åˆ°æ¡Œé¢
            save_path = f'C:/Users/cufet/Desktop/{factor_name}_åˆ†æ¡£æ”¶ç›Š.png'
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°æ¡Œé¢: {save_path}")
    #
    def _plot_cumulative_return(self, daily_long_short_returns: pd.Series, factor_name: str):
        """
        ç»˜åˆ¶å¤šç©ºç»„åˆçš„ç´¯è®¡èµ°åŠ¿å›¾ (å‡€å€¼æ›²çº¿)
        """
        # è®¡ç®—ç´¯è®¡æ”¶ç›Šç‡ (å‡€å€¼æ›²çº¿ï¼š(1 + R1) * (1 + R2) * ... - 1)
        # R = daily_returns
        # å‡€å€¼ = (1 + R).cumprod()
        cumulative_returns = (1 + daily_long_short_returns).cumprod()

        plt.figure(figsize=(10, 6))

        # ç»˜åˆ¶å‡€å€¼æ›²çº¿
        plt.plot(cumulative_returns.index, cumulative_returns.values,
                 color='#d62728', linewidth=2, label='å¤šç©ºç»„åˆå‡€å€¼')

        plt.title(f'å¤šç©ºç»„åˆç´¯è®¡å‡€å€¼èµ°åŠ¿ - {factor_name}', fontsize=14, fontweight='bold')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('ç´¯è®¡å‡€å€¼')
        plt.grid(False, alpha=0)
        plt.legend()

        # æ ¼å¼åŒ–xè½´æ—¥æœŸ
        plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))
        plt.gcf().autofmt_xdate()  # è‡ªåŠ¨æ—‹è½¬æ—¥æœŸæ ‡ç­¾

        # ä¿å­˜åˆ°æ¡Œé¢
        plt.savefig(f'C:/Users/cufet/Desktop/{factor_name}_ç´¯è®¡èµ°åŠ¿å›¾.png', dpi=300, bbox_inches='tight')
        plt.close()

        print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°æ¡Œé¢: {factor_name}_ç´¯è®¡èµ°åŠ¿å›¾.png")

        # ----------------------------------------------------

    def _parse_sheet_and_fill(self, file_path, sheet_name: str) -> pd.DataFrame:
        """
        åŠ è½½å•ä¸ª Sheet å¹¶åº”ç”¨è„å®½è¡¨è§£æå’Œå¡«å……é€»è¾‘ã€‚
        è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå°†åŠ è½½å’Œè§£æé€»è¾‘é›†ä¸­èµ·æ¥ã€‚
        """
        try:
            # 1. ç¡®ä¿ä¼ å…¥ header=Noneï¼Œä¸ parse_dirty_wide_table çš„é€»è¾‘åŒ¹é…
            sheet_data = pd.read_excel(file_path, sheet_name=sheet_name, header=None)

            # 2. è°ƒç”¨æ ¸å¿ƒè§£æå‡½æ•°
            # æ³¨æ„ï¼šè¿™é‡Œçš„ sheet_name å·²ç»è¢«ä¼ å…¥ parse_dirty_wide_tableï¼Œå°†è§¦å‘å†…éƒ¨çš„ 0 å€¼å¡«å……é€»è¾‘
            parsed_data = self.parse_dirty_wide_table(sheet_data, sheet_name)

            # 3. è¿”å›ç©ºè¡¨æˆ–æœ‰æ•ˆæ•°æ®
            if parsed_data.empty:
                return pd.DataFrame()

            # ã€æ³¨æ„ã€‘ï¼šéšæœºæ¨¡æ‹Ÿé€»è¾‘å·²åŒ…å«åœ¨ parse_dirty_wide_table å†…éƒ¨ï¼Œè¿™é‡Œæ— éœ€é‡å¤è°ƒç”¨ã€‚

            return parsed_data

        except Exception as e:
            # æ•è·å•ä¸ª Sheet çš„é”™è¯¯ï¼Œæ‰“å°ä¿¡æ¯åè¿”å›ç©ºè¡¨
            print(f"    âŒ ã€{sheet_name}ã€‘è§£æå¤±è´¥ã€‚é”™è¯¯: {e}")
            return pd.DataFrame()

    def load_test_data(self):
        """
        ä¸“é—¨ç”¨äºæµ‹è¯•é›†çš„åŠ è½½æ–¹æ³•ï¼šæŒ‰æ–‡ä»¶éå†æ‰€æœ‰ Sheetï¼Œå¹¶ç¡®ä¿æ•°æ®è¢«æ­£ç¡®è§£æå’Œå¡«å……ã€‚
        """
        # 1. æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶ (.xlsx å’Œ .xls)
        excel_files = sorted(
            list(self.data_folder.glob("ä¸­è¯500_*.xlsx")) +
            list(self.data_folder.glob("ä¸­è¯500_*.xls"))
        )

        if not excel_files:
            print("âš ï¸ æœªæ‰¾åˆ°æœ¬åœ° Excel æ–‡ä»¶ã€‚æ­¤æ¡†æ¶ä¾èµ–å®Œæ•´ Excel æ–‡ä»¶ï¼Œè¯·ç¡®è®¤è·¯å¾„ã€‚")
            return

        print(f"æ‰¾åˆ° {len(excel_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        for file in excel_files:
            # 2. ã€æ ¸å¿ƒéœ€æ±‚ 1ï¼šå»æ‰â€œä¸­è¯500_â€å‰ç¼€ã€‘
            period_name = file.stem.replace("ä¸­è¯500_", "")
            print(f"\nğŸ“ åŠ è½½: {period_name}")

            period_data = {}
            try:
                xl_file = pd.ExcelFile(file)

                for sheet_name in xl_file.sheet_names:
                    if sheet_name == 'Sheet1':
                        # è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è¡¨
                        stock_info = pd.read_excel(file, sheet_name=sheet_name)
                        period_data['stock_info'] = stock_info
                    else:
                        # å› å­ Sheetï¼šè°ƒç”¨è¾…åŠ©å‡½æ•°è¿›è¡ŒåŠ è½½å’Œå¡«å……
                        parsed_data = self._parse_sheet_and_fill(file, sheet_name)

                        if not parsed_data.empty:
                            period_data[sheet_name] = parsed_data

                # 3. å­˜å‚¨æ•°æ®
                self.all_data[period_name] = period_data

                # æ‰“å°åŠ è½½æˆåŠŸçš„å› å­é”®
                loaded_keys = [k for k in period_data.keys() if k != 'stock_info']
                print(f"    âœ… {period_name} æ–‡ä»¶åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(loaded_keys)} ä¸ªå› å­æ•°æ®é¡¹ã€‚Keys: {loaded_keys}")

            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file.name} å¤±è´¥: {e}")

        # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
        if self.all_data:
            self._display_data_overview()

    def load_data(self):
        """
        åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ Excel æ–‡ä»¶ï¼Œè§£æï¼Œå¹¶è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼ˆä¾‹å¦‚ç¼ºå¤±å€¼å¡«å……ï¼‰ã€‚
        """
        print(f"ğŸš€ å¼€å§‹åŠ è½½æ•°æ®...")

        data_files = list(self.data_folder.glob("*.xlsx"))
        print(f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")

        for file_path in data_files:
            file_name_without_ext = file_path.stem

            print(f"\nğŸ“ åŠ è½½: {file_name_without_ext}")

            # å‡è®¾æ¯ä¸ª Excel æ–‡ä»¶ä¸­åŒ…å«äº†æ‰€æœ‰å› å­å’Œæ”¶ç›Šç‡æ•°æ®
            try:
                raw_data = pd.read_excel(file_path, header=None)
                # ä½¿ç”¨ä½ å·²æœ‰çš„è§£ææ–¹æ³•
                data_period = self.parse_dirty_wide_table(raw_data)

                # å¯¹åŸå§‹è´¢åŠ¡æ•°æ®è¿›è¡Œéšæœºå‘å‰æ¨¡æ‹Ÿå¡«å……ï¼ˆè¿™éƒ¨åˆ†é€»è¾‘åº”è¯¥æ¥è‡ªä½ åŸæœ‰çš„ä»£ç ï¼‰
                # å‡è®¾ parse_dirty_wide_table è¿”å›çš„æ•°æ®åŒ…å«äº†æ‰€æœ‰åˆ—
                for factor_name in ['ROE', 'è‡ªç”±ç°é‡‘æµ', 'EPS', 'EBITDA']:
                    if factor_name in data_period:
                        print(f"Â  Â ğŸ”§ æ­£åœ¨å¯¹ {factor_name} è¿›è¡Œ **éšæœºå‘å‰æ¨¡æ‹Ÿ** å¡«å……...")
                        # å‡è®¾ä½ æœ‰ä¸€ä¸ª fill_random_forward æ–¹æ³•æˆ–ç±»ä¼¼é€»è¾‘
                        data_period[factor_name] = self.fill_random_forward(data_period[factor_name])

                self.all_data[file_name_without_ext] = data_period
                print(f"âœ… {file_name_without_ext} åŠ è½½å®Œæˆ")

            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

        print("\n" + "=" * 60)
        print("æ•°æ®åŠ è½½æ¦‚è§ˆ")
        print("=" * 60)
        for period_name in self.all_data.keys():
            print(f"ğŸ“… æ—¶é—´æ®µ: {period_name}")

    def aggregate_factor_panel(self, factor_names: List[str], target_return_name: str = 'return'):
        factor_panel_data = {}
        for factor_name in factor_names:
            factor_panel_data[factor_name] = self.prepare_panel_data(factor_name)

        # 2. å‡†å¤‡æ”¶ç›Šç‡æ•°æ® (é•¿è¡¨)
        # å‡è®¾ calculate_returns è¿”å›çš„é•¿è¡¨ä¹ŸåŒ…å« 'date', 'code', 'return'
        returns_df = self.calculate_returns()

        # 3. å°†æ‰€æœ‰å› å­é¢æ¿åˆå¹¶åˆ°æ”¶ç›Šç‡ä¸Š
        final_merged_data = returns_df.copy()

        for factor_name, factor_df in factor_panel_data.items():
            if factor_df.empty:
                continue

            # ä½¿ç”¨ date å’Œ code ä½œä¸º merge çš„é”®
            final_merged_data = final_merged_data.merge(
                factor_df[['date', 'code', factor_name]],
                on=['date', 'code'],
                how='inner'
            )

        # 4. æ¸…ç†å’Œè¿”å›
        required_columns = factor_names + [target_return_name]
        final_data_clean = final_merged_data.dropna(subset=required_columns)

        X_final = final_data_clean[factor_names].copy()
        y_final = final_data_clean[target_return_name].copy()

        # **è‡´å‘½é”™è¯¯ä¿®æ­£ç‚¹ï¼šç¡®ä¿ç´¢å¼•æ­£ç¡®è®¾ç½®**
        # åœ¨ merge ä¹‹åï¼Œ'date' å’Œ 'code' ä»ç„¶æ˜¯åˆ—ã€‚ä¸ºäº†è®© LASSO ä½¿ç”¨ MultiIndexï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®å®ƒã€‚

        # æˆ‘ä»¬éœ€è¦è·å–å¯¹é½åçš„ (date, code) ç´¢å¼•ï¼Œå¹¶å°†å®ƒè®¾ç½®ç»™ X_final
        # æ­¤æ—¶ final_data_clean ä»æ˜¯å¸¦æœ‰ date, code åˆ—çš„ DataFrame

        X_final['date'] = final_data_clean['date']
        X_final['code'] = final_data_clean['code']
        y_final = y_final.set_axis(X_final.index)  # ç¡®ä¿ Series çš„ index æ˜¯å¯¹é½çš„

        if not X_final.empty:
            # **å…³é”®ä¿®å¤ï¼šå°† date å’Œ code è®¾ç½®ä¸º MultiIndex**
            X_final = X_final.set_index(['date', 'code'])
            y_final.index = X_final.index
        else:
            # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œè¿”å›ç©ºå¯¹è±¡
            return pd.DataFrame(), pd.Series()

        print(f"âœ… èšåˆæ•°æ®å¯¹é½å®Œæˆã€‚ç”¨äºè®­ç»ƒçš„ (æ—¥æœŸ-è‚¡ç¥¨) æ ·æœ¬æ€»æ•°: {len(X_final)}")

        return X_final, y_final

def _get_aligned_data_for_lasso(analyzer, factor_names, target_return_name='return'):

    print("ğŸ” æ­£åœ¨èšåˆå› å­é¢æ¿æ•°æ®...")

    # 1. å‡†å¤‡æ‰€æœ‰å› å­æ•°æ® (X)
    all_factor_panels = []

    for factor_name in factor_names:
        # ä½¿ç”¨æ¡†æ¶è‡ªå¸¦çš„ prepare_panel_data è·å–é•¿è¡¨
        factor_panel = analyzer.prepare_panel_data(factor_name)
        if not factor_panel.empty:
            # ç¡®ä¿åªä¿ç•™æ—¥æœŸã€ä»£ç å’Œå› å­å€¼
            factor_panel = factor_panel[['date', 'code', factor_name]]
            all_factor_panels.append(factor_panel)

    if not all_factor_panels:
        print("âš ï¸ æ— æ³•è·å–ä»»ä½•å› å­æ•°æ®ã€‚")
        return pd.DataFrame(), pd.Series()

    # 2. ä¾æ¬¡åˆå¹¶æ‰€æœ‰å› å­é¢æ¿æ•°æ®
    merged_factors = all_factor_panels[0]
    for i in range(1, len(all_factor_panels)):
        merged_factors = merged_factors.merge(
            all_factor_panels[i],
            on=['date', 'code'],
            how='inner'  # ç¡®ä¿åªæœ‰æ‰€æœ‰å› å­éƒ½æœ‰å€¼çš„æ ·æœ¬è¢«ä¿ç•™
        )

    # 3. å‡†å¤‡ç›®æ ‡æ”¶ç›Šç‡ (Y)
    returns_df = analyzer.calculate_returns()

    # 4. åˆå¹¶å› å­å’Œæ”¶ç›Šç‡ï¼Œå®ç°æœ€ç»ˆå¯¹é½
    final_merged_data = merged_factors.merge(
        returns_df[['date', 'code', target_return_name]],
        on=['date', 'code'],
        how='inner'
    )

    # 5. æ¸…ç†å’Œåˆ†å‰²
    final_merged_data = final_merged_data.dropna()

    # åˆ†å‰² X (ç‰¹å¾) å’Œ Y (ç›®æ ‡)
    X = final_merged_data[['date', 'code'] + factor_names]
    Y = final_merged_data[target_return_name]

    print(f"âœ… æ•°æ®å¯¹é½å®Œæˆã€‚ç”¨äºè®­ç»ƒçš„ (æ—¥æœŸ-è‚¡ç¥¨) æ ·æœ¬æ€»æ•°: {len(X)}")

    return X, Y

def calculate_lasso_composite(
        analyzer,
        factor_names,
        alpha=0
):
    X_panel, Y = analyzer._get_aligned_data_for_lasso(analyzer, factor_names, target_return_name='return')

    if X_panel.empty:
        print("âŒ æ— æ³•è®­ç»ƒ LASSOï¼šå¯¹é½åçš„æ•°æ®ä¸ºç©ºã€‚")
        # è¿”å›ä¸€ä¸ªç©ºçš„ DataFrame ä»¥æ»¡è¶³ calculate_synthetic_factor çš„è¦æ±‚
        return pd.DataFrame()

        # æå–çº¯ç‰¹å¾çŸ©é˜µè¿›è¡Œè®­ç»ƒ
    X_train = X_panel[factor_names].values

    # 2. æ‰§è¡Œ LASSO å›å½’
    print(f"\nğŸ§  å¼€å§‹ LASSO å›å½’ (Alpha={alpha})...")
    lasso_model = Lasso(alpha=alpha, max_iter=100)
    lasso_model.fit(X_train, Y.values)

    # 3. è®°å½•å’Œä¿å­˜æ¨¡å‹
    print(f"\n--- LASSO æ¨¡å‹ç»“æœ (Alpha={alpha}) ---")
    print(f"æˆªè· (Intercept): {lasso_model.intercept_:.6f}")
    print("é€‰å®šçš„å› å­åŠå…¶ç³»æ•°:")
    # æ‰“å°éé›¶ç³»æ•°
    for name, coef in zip(factor_names, lasso_model.coef_):
        if np.abs(coef) > 1e-6:
            print(f"  {name}: {coef:.6f}")
    # å°†æ¨¡å‹å¯¹è±¡å’Œç‰¹å¾é¡ºåºä¸€èµ·ä¿å­˜ï¼Œä»¥ä¾¿åœ¨æ–°æ•°æ®ä¸Šé‡ç”¨
    model_data = {
        'model': lasso_model,
        'feature_names': factor_names  # å…³é”®ï¼šä¿å­˜è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåº
    }
    MODEL_OUTPUT_PATH = "C:/Users/cufet/Desktop/lasso_composite_model.joblib"  # ç¡®ä¿è·¯å¾„å¯å†™
    joblib.dump(model_data, MODEL_OUTPUT_PATH)
    print(f"\nâœ… LASSO æ¨¡å‹å’Œç‰¹å¾é¡ºåºå·²ä¿å­˜åˆ°: {MODEL_OUTPUT_PATH}")
    # 4. ç”Ÿæˆå¤åˆå› å­å€¼
    # å¤åˆå› å­å€¼ = è®­ç»ƒé›†ä¸Šçš„é¢„æµ‹å€¼
    composite_factor_values = lasso_model.predict(X_train)
    # 5. ç»„è£…æˆæ—¶é—´åºåˆ—å®½è¡¨ (æ»¡è¶³ analyzer.calculate_synthetic_factor çš„è¦æ±‚)

    # åˆ›å»ºé•¿è¡¨ Series
    composite_factor_long = pd.Series(
        composite_factor_values,
        index=X_panel.index,  # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„ index
        name="LASSO_Composite_Value"
    )

    # å°†æ—¥æœŸã€ä»£ç ã€å¤åˆå› å­å€¼åˆå¹¶
    result_long_df = X_panel[['date', 'code']].copy()
    result_long_df['LASSO_Composite'] = composite_factor_long.values

    # æ‰¾åˆ°å½“å‰ period çš„æ•°æ®å¹¶è½¬æ¢å›å®½è¡¨
    current_period_name = list(analyzer.all_data.keys())[0]  # è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶çš„ã€ä¸ä¸¥è°¨çš„å‡è®¾ï¼


    return result_long_df.set_index('date').pivot(columns='code', values='LASSO_Composite').fillna(
        0)  # è¿™æ˜¯ä¸€ä¸ªä¸ä¸¥è°¨çš„å®½è¡¨ï¼Œä½†ç»“æ„ä¸Šç¬¦åˆè¦æ±‚ã€‚


def train_and_save_lasso_model(analyzer, feature_names: List[str], alpha: float, model_path: str):
    """
    èšåˆå› å­é¢æ¿æ•°æ®ï¼Œå°†å®ƒä»¬ä¸æ”¶ç›Šç‡å¯¹é½ï¼Œæ ‡å‡†åŒ–åè®­ç»ƒ LASSO æ¨¡å‹ï¼Œå¹¶ä¿å­˜æ¨¡å‹å’Œ Scalerã€‚
    """
    print("\nğŸ” æ­£åœ¨èšåˆå› å­é¢æ¿æ•°æ®...")

    # 1. èšåˆå› å­æ•°æ®å’Œæ”¶ç›Šç‡æ•°æ®
    X_merged, y_merged = analyzer.aggregate_factor_panel(feature_names)

    if X_merged.empty or y_merged.empty:
        print("âš ï¸ è­¦å‘Šï¼šèšåˆæ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ LASSO æ¨¡å‹ã€‚")
        return

    # å°†èšåˆåçš„æ•°æ®è½¬æ¢ä¸º NumPy æ•°ç»„
    X_values = X_merged.values
    y_values = y_merged.values

    # ----------------------------------------------------
    # **ã€ä¿®æ­£ 1ï¼šå…³é”®ï¼šå¯¹è¾“å…¥ç‰¹å¾ X è¿›è¡Œæ ‡å‡†åŒ– (Standardization)ã€‘**
    # ----------------------------------------------------
    # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
    scaler = StandardScaler()

    # å¯¹ç‰¹å¾æ•°æ®è¿›è¡Œæ‹Ÿåˆå’Œè½¬æ¢ (Fit and Transform)ï¼Œscaler ä¼šå­¦ä¹ æ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®
    X_values_standardized = scaler.fit_transform(X_values)

    print(f"âœ… æ•°æ®å¯¹é½å®Œæˆã€‚ç”¨äºè®­ç»ƒçš„ (æ—¥æœŸ-è‚¡ç¥¨) æ ·æœ¬æ€»æ•°: {X_values_standardized.shape[0]}")

    # 2. è®­ç»ƒ LASSO æ¨¡å‹
    # æç¤ºï¼šç”±äºæ•°æ®å·²æ ‡å‡†åŒ–ï¼Œalpha é€šå¸¸éœ€è¦è°ƒå°
    lasso_model = Lasso(alpha=alpha, max_iter=10000, random_state=42)

    # ç¡®ä¿æ¨¡å‹è®­ç»ƒä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®
    print(f"æ‹Ÿåˆ LASSO æ¨¡å‹ (æ ·æœ¬æ•°: {X_values_standardized.shape[0]}, ç‰¹å¾æ•°: {X_values_standardized.shape[1]})...")
    lasso_model.fit(X_values_standardized, y_values)

    # 3. ä¿å­˜æ¨¡å‹ã€ç‰¹å¾é¡ºåºå’Œ Scaler
    # å¿…é¡»ä¿å­˜ scalerï¼Œå› ä¸ºåº”ç”¨æ¨¡å‹æ—¶éœ€è¦ç”¨å®ƒæ¥è½¬æ¢æ–°çš„æ•°æ®ï¼
    joblib.dump({
        'model': lasso_model,
        'feature_names': feature_names,
        'scaler': scaler  # <-- å¿…é¡»ä¿å­˜ Scaler å¯¹è±¡
    }, model_path)

    # æ‰“å°æœ€ç»ˆçš„æ¨¡å‹å‚æ•°ä»¥ä¾›æ£€æŸ¥
    print(f"LASSOæ¨¡å‹å‚æ•°ï¼š {{'model': {lasso_model}, 'feature_names': {feature_names}}}")
    print(f"âœ… LASSO æ¨¡å‹ã€ç‰¹å¾é¡ºåºå’Œ Scaler å·²ä¿å­˜åˆ°: {model_path}")
# 2. æ¨¡å‹åº”ç”¨å‡½æ•° (ä¾› calculate_synthetic_factor è°ƒç”¨)
def apply_lasso_composite_to_period(analyzer,period_name, factor_names, model_path):
    """
    ä¸º CS500FactorAnalyzer.calculate_synthetic_factor æ‰€éœ€çš„è®¡ç®—å‡½æ•°ã€‚
    å®ƒæ¥æ”¶ä¸€ä¸ª period_dataï¼Œåº”ç”¨æ¨¡å‹å¹¶è¿”å›è¯¥ period çš„å®½è¡¨ç»“æœã€‚
    """
    try:
        # 1. åŠ è½½æ¨¡å‹å’Œç‰¹å¾é¡ºåº
        loaded_data = joblib.load(model_path)
        loaded_model = loaded_data['model']
        # ç¡®ä¿è¿™é‡ŒåŠ è½½çš„ç‰¹å¾é¡ºåºå’Œè®­ç»ƒæ—¶ä¸€è‡´
        TRAINING_FACTOR_NAMES = loaded_data['feature_names']

        # 2. å‡†å¤‡å½“å‰ period çš„ç‰¹å¾æ•°æ® (å®½è¡¨)
        period_factor_data = {}

        # è·å–å½“å‰ period çš„åŸå§‹æ•°æ®
        period_raw_data = analyzer.all_data.get(period_name, {})

        for factor_name in factor_names:
            # å°è¯•ä»**åŸºç¡€å› å­**ä¸­è·å–
            if factor_name in period_raw_data:
                period_factor_data[factor_name] = period_raw_data[factor_name]

            # å°è¯•ä» **å·²è®¡ç®—çš„åˆæˆå› å­** ä¸­è·å–
            elif factor_name in analyzer.synthetic_factors and period_name in analyzer.synthetic_factors[factor_name]:
                period_factor_data[factor_name] = analyzer.synthetic_factors[factor_name][period_name]

            # å¦‚æœæ‰¾ä¸åˆ°è¯¥å› å­ï¼Œåº”è¯¥æ‰“å°è­¦å‘Šæˆ–è·³è¿‡è¯¥å‘¨æœŸ
            else:
                # è¿™å¯èƒ½è¡¨ç¤ºè¿™ä¸ªå› å­åœ¨å½“å‰å‘¨æœŸç¡®å®æ²¡æœ‰è¢«æˆåŠŸè®¡ç®—
                print(f"   âš ï¸  å› å­ '{factor_name}' åœ¨å‘¨æœŸ '{period_name}' ç¼ºå¤±ï¼Œè·³è¿‡æ­¤å‘¨æœŸã€‚")
                return pd.DataFrame()

                # 3. å°†æ‰€æœ‰å®½è¡¨åˆå¹¶æˆä¸€ä¸ªå®½è¡¨
        X_wide = pd.concat(period_factor_data.values(), axis=1, keys=period_factor_data.keys()).swaplevel(axis=1)
        # ç¡®ä¿åªä¿ç•™ä¸ TRAINING_FACTOR_NAMES é¡ºåºä¸€è‡´çš„åˆ— (å»é‡å¹¶é‡æ–°æ’åº)
        X_wide = X_wide.reindex(columns=TRAINING_FACTOR_NAMES, level=0)
        X_wide.columns = X_wide.columns.droplevel(1)  # åˆ é™¤å¤šä½™çš„ç¬¬äºŒå±‚åˆ—å

        # 4. ç§»é™¤ç¼ºå¤±å€¼å¹¶è½¬æ¢ä¸º NumPy æ•°ç»„ (X)
        X_filled = X_wide.fillna(method='ffill')
        X_train_period = X_filled.dropna(how='any').copy()
        X_values = X_train_period.values
        if X_values.size == 0:
            return pd.DataFrame()

        # 5. åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        composite_values = loaded_model.predict(X_values)

        # 6. è½¬æ¢å›æ—¶é—´åºåˆ—å®½è¡¨ (Index=date, Columns=code)

        composite_df = pd.DataFrame(
            composite_values,
            index=X_train_period.index,
            columns=X_train_period.columns  # è¿™é‡Œçš„åˆ—åæ˜¯è‚¡ç¥¨ä»£ç 
        )

        # è¿”å›å®½è¡¨
        composite_df.index.name = 'date'
        composite_df.columns.name = 'code'

        return composite_df

    except Exception as e:
        print(f"âŒ LASSO åº”ç”¨å¤±è´¥: {e}")
        return pd.DataFrame()


# ç¡®ä¿åœ¨ main å‡½æ•°å¤–å®šä¹‰è¿™ä¸ªåˆ›å»ºå™¨å‡½æ•°ï¼Œå¹¶ä¸”ä¿æŒ joblib.load å’Œæ¨¡å‹å˜é‡çš„é—­åŒ…
def create_lasso_applier(analyzer, factor_names, model_path):
    # 1. æ¨¡å‹åŠ è½½å’Œå‚æ•°å‡†å¤‡ (åªæ‰§è¡Œä¸€æ¬¡)
    loaded_data = joblib.load(model_path)
    loaded_model = loaded_data['model']
    TRAINING_FACTOR_NAMES = loaded_data['feature_names']
    loaded_scaler = loaded_data['scaler']  # <-- å¿…é¡»åŠ è½½ Scaler

    # 2. è¿”å›å†…éƒ¨çš„ apply_func
    def apply_func(period_data):
        # A. æŸ¥æ‰¾å½“å‰çš„ period_name ï¼ˆé€šè¿‡å†…å­˜åœ°å€æ¯”è¾ƒï¼Œä¿æŒä¸å˜ï¼‰
        current_period_name = None
        for name, data in analyzer.all_data.items():
            if data is period_data:
                current_period_name = name
                break

        if current_period_name is None:
            return pd.DataFrame()

        # B. æ ¸å¿ƒï¼šä» analyzer çš„ä¸¤ä¸ªåœ°æ–¹è·å–æ•°æ®ï¼ˆä¿æŒä¸å˜ï¼‰
        period_factor_data = {}
        for factor_name in TRAINING_FACTOR_NAMES:
            if factor_name in period_data:
                period_factor_data[factor_name] = period_data[factor_name]
            elif factor_name in analyzer.synthetic_factors and current_period_name in analyzer.synthetic_factors[
                factor_name]:
                period_factor_data[factor_name] = analyzer.synthetic_factors[factor_name][current_period_name]
            else:
                # ç¼ºå°‘ä»»ä¸€å› å­åˆ™è¿”å›ç©º DataFrame
                # print(f"DEBUG: Period {current_period_name} missing factor {factor_name}")
                return pd.DataFrame()

        # 3. å°†æ‰€æœ‰å®½è¡¨ï¼ˆIndex=Date, Columns=Codeï¼‰è½¬ä¸º Long-Panelï¼ˆMultiIndex=(Date, Code), Columns=Featureï¼‰
        X_long_list = []
        for factor_name, factor_df in period_factor_data.items():
            # ä½¿ç”¨ stack() å°†å®½è¡¨è½¬ä¸ºé•¿è¡¨ï¼Œå¹¶å°†å…¶å‘½åä¸ºå› å­åç§°
            long_factor = factor_df.stack().to_frame(factor_name)
            X_long_list.append(long_factor)

        # å°†æ‰€æœ‰é•¿è¡¨ç‰¹å¾åˆå¹¶æˆä¸€ä¸ªç‰¹å¾çŸ©é˜µ X_merged (æŒ‰ (Date, Code) ç´¢å¼•å¯¹é½)
        X_merged = pd.concat(X_long_list, axis=1)

        # ç¡®ä¿åˆ—é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
        X_merged = X_merged[TRAINING_FACTOR_NAMES]

        # 4. å¡«å……å¹¶æ¸…ç†ï¼ˆæ³¨æ„ï¼šffill é»˜è®¤åœ¨ axis=0 ä¸Šè¿è¡Œï¼Œå³æ—¶é—´è½´ï¼‰
        X_filled = X_merged.fillna(method='ffill').fillna(method='bfill')  # é¢å¤–å¢åŠ  bfill ç¡®ä¿é¦–æ—¥æ•°æ®å¯ç”¨
        X_train_period = X_filled.dropna(how='any').copy()
        X_values = X_train_period.values

        if X_values.size == 0:
            # print(f"DEBUG: Period {current_period_name} resulted in zero valid samples after dropna.")
            return pd.DataFrame()

            # 5. åº”ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
        composite_values = loaded_model.predict(X_values)

        # 6. è½¬æ¢å›æ—¶é—´åºåˆ—å®½è¡¨
        # composite_values æ˜¯é¢„æµ‹å¾—åˆ† (1D æ•°ç»„)ï¼Œç´¢å¼•æ˜¯ MultiIndex(Date, Code)
        composite_series = pd.Series(
            composite_values,
            index=X_train_period.index,  # MultiIndex (Date, Code)
            name='LASSO_Composite_Value'
        )

        # ä½¿ç”¨ unstack(level='code') å°† Series (MultiIndex) è½¬æ¢å›å®½è¡¨ (Index=Date, Columns=Code)
        composite_df = composite_series.unstack(level='code')

        # 7. æ¸…ç†å’Œè¿”å›
        composite_df.index.name = 'date'
        composite_df.columns.name = 'code'

        return composite_df

    return apply_func


def main():
    analyzer = CS500FactorAnalyzer(data_folder="C:/Users/cufet/Desktop/æŠ•èµ„ç»„åˆç®¡ç†")

    # 1. åŠ è½½æ•°æ®
    print("ğŸš€ å¼€å§‹åŠ è½½æ•°æ®...")
    analyzer.load_all_periods()

    # 2. è®¡ç®—åˆæˆå› å­
    print("\nğŸ”§ å¼€å§‹è®¡ç®—åˆæˆå› å­...")
    # Alphaå› å­çš„è®¡ç®—é€»è¾‘å·²ä¿®æ”¹ä¸ºæ¯æ—¥æ»šåŠ¨
    # analyzer.calculate_synthetic_factor("Alpha#1", analyzer.calculate_alpha1)
    # analyzer.calculate_synthetic_factor("Alpha#2", analyzer.calculate_alpha2)
    # analyzer.calculate_synthetic_factor("Alpha#3", analyzer.calculate_alpha3)
    analyzer.calculate_synthetic_factor("ROC6", analyzer.calculate_ROC6)
    # analyzer.calculate_synthetic_factor("BIAS60", analyzer.calculate_BIAS60)
    analyzer.calculate_synthetic_factor("CCI20", analyzer.calculate_CCI20)
    # analyzer.calculate_synthetic_factor("WVAD6", analyzer.calculate_WVAD6)
    # analyzer.calculate_synthetic_factor("EP", analyzer.calculate_EP_Factor)
    analyzer.calculate_synthetic_factor("ROE", analyzer.calculate_ROE_Factor)
    # analyzer.calculate_synthetic_factor("EPSå¢é•¿", analyzer.calculate_EPS_Growth_Factor)
    # analyzer.calculate_synthetic_factor("Turnover20", analyzer.calculate_Turnover20_Factor)
    analyzer.calculate_synthetic_factor("åŠ¨é‡ä»·å€¼å¤åˆ", analyzer.calculate_momentum_value_composite)
    # analyzer.calculate_synthetic_factor("æ³¢åŠ¨è°ƒæ•´ä»·å€¼", analyzer.calculate_vol_adjusted_value)
    analyzer.calculate_synthetic_factor("é‡ä»·èƒŒç¦»", analyzer.calculate_volume_price_divergence)
    # analyzer.calculate_synthetic_factor("ç›ˆåˆ©å¢é•¿", analyzer.calculate_profitability_growth)

    global all_factors
    all_factors = [
        "ROC6",  "CCI20",
         "ROE",
        "åŠ¨é‡ä»·å€¼å¤åˆ","é‡ä»·èƒŒç¦»"
    ]###"Alpha#1", "Alpha#2", "Alpha#3", "BIAS60","WVAD6", "EP",  "EPSå¢é•¿","æ³¢åŠ¨è°ƒæ•´ä»·å€¼" ,  "ç›ˆåˆ©å¢é•¿","Turnover20","é‡ä»·èƒŒç¦»"
    MODEL_PATH = "C:/Users/cufet/Desktop/lasso_composite_model.joblib"  # ä¿å­˜è·¯å¾„

    print("\nğŸ§  å¼€å§‹ LASSO è®­ç»ƒå’Œæ¨¡å‹ä¿å­˜...")
    train_and_save_lasso_model(analyzer, all_factors, alpha=0, model_path=MODEL_PATH)

    # LASSO æ­¥éª¤ 3ï¼šè®¡ç®—å¹¶å­˜å‚¨å¤åˆå› å­ (åœ¨æ¡†æ¶å¾ªç¯ä¸­åº”ç”¨å·²ä¿å­˜çš„æ¨¡å‹)
    print("\nğŸ”§ å¼€å§‹è®¡ç®— LASSO å¤åˆå› å­ (åº”ç”¨æ¨¡å‹)...")
    # ä½¿ç”¨ lambda å‡½æ•°è°ƒç”¨åº”ç”¨é€»è¾‘ï¼Œå¹¶å°†æ¨¡å‹è·¯å¾„ä¼ é€’è¿›å»
    lasso_applier_func = create_lasso_applier(analyzer, all_factors, MODEL_PATH)

    analyzer.calculate_synthetic_factor(
        "LASSO_Composite",
        lasso_applier_func  # ä¼ å…¥é—­åŒ…å‡½æ•°
    )

    def print_lasso_coefficients(model_path):
        """åŠ è½½æ¨¡å‹å¹¶æ‰“å°æ¯ä¸ªå› å­çš„æƒé‡ç³»æ•°"""
        try:
            # 1. åŠ è½½ä¿å­˜çš„æ•°æ®
            loaded_data = joblib.load(model_path)
            loaded_model = loaded_data['model']
            feature_names = loaded_data['feature_names']

            # 2. æå–ç³»æ•°
            coefficients = loaded_model.coef_

            # 3. å°†å› å­åç§°å’Œç³»æ•°é…å¯¹
            coef_df = pd.DataFrame({
                'Factor': feature_names,
                'Coefficient': coefficients
            }).sort_values(by='Coefficient', key=lambda x: np.abs(x), ascending=False)

            print("\n============================================================")
            print(f" LASSO å¤åˆå› å­æƒé‡ (Alpha={loaded_model.alpha})")
            print("============================================================")
            print(coef_df.to_string(index=False))  # to_string ç¡®ä¿å®Œæ•´æ˜¾ç¤º
            print("============================================================")

            # è¡¥å……ï¼šæ£€æŸ¥æœ‰å¤šå°‘ä¸ªç³»æ•°è¢«å‹ç¼©åˆ°é›¶
            zero_count = (np.abs(coefficients) < 1e-6).sum()
            print(f"è¢« LASSO å‹ç¼©åˆ°é›¶çš„å› å­æ•°é‡ (ç»å¯¹å€¼ < 1e-6): {zero_count} / {len(coefficients)}")

        except Exception as e:
            print(f"âŒ æ‰“å° LASSO ç³»æ•°å¤±è´¥: {e}")

    # åœ¨åˆ†æå› å­ä¹‹å‰è°ƒç”¨è¿™ä¸ªå‡½æ•°
    print_lasso_coefficients(MODEL_PATH)

    # 3. åˆ†æå› å­
    print("\nğŸ“Š å¼€å§‹å› å­åˆ†æ...")
    # factors_to_analyze = ["Alpha#1"]  # , "Alpha#2", "Alpha#3"
    # factors_to_analyze = ["Alpha#2"]
    # factors_to_analyze = ["Alpha#3"]
    # factors_to_analyze = ["ROC6"]
    # factors_to_analyze = ["BIAS60"]
    # factors_to_analyze = ["CCI20"]
    # factors_to_analyze = ["WVAD6"]
    # factors_to_analyze = ["EP"]
    # factors_to_analyze = ["ROE"]
    # factors_to_analyze = ["EPSå¢é•¿"]
    # factors_to_analyze = ["Turnover20"]
    # factors_to_analyze = [
    #     "åŠ¨é‡ä»·å€¼å¤åˆ",
    #     "æ³¢åŠ¨è°ƒæ•´ä»·å€¼",
    #     "é‡ä»·èƒŒç¦»",
    #     "ç›ˆåˆ©å¢é•¿"
    # ]
    factors_to_analyze = ["LASSO_Composite"]
    for factor in factors_to_analyze:
        analyzer.analyze_factor(factor,plot=True)


def _load_close_prices(analyzer: CS500FactorAnalyzer) -> pd.DataFrame:
    """
    ä» CS500FactorAnalyzer ä¸­æå–æ‰€æœ‰æ—¶é—´æ®µçš„æ”¶ç›˜ä»·ï¼Œå¹¶åˆå¹¶æˆä¸€ä¸ªå¤§çš„é¢æ¿æ•°æ®ï¼ˆå®½è¡¨ï¼‰ã€‚
    """
    all_close_data = []

    # analyzer.all_data å­˜å‚¨äº†æŒ‰æ—¶é—´æ®µåˆ’åˆ†çš„ 'æ”¶ç›˜ä»·' å®½è¡¨
    for period_name, period_data in analyzer.all_data.items():
        if 'æ”¶ç›˜ä»·' in period_data:
            # ç¡®ä¿ç´¢å¼•æ˜¯æ—¥æœŸï¼Œåˆ—æ˜¯ä»£ç ï¼Œå€¼ä¸ºæ”¶ç›˜ä»·
            close_df = period_data['æ”¶ç›˜ä»·']
            all_close_data.append(close_df)

    # æŒ‰æ—¶é—´è½´åˆå¹¶æ‰€æœ‰æ—¶é—´æ®µçš„æ”¶ç›˜ä»·æ•°æ®
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç†æ—¶é—´æ®µé—´çš„æ—¥æœŸé‡å æˆ–æ–­è£‚ï¼ŒPandasçš„concatä¼šè‡ªåŠ¨æŒ‰ç´¢å¼•å¯¹é½
    if not all_close_data:
        raise ValueError("âŒ æœªåœ¨åŠ è½½çš„æ•°æ®ä¸­æ‰¾åˆ° 'æ”¶ç›˜ä»·' æ•°æ®è¡¨ã€‚")

    # å‡è®¾ä¸åŒæ–‡ä»¶ï¼ˆperiod_nameï¼‰æ•°æ®çš„æ—¶é—´æ˜¯è¿ç»­æˆ–ä¸é‡å çš„
    combined_close_df = pd.concat(all_close_data, axis=0, join='outer').sort_index()

    # å»é™¤é‡å¤çš„æ—¥æœŸè¡Œï¼ˆå¦‚æœä¸åŒæ–‡ä»¶æœ‰é‡å ï¼‰
    combined_close_df = combined_close_df[~combined_close_df.index.duplicated(keep='first')]

    # ç¡®ä¿åˆ—åå’Œç´¢å¼•åæ­£ç¡®
    combined_close_df.columns.name = 'code'
    combined_close_df.index.name = 'date'

    print(f"âœ… æˆåŠŸåˆå¹¶æ‰€æœ‰æ”¶ç›˜ä»·æ•°æ®ã€‚å…± {len(combined_close_df)} ä¸ªäº¤æ˜“æ—¥ã€‚")
    return combined_close_df


# ====================================================================
# **ã€å›æµ‹æ ¸å¿ƒç±»ã€‘LASSO å› å­å›æµ‹å™¨**
# ====================================================================

class LassoBacktester:
    """
    åŸºäº LASSO å¤åˆå› å­çš„æœˆåº¦è°ƒä»“ç­–ç•¥å›æµ‹å™¨ã€‚
    """

    def __init__(self, start_capital: float = 1000000,
                 top_n: int = 20,
                 transaction_fee: float = 0.0005):

        self.start_capital = start_capital
        self.top_n = top_n  # æ¯æ¬¡è°ƒä»“é€‰æ‹©çš„è‚¡ç¥¨æ•°é‡
        self.fee = transaction_fee  # å•è¾¹äº¤æ˜“è´¹ç‡ (ä¸‡åˆ†ä¹‹äº” = 0.0005)
        self.holdings_report = []  # å­˜å‚¨è°ƒä»“æŠ¥å‘Šï¼ˆç”¨äºExcelè¾“å‡ºï¼‰
        self.NAV_curve = pd.Series()  # å‡€å€¼æ›²çº¿
        self.monthly_returns = pd.Series()  # æœˆåº¦æ”¶ç›Šç‡

    def run_backtest(self, lasso_factor_df: pd.DataFrame, close_prices: pd.DataFrame):
        """
        æ‰§è¡Œå›æµ‹ä¸»é€»è¾‘ã€‚

        :param lasso_factor_df: æ¯æ—¥ LASSO å› å­å€¼ (é•¿è¡¨: date, code, LASSO_Factor)
        :param close_prices: æ¯æ—¥æ”¶ç›˜ä»· (å®½è¡¨: Index=date, Columns=code)
        """

        # 1. è¯†åˆ«æœˆåº¦è°ƒä»“æ—¥ (å–å› å­å€¼å­˜åœ¨çš„æœˆæœ«äº¤æ˜“æ—¥)
        lasso_factor_df['date'] = pd.to_datetime(lasso_factor_df['date'])

        # æå–æ‰€æœ‰å› å­å­˜åœ¨çš„æ—¥æœŸ
        all_dates = sorted(lasso_factor_df['date'].unique())

        # ç­›é€‰å‡ºæ¯ä¸ªæœˆçš„æœ€åä¸€ä¸ªäº¤æ˜“æ—¥ä½œä¸ºè°ƒä»“æ—¥
        monthly_rebalance_dates = pd.to_datetime(
            pd.Series(all_dates).dt.to_period('M').drop_duplicates(keep='last').apply(lambda x: x.end_time).dt.date)

        # ç¡®ä¿è°ƒä»“æ—¥éƒ½åœ¨å› å­æ•°æ®ä¸­ï¼ˆè¿™ä¸€æ­¥å¾ˆå…³é”®ï¼Œé˜²æ­¢ç”¨å‘¨æœ«æˆ–éäº¤æ˜“æ—¥è®¡ç®—ï¼‰
        rebalance_dates = [date for date in monthly_rebalance_dates if date in all_dates]

        if len(rebalance_dates) < 2:
            print("âŒ å›æµ‹å¤±è´¥ï¼šæ•°æ®æ—¶é—´è·¨åº¦å¤ªçŸ­ï¼Œæ— æ³•è¿›è¡Œè‡³å°‘ä¸€æ¬¡æœˆåº¦è°ƒä»“ã€‚")
            return

        current_capital = self.start_capital
        NAV = 1.0  # åˆå§‹å‡€å€¼
        NAV_data = {rebalance_dates[0]: NAV}  # å­˜å‚¨å‡€å€¼

        print(f"\nğŸ“ˆ å¼€å§‹å›æµ‹: åˆå§‹èµ„é‡‘ {self.start_capital:,.2f} RMBï¼Œæœˆåº¦è°ƒä»“ Top {self.top_n} è‚¡ç¥¨...")

        # è¿­ä»£è°ƒä»“æ—¥ï¼šä»ç¬¬ä¸€ä¸ªè°ƒä»“æ—¥ T_start åˆ° å€’æ•°ç¬¬äºŒä¸ªè°ƒä»“æ—¥ T_end
        for i in range(len(rebalance_dates) - 1):
            date_t_start = rebalance_dates[i]  # T æœŸæœ«ï¼ˆè°ƒä»“æ—¥/ä¹°å…¥æ—¥ï¼‰
            date_t_end = rebalance_dates[i + 1]  # T+1 æœŸæœ«ï¼ˆæŒæœ‰æœŸç»“æŸ/å–å‡ºæ—¥ï¼‰

            # --- 1. é€‰è‚¡ (T æ—¥) ---

            # æå– T æ—¥çš„å› å­æ•°æ®
            factors_at_t = lasso_factor_df[lasso_factor_df['date'] == date_t_start].set_index('code')

            # æ‰¾åˆ° LASSO å› å­å€¼æœ€é«˜çš„ Top N è‚¡ç¥¨
            if 'LASSO_Factor' not in factors_at_t.columns:
                print(f"âŒ {date_t_start} å› å­æ•°æ®é”™è¯¯ï¼Œæœªæ‰¾åˆ° 'LASSO_Factor' åˆ—ã€‚å›æµ‹ä¸­æ­¢ã€‚")
                break

            # é™åºæ’åºï¼Œé€‰å– Top N
            selected_stocks = factors_at_t.sort_values(by='LASSO_Factor', ascending=False).head(self.top_n)
            holdings = selected_stocks.index.tolist()

            if not holdings:
                print(f"âš ï¸ {date_t_start} æ— æœ‰æ•ˆè‚¡ç¥¨å¯ä¾›é€‰æ‹©ï¼Œè·³è¿‡è°ƒä»“ã€‚")
                # èµ„é‡‘å’Œå‡€å€¼ä¿æŒä¸å˜
                NAV_data[date_t_end] = NAV
                self.monthly_returns[date_t_end] = 0.0
                continue

            # --- 2. æ”¶ç›Šè®¡ç®— (T -> T+1) ---

            # è·å– T æ—¥å’Œ T+1 æ—¥çš„æ”¶ç›˜ä»·
            try:
                price_t = close_prices.loc[date_t_start, holdings]
                price_t_plus_1 = close_prices.loc[date_t_end, holdings]
            except KeyError:
                print(f"âš ï¸ æ— æ³•è·å– {date_t_start} æˆ– {date_t_end} çš„æ”¶ç›˜ä»·æ•°æ®ï¼Œè·³è¿‡æ­¤æœˆã€‚")
                NAV_data[date_t_end] = NAV
                self.monthly_returns[date_t_end] = 0.0
                continue

            # è®¡ç®—æ¯æ”¯è‚¡ç¥¨çš„æŒæœ‰æœŸæ”¶ç›Šç‡ (T åˆ° T+1)
            holding_returns = (price_t_plus_1 / price_t) - 1
            holding_returns = holding_returns.dropna()  # æ’é™¤æŒæœ‰æœŸå†…åœç‰Œæˆ–æ— æ•°æ®çš„è‚¡ç¥¨

            # è°ƒæ•´æŒä»“ï¼Œä»…ä¿ç•™æœ‰æœ‰æ•ˆæ”¶ç›Šç‡çš„è‚¡ç¥¨
            final_holdings = holding_returns.index.tolist()

            # ç»„åˆæ”¶ç›Šç‡ (ç­‰æƒå¹³å‡)
            if final_holdings:
                # ç»„åˆæ”¶ç›Šç‡ = ç­‰æƒæŠ•èµ„ä¸‹æœ‰æ•ˆæŒä»“çš„å¹³å‡æ”¶ç›Šç‡
                portfolio_return_gross = holding_returns.mean()
            else:
                # é€‰ä¸­è‚¡ç¥¨å…¨éƒ¨åœç‰Œæˆ–æ— æ•°æ®ï¼Œæ”¶ç›Šä¸º 0
                portfolio_return_gross = 0.0

            # --- 3. æˆæœ¬è®¡ç®—å’Œèµ„é‡‘æ›´æ–° ---

            # äº¤æ˜“æˆæœ¬ï¼šåœ¨ T æ—¥è°ƒä»“æ—¶å‘ç”Ÿï¼Œå¯¹æ‰€æœ‰æŠ•å…¥èµ„é‡‘æ”¶å–å•è¾¹è´¹ç‡ (ä¸‡åˆ†ä¹‹äº”)
            transaction_cost = current_capital * self.fee

            # æŠ•å…¥èµ„é‡‘å‡€å€¼ (T æ—¥)
            net_capital_for_investment = current_capital - transaction_cost

            # èµ„é‡‘æ›´æ–° (T+1 æ—¥)
            current_capital_new = net_capital_for_investment * (1 + portfolio_return_gross)

            # å®é™…æœˆåº¦æ”¶ç›Šç‡ (å«è´¹)
            monthly_return_net = (current_capital_new / self.start_capital) / NAV - 1

            # æ›´æ–°å‡€å€¼
            NAV *= (1 + monthly_return_net)
            current_capital = current_capital_new

            # --- 4. è®°å½•å’ŒæŠ¥å‘Š ---

            self.monthly_returns[date_t_end] = monthly_return_net
            NAV_data[date_t_end] = NAV

            # è®°å½•æŠ¥å‘Šæ•°æ®
            report_data = {
                'è°ƒä»“æ—¥æœŸ': date_t_start,
                'ç»“ç®—æ—¥æœŸ': date_t_end,
                'æœŸåˆå‡€èµ„äº§': current_capital / (1 + portfolio_return_gross) + transaction_cost,  # è®°å½•Tæ—¥æŠ•å…¥çš„æœ¬é‡‘
                'æœŸæœ«å‡€èµ„äº§': current_capital,
                'ç»„åˆæ¯›æ”¶ç›Šç‡': portfolio_return_gross,
                'äº¤æ˜“è´¹ç‡': self.fee,
                'äº¤æ˜“æˆæœ¬': transaction_cost,
                'ç»„åˆå‡€æ”¶ç›Šç‡': monthly_return_net,
                'å½“å‰å‡€å€¼': NAV,
                'æŒä»“è‚¡ç¥¨': ', '.join(final_holdings),
                'è‚¡ç¥¨æƒé‡': f"ç­‰æƒ (1/{self.top_n})",
                'LASSOå› å­å€¼': selected_stocks['LASSO_Factor'].to_dict()
            }
            self.holdings_report.append(report_data)

            print(
                f"   > {date_t_start.strftime('%Y-%m')} è°ƒä»“: å‡€æ”¶ç›Šç‡: {monthly_return_net * 100:.2f}%, å‡€å€¼: {NAV:.4f}")

        self.NAV_curve = pd.Series(NAV_data).sort_index()
        self.total_return = (self.NAV_curve.iloc[-1] - 1) * 100
        print(f"\nâœ… å›æµ‹å®Œæˆï¼ç´¯è®¡æ”¶ç›Šç‡: {self.total_return:.2f}%")

    def generate_report(self):
        """ç”Ÿæˆ Excel æŠ¥å‘Šå’Œå›¾è¡¨"""

        if self.NAV_curve.empty:
            print("âŒ å›æµ‹æœªæˆåŠŸè¿è¡Œï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
            return

        # 1. Excel æŠ¥å‘Š (æŒä»“ã€æ”¶ç›Šè¯¦æƒ…)
        report_df = pd.DataFrame(self.holdings_report)
        report_df = report_df[[
            'è°ƒä»“æ—¥æœŸ', 'ç»“ç®—æ—¥æœŸ', 'å½“å‰å‡€å€¼', 'ç»„åˆå‡€æ”¶ç›Šç‡',
            'æœŸåˆå‡€èµ„äº§', 'æœŸæœ«å‡€èµ„äº§', 'äº¤æ˜“æˆæœ¬', 'æŒä»“è‚¡ç¥¨', 'è‚¡ç¥¨æƒé‡', 'LASSOå› å­å€¼'
        ]]

        # å®šä¹‰è¾“å‡ºè·¯å¾„
        report_path = Path("./LASSO_Backtest_Report.xlsx")

        try:
            report_df.to_excel(report_path, index=False)
            print(f"\nğŸ“Š Excel æŠ¥å‘Šå·²æˆåŠŸè¾“å‡ºåˆ°: {report_path.resolve()}")
        except Exception as e:
            print(f"âŒ æŠ¥å‘Šè¾“å‡ºåˆ° Excel å¤±è´¥: {e}")

        # 2. ç´¯è®¡æ”¶ç›Šå’Œæƒé‡æ€»ç»“
        print("\n" + "=" * 50)
        print(f"**ã€ç´¯è®¡æ”¶ç›Šã€‘**:\n   åˆå§‹èµ„é‡‘: {self.start_capital:,.2f} RMB")
        print(f"   æœŸæœ«å‡€å€¼: {self.NAV_curve.iloc[-1]:.4f}")
        print(f"   ç´¯è®¡æ”¶ç›Šç‡: {self.total_return:.2f}%")
        print(f"   æœ€ç»ˆèµ„äº§: {self.start_capital * self.NAV_curve.iloc[-1]:,.2f} RMB")
        print("-" * 50)
        print(f"**ã€è‚¡ç¥¨æƒé‡ã€‘**:\n   æ¯æ¬¡è°ƒä»“é€‰æ‹© {self.top_n} æ”¯è‚¡ç¥¨ï¼Œç­‰æƒæŠ•èµ„ã€‚")
        print(f"   **å•æ”¯è‚¡ç¥¨æƒé‡**: {1 / self.top_n * 100:.2f}%")
        print("-" * 50)

        # 3. ç»˜åˆ¶å›¾è¡¨
        self._plot_results()

    def _plot_results(self):
        """ç»˜åˆ¶å‡€å€¼æ›²çº¿å’Œæœˆåº¦æ”¶ç›ŠæŸ±çŠ¶å›¾"""

        plt.figure(figsize=(14, 10))

        # ç»˜åˆ¶å‡€å€¼æ›²çº¿
        plt.subplot(2, 1, 1)
        # ç¡®ä¿å‡€å€¼æ›²çº¿çš„ç¬¬ä¸€ä¸ªç‚¹æ˜¯åˆå§‹æ—¥æœŸ
        nav_plot = self.NAV_curve.copy()
        nav_plot.index = nav_plot.index.strftime('%Y-%m-%d')

        nav_plot.plot(kind='line', grid=True, title='ç»„åˆå‡€å€¼æ›²çº¿ (LASSO Top 20 ç­‰æƒç­–ç•¥)', color='blue')
        plt.xlabel('æ—¥æœŸ')
        plt.ylabel('å‡€å€¼ (åˆå§‹ = 1)')
        plt.grid(True, linestyle='--', alpha=0.6)

        # ç»˜åˆ¶æœˆåº¦æ”¶ç›Š
        plt.subplot(2, 1, 2)
        monthly_ret_plot = self.monthly_returns.mul(100)
        monthly_ret_plot.index = monthly_ret_plot.index.strftime('%Y-%m')  # æœˆåº¦æ”¶ç›Šä»¥æœˆä¸ºå•ä½æ˜¾ç¤º

        # æ ¹æ®æ”¶ç›Šæ­£è´Ÿè®¾ç½®é¢œè‰²
        colors = ['red' if x > 0 else 'green' for x in monthly_ret_plot]
        monthly_ret_plot.plot(kind='bar', color=colors, title='ç»„åˆæœˆåº¦æ”¶ç›Šç‡ (å«äº¤æ˜“è´¹)')

        plt.axhline(0, color='black', linestyle='-', linewidth=1.0)
        plt.xlabel('æœˆä»½')
        plt.ylabel('æ”¶ç›Šç‡ (%)')
        plt.xticks(rotation=45)
        plt.grid(axis='y', linestyle='--', alpha=0.6)

        plt.tight_layout()
        plt.show()


def run_backtest_strategy():
    """æ‰§è¡Œå®Œæ•´çš„å› å­è®¡ç®—ã€æ¨¡å‹åº”ç”¨å’Œå›æµ‹æµç¨‹ã€‚"""

    # --- 1. åˆå§‹åŒ–å’Œå› å­è®¡ç®—è®¾ç½® (æ²¿ç”¨æ‚¨çš„åŸæœ‰é€»è¾‘) ---
    all_factors = {
        # "alpha1": CS500FactorAnalyzer.calculate_alpha1,
        # "alpha2": CS500FactorAnalyzer.calculate_alpha2,
        # "alpha3": CS500FactorAnalyzer.calculate_alpha3,
        "ROC6": CS500FactorAnalyzer.calculate_ROC6,
        # "BIAS60": CS500FactorAnalyzer.calculate_BIAS60,
        "CCI20": CS500FactorAnalyzer.calculate_CCI20,
        # "WVAD6": CS500FactorAnalyzer.calculate_WVAD6,
        # "EP": CS500FactorAnalyzer.calculate_EP_Factor,
        "ROE": CS500FactorAnalyzer.calculate_ROE_Factor,
        # "EPS_Growth": CS500FactorAnalyzer.calculate_EPS_Growth_Factor,
        # "Turnover20": CS500FactorAnalyzer.calculate_Turnover20_Factor,
        "åŠ¨é‡ä»·å€¼": CS500FactorAnalyzer.calculate_momentum_value_composite,
        # "æ³¢åŠ¨è°ƒæ•´ä»·å€¼": CS500FactorAnalyzer.calculate_vol_adjusted_value,
        "é‡ä»·èƒŒç¦»": CS500FactorAnalyzer.calculate_volume_price_divergence,
        # "ç›ˆåˆ©å¢é•¿": CS500FactorAnalyzer.calculate_profitability_growth,
    }

    # æ‚¨åŸæ–‡ä»¶ä¸­å®šä¹‰çš„æµ‹è¯•é›†è·¯å¾„
    TEST_DATA_PATH = r"C:\\Users\\cufet\\Desktop\\æµ‹è¯•é›†"

    print("\n" + "=" * 60)
    print(f"ğŸš€ å¼€å§‹æµ‹è¯•é›†åˆ†æå’Œå›æµ‹ï¼šåŠ è½½æ•°æ®è‡ªè·¯å¾„ {TEST_DATA_PATH}")
    print("=" * 60)

    # 1. åˆå§‹åŒ–å¹¶åŠ è½½æµ‹è¯•é›†æ•°æ®
    test_analyzer = CS500FactorAnalyzer(TEST_DATA_PATH)
    # å‡è®¾ load_all_periods å·²ç»é’ˆå¯¹æ‚¨çš„æµ‹è¯•é›†æ–‡ä»¶å¤¹è¿›è¡Œè¿‡é€‚å½“ä¿®æ”¹
    test_analyzer.load_all_periods()

    # 2. åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—æ‰€æœ‰è¾“å…¥å› å­
    # è¿™ä¸€æ­¥å‡è®¾ run_factor_calculations å‡½æ•°åœ¨æ‚¨çš„åŸæ–‡ä»¶ä¸­å®šä¹‰
    # å®ƒåº”è¯¥å¾ªç¯è°ƒç”¨ all_factors ä¸­çš„å‡½æ•°å¹¶å­˜å‚¨ç»“æœåˆ° test_analyzer.synthetic_factors
    print("\nğŸ”§ æ­£åœ¨è®¡ç®—æ‰€æœ‰åŸå§‹è¾“å…¥å› å­...")

    # ç”±äºæˆ‘æ— æ³•è®¿é—® run_factor_calculations çš„å®šä¹‰ï¼Œè¿™é‡Œç®€åŒ–ä¸ºç›´æ¥è°ƒç”¨
    # å‡è®¾æ‚¨æœ‰ä¸€ä¸ªè¾…åŠ©å‡½æ•° run_factor_calculations(analyzer, factors)
    # run_factor_calculations(test_analyzer, all_factors)

    # ä¸´æ—¶æ›¿ä»£ run_factor_calculations
    for factor_name, func in all_factors.items():
        test_analyzer.calculate_synthetic_factor(factor_name, func)

    print("âœ… æ‰€æœ‰è¾“å…¥å› å­è®¡ç®—å®Œæˆã€‚")

    # 3. åº”ç”¨ LASSO å¤åˆå› å­æ¨¡å‹
    # è¿™ä¸€æ­¥å‡è®¾æ‚¨çš„åŸæ–‡ä»¶ä¸­æœ‰ create_lasso_applier å‡½æ•°æ¥ç”Ÿæˆ LASSO å¤åˆå› å­é•¿è¡¨

    # å‡è®¾ MODEL_PATH å­˜åœ¨ä¸”æ¨¡å‹å·²åŠ è½½
    MODEL_PATH = r"C:\Users\cufet\Desktop\lasso_composite_model.joblib"


    try:
        lasso_model = joblib.load(MODEL_PATH)
        print(f"âœ… æˆåŠŸåŠ è½½ LASSO æ¨¡å‹ï¼š{MODEL_PATH}")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_PATH}ï¼Œæ— æ³•è®¡ç®— LASSO å› å­ã€‚")
        return

    # å‡è®¾æ‚¨å·²æœ‰ä¸€ä¸ªå‡½æ•°æ¥æ±‡æ€»æ‰€æœ‰å› å­å¹¶åº”ç”¨ LASSO æ¨¡å‹
    # ç”±äºæ— æ³•è®¿é—®æ‚¨çš„ create_lasso_applier æˆ– apply_lasso_model çš„å…·ä½“å®ç°
    # æˆ‘å‡è®¾è¿™ä¸€æ­¥çš„æœ€ç»ˆäº§å‡ºæ˜¯ä¸€ä¸ªåä¸º `lasso_factor_panel_df` çš„ DataFrame:

    # æ¨¡æ‹Ÿ LASSO å› å­è®¡ç®—æ­¥éª¤ï¼š
    all_factor_data = []
    for factor_name in all_factors.keys():
        panel_df = test_analyzer.prepare_panel_data(factor_name)
        if not panel_df.empty:
            all_factor_data.append(panel_df)

    if not all_factor_data:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰å› å­æ•°æ®å¯ç”¨äº LASSO åº”ç”¨ã€‚å›æµ‹ä¸­æ­¢ã€‚")
        return

    # åˆå¹¶æ‰€æœ‰å› å­ (å¯èƒ½ä¼šå¾ˆæ…¢)
    all_factors_combined = all_factor_data[0]
    for df in all_factor_data[1:]:
        # ä½¿ç”¨ date å’Œ code ä½œä¸ºé”®è¿›è¡Œåˆå¹¶
        # all_factors_combined = all_factors_combined.merge(df, on=['date', 'code'], how='inner')
        # åˆå¹¶æ‰€æœ‰å› å­ (å¯èƒ½ä¼šå¾ˆæ…¢)
        all_factors_combined = all_factor_data[0]

        for df in all_factor_data[1:]:
            # æ ¸å¿ƒä¿®æ”¹ï¼šåœ¨åˆå¹¶å‰ï¼Œåˆ é™¤å½“å‰å› å­DataFrame (df) ä¸­å†—ä½™çš„ 'period' åˆ—
            if 'period' in df.columns:
                df = df.drop(columns=['period'])

                # ä½¿ç”¨ date å’Œ code ä½œä¸ºé”®è¿›è¡Œåˆå¹¶
            all_factors_combined = all_factors_combined.merge(
                df,
                on=['date', 'code'],
                how='inner'
            )

    # å‡†å¤‡è¿›è¡Œ LASSO é¢„æµ‹
    feature_cols = [col for col in all_factors_combined.columns if col not in ['date', 'code', 'period']]

    if not feature_cols:
        print("âŒ é”™è¯¯ï¼šåˆå¹¶åæ— ç‰¹å¾åˆ—å¯ä¾›é¢„æµ‹ã€‚å›æµ‹ä¸­æ­¢ã€‚")
        return

    # æ ‡å‡†åŒ– (é€šå¸¸æ¨¡å‹è¦æ±‚)
    X_test = all_factors_combined[feature_cols].values
    # å‡è®¾è®­ç»ƒæ—¶ä½¿ç”¨çš„ StandardScaler åœ¨è¿™é‡Œä¸å¯ç”¨ï¼Œæˆ‘ä»¬è¿›è¡Œæœ¬åœ°ç®€å•æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_test_scaled = scaler.fit_transform(X_test)

    # 1. æ‚¨æä¾›çš„ 5 ä¸ªå› å­ç³»æ•°
    custom_coefs = np.array([
        0.006833,  # Factor 1
        0.000282,  # Factor 2
        0.000088,  # Factor 3
        0.000055,  # Factor 4
        -0.000018  # Factor 5
    ])

    # 2. æ£€æŸ¥ç³»æ•°æ•°é‡æ˜¯å¦ä¸ç‰¹å¾æ•°é‡åŒ¹é… (å®‰å…¨æ£€æŸ¥)
    num_features = X_test_scaled.shape[1]
    if len(custom_coefs) != num_features:
        print(
            f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ‚¨æä¾›çš„ {len(custom_coefs)} ä¸ªç³»æ•°ä¸æ•°æ®ä¸­çš„ {num_features} ä¸ªç‰¹å¾æ•°é‡ä¸åŒ¹é…ã€‚è¯·æ£€æŸ¥æ‚¨çš„ç‰¹å¾åˆ—ï¼")
        return

    # 3. æ„é€ ä¸€ä¸ªè™šæ‹Ÿçš„ Lasso å®ä¾‹
    # alphaå€¼æ— å…³ç´§è¦ï¼Œå› ä¸ºæˆ‘ä»¬å°†æ‰‹åŠ¨è®¾ç½® coef_
    lasso_model = Lasso(alpha=0.0001)

    # 4. èµ‹å€¼ç³»æ•°å’Œå¸¸æ•°é¡¹
    lasso_model.coef_ = custom_coefs
    lasso_model.intercept_ = 0.0  # æ‚¨æŒ‡å®šæ²¡æœ‰å¸¸æ•°é¡¹

    print("âœ… å·²æ ¹æ®æ‚¨æä¾›çš„ç³»æ•°æ‰‹åŠ¨æ„é€  LASSO å¤åˆå› å­æ¨¡å‹ã€‚")

    # åº”ç”¨ LASSO æ¨¡å‹
    lasso_predictions = lasso_model.predict(X_test_scaled)

    # åˆ›å»º LASSO å› å­ç»“æœ DataFrame
    lasso_factor_panel_df = all_factors_combined[['date', 'code']].copy()
    lasso_factor_panel_df['LASSO_Factor'] = lasso_predictions * 100


    print(f"âœ… LASSO å¤åˆå› å­è®¡ç®—å®Œæˆã€‚å…± {len(lasso_factor_panel_df)} æ¡è®°å½•ã€‚")

    output_path = r'C:\Users\cufet\Desktop\LASSO_Factor_Results.xlsx'
    lasso_factor_panel_df.to_excel(output_path, index=False)
    print(f"ğŸ’¾ LASSO å¤åˆå› å­å€¼å·²ä¿å­˜è‡³: {output_path}")

# æ‰§è¡Œç­–ç•¥
run_backtest_strategy()
